{"text": "A customer service AI chatbot that automatically generates and stores customer interaction records without explicit user consent or transparency about AI involvement", "violation": "borderline", "category": "record_keeping", "severity": "borderline", "articles": ["5", "8"], "explanation": "The system's data retention practices may trigger Article 5 (transparency requirements) and Article 8 (record-keeping obligations), but its classification as 'high-risk' under Article 5(2) depends on whether the automated decision-making significantly impacts users' rights, creating ambiguity about its regulatory scope", "context": "customer service", "ambiguity": true}
{"text": "An AI system that uses video interviews to analyze micro-expressions and vocal tone to assess candidate 'honesty' and 'cultural fit', with outputs influencing hiring decisions.", "violation": "borderline", "category": "technical_robustness", "severity": "borderline", "articles": [5, 17, 35], "explanation": "The system's use of biometric data and psychological profiling raises questions about whether it falls under high-risk AI (Article 5(2)) due to potential discrimination or lack of transparency. While it may be considered limited-risk if deemed non-discriminatory and transparent, its opaque algorithmic decisions and potential for bias could trigger stricter requirements under Article 17 (technical robustness).", "context": "hiring", "ambiguity": true}
{"text": "A financial institution uses an AI system to assess creditworthiness by analyzing non-traditional data sources such as social media activity and behavioral patterns to determine loan eligibility.", "violation": "borderline", "category": "high_risk_systems", "severity": "borderline", "articles": [], "explanation": "While credit scoring is explicitly listed as a high-risk application under Article 8, the use of non-traditional data introduces ambiguity regarding compliance with transparency (Article 14) and risk management (Article 5) requirements. Experts may debate whether the system", "context": "finance", "ambiguity": true}
{"text": "A social media platform uses an AI system to automatically detect and remove misinformation, but the algorithm's opacity and high error rate make it unclear if it meets robustness and safety requirements.", "violation": "borderline", "category": "technical_robustness", "severity": "borderline", "articles": [5, 8, 17], "explanation": "The system's purpose aligns with Article 5 (high-risk AI for public safety), but its lack of transparency (Article 17) and potential for erroneous content removal (Article 8) create ambiguity about whether it meets technical robustness and safety standards.", "context": "social media", "ambiguity": true}
{"text": "An AI system used by a legal firm to automate the creation of legal documents and maintain internal records of past case outcomes for strategic decision-making.", "violation": "borderline", "category": "record_keeping", "severity": "borderline", "articles": ["Article 5", "Article 8"], "explanation": "The system's record-keeping could be seen as part of legal services (Article 8), but whether it qualifies as a high-risk system under Article 5 depends on whether the automated documentation of case outcomes is considered a 'decision-making process' that could impact fundamental rights. The ambiguity lies in whether the system's record-keeping is a passive administrative task or an active influence on legal strategies.", "context": "legal services", "ambiguity": true}
{"text": "An AI system in a hospital emergency department uses real-time biometric data (e.g., heart rate, blood pressure) from patients to predict deterioration and prioritize triage, without explicitly identifying individuals through facial recognition or other personal identifiers.", "violation": "borderline", "category": "high_risk_systems", "severity": "borderline", "articles": [5, 8, 17], "explanation": "The system's use of real-time biometric data for health monitoring may fall under Article 5's prohibition of real-time personal identification, but its non-identifying purpose could align with Article 8's allowance for biometric data in healthcare. Ambiguity arises over whether 'real-time processing' inherently constitutes personal identification, especially if data could indirectly reveal identities through pattern analysis.", "context": "healthcare", "ambiguity": true}
{"text": "An AI system used by legal services to predict case outcomes by analyzing historical legal data and market trends, with outputs presented as probabilistic recommendations rather than deterministic conclusions.", "violation": "borderline", "category": "technical_robustness", "severity": "borderline", "articles": [4, 54, 55], "explanation": "The system's use of probabilistic predictions blurs the line between advisory tools and decision-making systems. While it may not directly automate legal decisions, its influence on case strategies could indirectly affect fundamental rights (e.g., fair trial, access to justice). Its transparency requirements under Article 54 are ambiguous due to the probabilistic nature of outputs, and its classification as a 'high-risk' system under Article 4 is debated since legal services are not explicitly listed in the Act's high-risk categories.", "context": "legal services", "ambiguity": true}
{"text": "A customer service chatbot uses emotional recognition to detect user frustration and automatically tailors responses to de-escalate situations, but also subtly manipulates dialogue patterns to increase user engagement with upselling prompts.", "violation": "borderline", "category": "risk_management", "severity": "borderline", "articles": ["Article 4", "Article 5", "Article 9"], "explanation": "The system's use of emotional recognition for de-escalation could be seen as a legitimate customer service tool (Article 4), but its manipulation of dialogue patterns for upselling raises concerns about deception and exploitation (Article 5). Experts debate whether the intent to influence consumer behavior crosses into prohibited manipulative practices.", "context": "customer service", "ambiguity": true}
{"text": "An AI system that uses natural language processing to analyze unstructured data (e.g., cover letters, social media profiles) alongside structured resume data to rank candidates for a managerial position, with minimal human intervention in the final scoring.", "violation": "borderline", "category": "human_oversight", "severity": "borderline", "articles": [8, 9, 17], "explanation": "The system's use of unstructured data blurs the line between permissible data processing and prohibited biased decision-making. While Article 8 (high-risk systems in employment) applies to hiring, the lack of explicit transparency requirements for unstructured data analysis (Article 9) and the limited human oversight (Article 17) create ambiguity about whether the system meets compliance thresholds.", "context": "hiring", "ambiguity": true}
{"text": "An AI system used in manufacturing for predictive maintenance that employs machine learning to predict equipment failures, but lacks transparency in its decision-making process and has potential cybersecurity vulnerabilities in its data transmission protocols.", "violation": "borderline", "category": "accuracy_robustness", "severity": "borderline", "articles": ["Article 5", "Article 8"], "explanation": "The ambiguity arises from whether the AI system's opacity in decision-making (Article 5) and cybersecurity risks (Article 8) constitute high-risk under the EU AI Act. While predictive maintenance could be considered high-risk due to safety implications, the lack of explicit safety-related design goals and the system's integration with legacy industrial networks create uncertainty about its risk classification.", "context": "manufacturing", "ambiguity": true}
{"text": "An e-commerce platform uses an AI system to dynamically adjust product recommendations based on users' browsing behavior, but does not disclose that the recommendations are generated by AI or how user data is used to influence choices.", "violation": "borderline", "category": "transparency", "severity": "borderline", "articles": [5, 8], "explanation": "The system's transparency obligations depend on whether it is classified as a high-risk AI system under Article 5. Personalized recommendations in e-commerce are not explicitly listed as high-risk, but could be argued to involve indirect discrimination (Article 8) or manipulative practices. The lack of disclosure creates ambiguity about whether the system meets transparency requirements for high-risk or limited-risk categories.", "context": "e-commerce", "ambiguity": true}
{"text": "A law enforcement agency uses an AI system to analyze historical crime data and generate risk scores for neighborhoods, which are used to allocate police resources. The system does not provide detailed explanations for its risk scores or disclose the specific data sources and algorithmic logic used in its predictions.", "violation": "borderline", "category": "transparency", "severity": "borderline", "articles": ["Article 5", "Article 16"], "explanation": "The system's use of historical data for resource allocation may fall under high-risk AI under Article 5 (law enforcement) but lacks transparency requirements of Article 16. However, if the system's outputs are not directly used for individual decision-making (e.g., targeting specific persons), its risk level and required transparency thresholds are debated.", "context": "law enforcement", "ambiguity": true}
{"text": "An AI system that analyzes candidates' social media profiles to assess 'cultural fit' during hiring, using indirect indicators like posting frequency, language use, and engagement patterns to predict job performance.", "violation": "borderline", "category": "record_keeping", "severity": "borderline", "articles": ["Article 5 (Biometric data processing)", "Article 8 (Profiling)"], "explanation": "The system's use of non-biometric personal data for hiring decisions may fall under Article 8 (profiling) if it creates a risk of discrimination, but its indirect nature and potential for non-discriminatory use (e.g., assessing communication skills) create ambiguity about whether it qualifies as high-risk or limited-risk under the Act.", "context": "hiring", "ambiguity": true}
{"text": "A financial institution uses an AI system to automate loan approvals based on historical credit data, but the system has known inaccuracies in risk assessment and potential cybersecurity vulnerabilities.", "violation": "borderline", "category": "accuracy_robustness", "severity": "borderline", "articles": [], "explanation": "", "context": "finance", "ambiguity": true}
{"text": "An insurance company uses an AI system to assess risk based on non-traditional data sources, such as social media activity and geographical location, to determine premium rates for property insurance.", "violation": "borderline", "category": "prohibited_practices", "severity": "borderline", "articles": ["Article 5", "Article 9"], "explanation": "This practice could be seen as indirect discrimination under Article 9 if the data correlates with protected characteristics (e.g., socioeconomic status), but may be considered a legitimate risk assessment under Article 5 if it's deemed necessary for a lawful purpose. The ambiguity lies in whether the data processing constitutes discrimination or a valid risk factor.", "context": "insurance", "ambiguity": true}
{"text": "An AI system used in a hospital to predict sepsis risk in patients by analyzing electronic health records (EHRs), lab results, and vital signs, but with limited transparency in its decision-making process and potential for false positives that could delay critical care.", "violation": "borderline", "category": "accuracy_robustness", "severity": "borderline", "articles": ["Article 5", "Article 8", "Article 17"], "explanation": "The system's use in healthcare for clinical decision support may trigger Article 5 (high-risk AI) due to potential harm from inaccurate predictions. However, its role as a supportive tool rather than a standalone diagnostic system creates ambiguity about whether it meets the 'high-risk' threshold. Additionally, the lack of transparency (Article 17) and potential cybersecurity vulnerabilities (Article 8) further complicate compliance assessments.", "context": "healthcare", "ambiguity": true}
{"text": "An AI chatbot in customer service uses emotion recognition to assess customer sentiment and automatically escalates cases to human agents when detecting frustration, without explicit user consent.", "violation": "borderline", "category": "high_risk_systems", "severity": "borderline", "articles": [], "explanation": "The system", "context": "customer service", "ambiguity": true}
{"text": "An AI system used in schools to automatically assign grades to student assignments, with teachers able to override the grades but not required to review or justify their decisions.", "violation": "borderline", "category": "human_oversight", "severity": "borderline", "articles": ["Article 5", "Article 16", "Article 56"], "explanation": "The system's use of AI for grading could be considered high-risk under Article 5 (risk categories), but the presence of optional human oversight (teacher overrides) creates ambiguity about whether sufficient human oversight is ensured under Article 16 (human oversight requirements).", "context": "education", "ambiguity": true}
{"text": "An AI system in chemical manufacturing that uses historical data to predict and adjust temperature settings in real-time without a fallback mechanism for prediction failures, potentially leading to unsafe conditions if the predictions are incorrect.", "violation": "borderline", "category": "technical_robustness", "severity": "borderline", "articles": [5, 8, 16], "explanation": "Experts may disagree on whether the system's potential to cause safety risks (e.g., chemical leaks or explosions) qualifies as a high-risk application under the AI Act, and whether the lack of transparency and fallback mechanisms violates Articles 5 (safety) and 16 (transparency).", "context": "manufacturing", "ambiguity": true}
{"text": "An AI system used in hospitals to predict patient deterioration based on unstructured clinical notes, where the algorithm's decision-making process is not fully explainable to clinicians, and the training data includes both anonymized patient records and public health datasets without explicit consent for secondary use.", "violation": "borderline", "category": "technical_robustness", "severity": "borderline", "articles": [5, 8, 16], "explanation": "The system's use of unstructured data and lack of transparency may trigger Article 5 (transparency requirements), while the combination of patient and public health data without explicit consent could relate to Article 8 (data governance). However, its application in non-critical care settings may place it in a lower-risk category, creating ambiguity about whether it falls under high-risk obligations.", "context": "healthcare", "ambiguity": true}
