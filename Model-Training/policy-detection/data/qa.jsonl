{"id": "133df2a5-b30e-45dc-8673-ca552ba98924", "question": "What is the primary purpose of Regulation (EU) 2024/1689?", "answer": "To improve the functioning of the internal market by establishing a uniform legal framework for AI systems, promoting human-centric and trustworthy AI, ensuring high protection of health, safety, fundamental rights, and supporting innovation while preventing market fragmentation.", "context": "### REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\n\nof 13 June 2024\n\nlaying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008,\n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and\nDirectives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)\n\n(Text with EEA relevance)\n\nTHE EUROPEAN PARLIAMENT AND THE COUNCIL OF THE EUROPEAN UNION,\n\nHaving regard to the Treaty on the Functioning of the European Union, and in particular Articles 16 and 114 thereof,\n\nHaving regard to the proposal from the European Commission,\n\nAfter transmission of the draft legislative act to the national parliaments,\n\nHaving regard to the opinion of the European Economic and Social Committee (^1 ),\n\nHaving regard to the opinion of the European Central Bank (^2 ),\n\nHaving regard to the opinion of the Committee of the Regions (^3 ),\n\nActing in accordance with the ordinary legislative procedure (^4 ),\n\nWhereas:\n\n(1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal\nframework in particular for the development, the placing on the mar...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0001", "generated_at": "2025-05-14T16:22:41.000678", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "efe75310-12ab-4802-91ea-75044d462420", "question": "Which Union values and principles guide the application of this Regulation?", "answer": "The values enshrined in the Charter of Fundamental Rights of the European Union, including democracy, the rule of law, environmental protection, and the protection of natural persons, undertakings, and fundamental rights.", "context": "### REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\n\nof 13 June 2024\n\nlaying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008,\n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and\nDirectives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)\n\n(Text with EEA relevance)\n\nTHE EUROPEAN PARLIAMENT AND THE COUNCIL OF THE EUROPEAN UNION,\n\nHaving regard to the Treaty on the Functioning of the European Union, and in particular Articles 16 and 114 thereof,\n\nHaving regard to the proposal from the European Commission,\n\nAfter transmission of the draft legislative act to the national parliaments,\n\nHaving regard to the opinion of the European Economic and Social Committee (^1 ),\n\nHaving regard to the opinion of the European Central Bank (^2 ),\n\nHaving regard to the opinion of the Committee of the Regions (^3 ),\n\nActing in accordance with the ordinary legislative procedure (^4 ),\n\nWhereas:\n\n(1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal\nframework in particular for the development, the placing on the mar...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0001", "generated_at": "2025-05-14T16:22:41.002970", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "c3efa84e-77e7-424c-9b41-93da2b7d54e2", "question": "Why is a uniform legal framework for AI systems necessary according to the Regulation?", "answer": "To prevent divergence in national rules that could fragment the internal market, decrease legal certainty for operators, and ensure consistent protection of public interests and rights across the Union.", "context": "### REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\n\nof 13 June 2024\n\nlaying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008,\n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and\nDirectives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)\n\n(Text with EEA relevance)\n\nTHE EUROPEAN PARLIAMENT AND THE COUNCIL OF THE EUROPEAN UNION,\n\nHaving regard to the Treaty on the Functioning of the European Union, and in particular Articles 16 and 114 thereof,\n\nHaving regard to the proposal from the European Commission,\n\nAfter transmission of the draft legislative act to the national parliaments,\n\nHaving regard to the opinion of the European Economic and Social Committee (^1 ),\n\nHaving regard to the opinion of the European Central Bank (^2 ),\n\nHaving regard to the opinion of the Committee of the Regions (^3 ),\n\nActing in accordance with the ordinary legislative procedure (^4 ),\n\nWhereas:\n\n(1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal\nframework in particular for the development, the placing on the mar...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0001", "generated_at": "2025-05-14T16:22:41.004338", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "14f01332-04f2-434f-91b2-5b2676230108", "question": "Which sectors and areas of society does the Regulation highlight as benefiting from AI?", "answer": "Economic, environmental, and societal benefits across industries such as healthcare, agriculture, education, media, transport, public services, security, justice, energy, and environmental monitoring.", "context": "### REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\n\nof 13 June 2024\n\nlaying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008,\n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and\nDirectives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)\n\n(Text with EEA relevance)\n\nTHE EUROPEAN PARLIAMENT AND THE COUNCIL OF THE EUROPEAN UNION,\n\nHaving regard to the Treaty on the Functioning of the European Union, and in particular Articles 16 and 114 thereof,\n\nHaving regard to the proposal from the European Commission,\n\nAfter transmission of the draft legislative act to the national parliaments,\n\nHaving regard to the opinion of the European Economic and Social Committee (^1 ),\n\nHaving regard to the opinion of the European Central Bank (^2 ),\n\nHaving regard to the opinion of the Committee of the Regions (^3 ),\n\nActing in accordance with the ordinary legislative procedure (^4 ),\n\nWhereas:\n\n(1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal\nframework in particular for the development, the placing on the mar...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0001", "generated_at": "2025-05-14T16:22:41.005526", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "c5646bd6-b90f-4071-92fc-499975309278", "question": "What are the key considerations for establishing common rules for high-risk AI systems?", "answer": "The rules must align with the Charter, be non-discriminatory, comply with Union international trade commitments, and consider the European Declaration on Digital Rights and the Ethics guidelines for trustworthy AI.", "context": "### REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\n\nof 13 June 2024\n\nlaying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008,\n(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and\nDirectives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)\n\n(Text with EEA relevance)\n\nTHE EUROPEAN PARLIAMENT AND THE COUNCIL OF THE EUROPEAN UNION,\n\nHaving regard to the Treaty on the Functioning of the European Union, and in particular Articles 16 and 114 thereof,\n\nHaving regard to the proposal from the European Commission,\n\nAfter transmission of the draft legislative act to the national parliaments,\n\nHaving regard to the opinion of the European Economic and Social Committee (^1 ),\n\nHaving regard to the opinion of the European Central Bank (^2 ),\n\nHaving regard to the opinion of the Committee of the Regions (^3 ),\n\nActing in accordance with the ordinary legislative procedure (^4 ),\n\nWhereas:\n\n(1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal\nframework in particular for the development, the placing on the mar...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0001", "generated_at": "2025-05-14T16:22:41.006728", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "b297ad64-bb53-45bc-b607-de758322dcda", "question": "Which EU regulations safeguard the fundamental right to the protection of personal data?", "answer": "Regulations (EU) 2016/679 and (EU) 2018/1725, along with Directive (EU) 2016/680.", "context": "(10) The fundamental right to the protection of personal data is safeguarded in particular by Regulations (EU)\n2016/679 (^11 ) and (EU) 2018/1725 (^12 ) of the European Parliament and of the Council and Directive (EU) 2016/\nof the European Parliament and of the Council (^13 ). Directive 2002/58/EC of the European Parliament and of the\nCouncil (^14 ) additionally protects private life and the confidentiality of communications, including by way of\nproviding conditions for any storing of personal and non-personal data in, and access from, terminal equipment.\nThose Union legal acts provide the basis for sustainable and responsible data processing, including where data sets\ninclude a mix of personal and non-personal data. This Regulation does not seek to affect the application of existing\nUnion law governing the processing of personal data, including the tasks and powers of the independent supervisory\nauthorities competent to monitor compliance with those instruments. It also does not affect the obligations of\nproviders and deployers of AI systems in their role as data controllers or processors stemming from Union or\nnational law on the protection of personal data in so far as the desig...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0002", "generated_at": "2025-05-14T16:24:45.922740", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "9c3b6d34-8b85-4594-9743-7610c3368a6a", "question": "Does this Regulation affect existing Union law on data processing?", "answer": "No, it does not affect the application of existing Union law governing the processing of personal data or the tasks of supervisory authorities.", "context": "(10) The fundamental right to the protection of personal data is safeguarded in particular by Regulations (EU)\n2016/679 (^11 ) and (EU) 2018/1725 (^12 ) of the European Parliament and of the Council and Directive (EU) 2016/\nof the European Parliament and of the Council (^13 ). Directive 2002/58/EC of the European Parliament and of the\nCouncil (^14 ) additionally protects private life and the confidentiality of communications, including by way of\nproviding conditions for any storing of personal and non-personal data in, and access from, terminal equipment.\nThose Union legal acts provide the basis for sustainable and responsible data processing, including where data sets\ninclude a mix of personal and non-personal data. This Regulation does not seek to affect the application of existing\nUnion law governing the processing of personal data, including the tasks and powers of the independent supervisory\nauthorities competent to monitor compliance with those instruments. It also does not affect the obligations of\nproviders and deployers of AI systems in their role as data controllers or processors stemming from Union or\nnational law on the protection of personal data in so far as the desig...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0002", "generated_at": "2025-05-14T16:24:45.924093", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "0615d1e4-fb8c-47fc-83a2-53e01268265d", "question": "What is the key characteristic of an AI system as defined in the text?", "answer": "The capability to infer, which includes deriving models or algorithms from inputs or data.", "context": "(10) The fundamental right to the protection of personal data is safeguarded in particular by Regulations (EU)\n2016/679 (^11 ) and (EU) 2018/1725 (^12 ) of the European Parliament and of the Council and Directive (EU) 2016/\nof the European Parliament and of the Council (^13 ). Directive 2002/58/EC of the European Parliament and of the\nCouncil (^14 ) additionally protects private life and the confidentiality of communications, including by way of\nproviding conditions for any storing of personal and non-personal data in, and access from, terminal equipment.\nThose Union legal acts provide the basis for sustainable and responsible data processing, including where data sets\ninclude a mix of personal and non-personal data. This Regulation does not seek to affect the application of existing\nUnion law governing the processing of personal data, including the tasks and powers of the independent supervisory\nauthorities competent to monitor compliance with those instruments. It also does not affect the obligations of\nproviders and deployers of AI systems in their role as data controllers or processors stemming from Union or\nnational law on the protection of personal data in so far as the desig...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0002", "generated_at": "2025-05-14T16:24:45.925279", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "48473322-a9bd-4df2-a378-67a408d43cf6", "question": "How is biometric identification defined in this Regulation?", "answer": "Automated recognition of physical, physiological, and behavioural human features to establish an individual's identity by comparing biometric data to a reference database.", "context": "(10) The fundamental right to the protection of personal data is safeguarded in particular by Regulations (EU)\n2016/679 (^11 ) and (EU) 2018/1725 (^12 ) of the European Parliament and of the Council and Directive (EU) 2016/\nof the European Parliament and of the Council (^13 ). Directive 2002/58/EC of the European Parliament and of the\nCouncil (^14 ) additionally protects private life and the confidentiality of communications, including by way of\nproviding conditions for any storing of personal and non-personal data in, and access from, terminal equipment.\nThose Union legal acts provide the basis for sustainable and responsible data processing, including where data sets\ninclude a mix of personal and non-personal data. This Regulation does not seek to affect the application of existing\nUnion law governing the processing of personal data, including the tasks and powers of the independent supervisory\nauthorities competent to monitor compliance with those instruments. It also does not affect the obligations of\nproviders and deployers of AI systems in their role as data controllers or processors stemming from Union or\nnational law on the protection of personal data in so far as the desig...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0002", "generated_at": "2025-05-14T16:24:45.926439", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "87d1b8c7-0477-44e5-b63f-c9711d89ad90", "question": "What distinguishes biometric categorisation from other uses of biometric data?", "answer": "It involves assigning natural persons to specific categories based on biometric data, excluding ancillary features intrinsically linked to other services.", "context": "(10) The fundamental right to the protection of personal data is safeguarded in particular by Regulations (EU)\n2016/679 (^11 ) and (EU) 2018/1725 (^12 ) of the European Parliament and of the Council and Directive (EU) 2016/\nof the European Parliament and of the Council (^13 ). Directive 2002/58/EC of the European Parliament and of the\nCouncil (^14 ) additionally protects private life and the confidentiality of communications, including by way of\nproviding conditions for any storing of personal and non-personal data in, and access from, terminal equipment.\nThose Union legal acts provide the basis for sustainable and responsible data processing, including where data sets\ninclude a mix of personal and non-personal data. This Regulation does not seek to affect the application of existing\nUnion law governing the processing of personal data, including the tasks and powers of the independent supervisory\nauthorities competent to monitor compliance with those instruments. It also does not affect the obligations of\nproviders and deployers of AI systems in their role as data controllers or processors stemming from Union or\nnational law on the protection of personal data in so far as the desig...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0002", "generated_at": "2025-05-14T16:24:45.927591", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "daee76f8-d94d-4762-a62b-6afde9b934f6", "question": "What is the definition of a remote biometric identification system according to the Regulation?", "answer": "A remote biometric identification system is an AI system intended for the identification of natural persons without their active involvement, typically at a distance, through the comparison of biometric data with a reference database. It excludes systems used for biometric verification such as authentication or access control.", "context": "(17) The notion of  remote biometric identification system  referred to in this Regulation should be defined functionally,\nas an AI system intended for the identification of natural persons without their active involvement, typically at\na distance, through the comparison of a person s biometric data with the biometric data contained in a reference\ndatabase, irrespectively of the particular technology, processes or types of biometric data used. Such remote\nbiometric identification systems are typically used to perceive multiple persons or their behaviour simultaneously in\norder to facilitate significantly the identification of natural persons without their active involvement. This excludes\nAI systems intended to be used for biometric verification, which includes authentication, the sole purpose of which\nis to confirm that a specific natural person is the person he or she claims to be and to confirm the identity of\na natural person for the sole purpose of having access to a service, unlocking a device or having security access to\npremises. That exclusion is justified by the fact that such systems are likely to have a minor impact on fundamental\nrights of natural persons compared to t...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0003", "generated_at": "2025-05-14T16:24:02.942494", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "3778d330-1712-4fc8-9123-106336abf36d", "question": "Which AI systems are excluded from the scope of this Regulation for military, defence, or national security purposes?", "answer": "AI systems used for military, defence, or national security purposes are excluded from the scope of this Regulation, regardless of whether the entity is public or private. However, if such systems are later used for other purposes like civilian or humanitarian activities, they fall within the Regulation's scope.", "context": "(24) If, and insofar as, AI systems are placed on the market, put into service, or used with or without modification of such\nsystems for military, defence or national security purposes, those should be excluded from the scope of this\nRegulation regardless of which type of entity is carrying out those activities, such as whether it is a public or private\nentity. As regards military and defence purposes, such exclusion is justified both by Article 4(2) TEU and by the\nspecificities of the Member States  and the common Union defence policy covered by Chapter 2 of Title V TEU that\nare subject to public international law, which is therefore the more appropriate legal framework for the regulation of\nAI systems in the context of the use of lethal force and other AI systems in the context of military and defence\nactivities. As regards national security purposes, the exclusion is justified both by the fact that national security\nremains the sole responsibility of Member States in accordance with Article 4(2) TEU and by the specific nature and\noperational needs of national security activities and specific national rules applicable to those activities. Nonetheless,\nif an AI system developed, p...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0004", "generated_at": "2025-05-14T16:23:01.960346", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "6a30c2ea-4de3-4707-9960-80a55fea9bd1", "question": "What is the rationale for excluding AI systems used for national security purposes?", "answer": "The exclusion is justified because national security remains the sole responsibility of Member States under Article 4(2) TEU, and national security activities have specific operational needs and rules. However, if such systems are repurposed for non-security uses, they are subject to the Regulation.", "context": "(24) If, and insofar as, AI systems are placed on the market, put into service, or used with or without modification of such\nsystems for military, defence or national security purposes, those should be excluded from the scope of this\nRegulation regardless of which type of entity is carrying out those activities, such as whether it is a public or private\nentity. As regards military and defence purposes, such exclusion is justified both by Article 4(2) TEU and by the\nspecificities of the Member States  and the common Union defence policy covered by Chapter 2 of Title V TEU that\nare subject to public international law, which is therefore the more appropriate legal framework for the regulation of\nAI systems in the context of the use of lethal force and other AI systems in the context of military and defence\nactivities. As regards national security purposes, the exclusion is justified both by the fact that national security\nremains the sole responsibility of Member States in accordance with Article 4(2) TEU and by the specific nature and\noperational needs of national security activities and specific national rules applicable to those activities. Nonetheless,\nif an AI system developed, p...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0004", "generated_at": "2025-05-14T16:23:01.962726", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "042f0f45-7553-4836-ae02-d78e94d9f94c", "question": "Which AI systems are exempted from the Regulation for scientific research and development?", "answer": "AI systems and models specifically developed and used solely for scientific research and development are excluded. However, if such systems are later placed on the market or used for other purposes, they fall under the Regulation's scope.", "context": "(24) If, and insofar as, AI systems are placed on the market, put into service, or used with or without modification of such\nsystems for military, defence or national security purposes, those should be excluded from the scope of this\nRegulation regardless of which type of entity is carrying out those activities, such as whether it is a public or private\nentity. As regards military and defence purposes, such exclusion is justified both by Article 4(2) TEU and by the\nspecificities of the Member States  and the common Union defence policy covered by Chapter 2 of Title V TEU that\nare subject to public international law, which is therefore the more appropriate legal framework for the regulation of\nAI systems in the context of the use of lethal force and other AI systems in the context of military and defence\nactivities. As regards national security purposes, the exclusion is justified both by the fact that national security\nremains the sole responsibility of Member States in accordance with Article 4(2) TEU and by the specific nature and\noperational needs of national security activities and specific national rules applicable to those activities. Nonetheless,\nif an AI system developed, p...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0004", "generated_at": "2025-05-14T16:23:01.964790", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "6ace3201-3d45-4ace-93eb-72e7d56e4cf9", "question": "What is the basis for the risk-based approach in regulating AI systems?", "answer": "The risk-based approach tailors rules to the intensity and scope of risks AI systems may generate, prohibiting unacceptable practices, setting requirements for high-risk systems, and imposing transparency obligations for certain systems.", "context": "(24) If, and insofar as, AI systems are placed on the market, put into service, or used with or without modification of such\nsystems for military, defence or national security purposes, those should be excluded from the scope of this\nRegulation regardless of which type of entity is carrying out those activities, such as whether it is a public or private\nentity. As regards military and defence purposes, such exclusion is justified both by Article 4(2) TEU and by the\nspecificities of the Member States  and the common Union defence policy covered by Chapter 2 of Title V TEU that\nare subject to public international law, which is therefore the more appropriate legal framework for the regulation of\nAI systems in the context of the use of lethal force and other AI systems in the context of military and defence\nactivities. As regards national security purposes, the exclusion is justified both by the fact that national security\nremains the sole responsibility of Member States in accordance with Article 4(2) TEU and by the specific nature and\noperational needs of national security activities and specific national rules applicable to those activities. Nonetheless,\nif an AI system developed, p...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0004", "generated_at": "2025-05-14T16:23:01.966976", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "47c492d1-b0e7-4d35-9a0a-934e926bd033", "question": "What are the prohibited AI practices related to manipulative techniques?", "answer": "AI systems that materially distort human behaviour, impair autonomy, or cause significant harm (e.g., through subliminal stimuli, virtual reality, or exploiting vulnerabilities) are prohibited. This includes techniques that deceive, manipulate, or exploit individuals' vulnerabilities.", "context": "(24) If, and insofar as, AI systems are placed on the market, put into service, or used with or without modification of such\nsystems for military, defence or national security purposes, those should be excluded from the scope of this\nRegulation regardless of which type of entity is carrying out those activities, such as whether it is a public or private\nentity. As regards military and defence purposes, such exclusion is justified both by Article 4(2) TEU and by the\nspecificities of the Member States  and the common Union defence policy covered by Chapter 2 of Title V TEU that\nare subject to public international law, which is therefore the more appropriate legal framework for the regulation of\nAI systems in the context of the use of lethal force and other AI systems in the context of military and defence\nactivities. As regards national security purposes, the exclusion is justified both by the fact that national security\nremains the sole responsibility of Member States in accordance with Article 4(2) TEU and by the specific nature and\noperational needs of national security activities and specific national rules applicable to those activities. Nonetheless,\nif an AI system developed, p...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0004", "generated_at": "2025-05-14T16:23:01.968761", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "785165c5-e00d-4a60-b693-8605d43d47f4", "question": "What is prohibited regarding biometric categorisation systems?", "answer": "Biometric categorisation systems that use natural persons' biometric data to infer political opinions, trade union membership, religious or philosophical beliefs, race, sex life, or sexual orientation are prohibited.", "context": "intention to distort behaviour where the distortion results from factors external to the AI system which are outside\nthe control of the provider or the deployer, namely factors that may not be reasonably foreseeable and therefore not\npossible for the provider or the deployer of the AI system to mitigate. In any case, it is not necessary for the provider\nor the deployer to have the intention to cause significant harm, provided that such harm results from the\nmanipulative or exploitative AI-enabled practices. The prohibitions for such AI practices are complementary to the\nprovisions contained in Directive 2005/29/EC of the European Parliament and of the Council (^17 ), in particular unfair\ncommercial practices leading to economic or financial harms to consumers are prohibited under all circumstances,\nirrespective of whether they are put in place through AI systems or otherwise. The prohibitions of manipulative and\nexploitative practices in this Regulation should not affect lawful practices in the context of medical treatment such as\npsychological treatment of a mental disease or physical rehabilitation, when those practices are carried out in\naccordance with the applicable law and me...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0005", "generated_at": "2025-05-14T16:26:34.385107", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "d3ee0ae9-f582-44ea-aa2e-30b8c936beaf", "question": "Why are AI systems providing social scoring prohibited?", "answer": "AI systems providing social scoring may lead to discriminatory outcomes, violate the right to dignity and non-discrimination, and result in disproportionate or unjustified treatment of individuals or groups.", "context": "intention to distort behaviour where the distortion results from factors external to the AI system which are outside\nthe control of the provider or the deployer, namely factors that may not be reasonably foreseeable and therefore not\npossible for the provider or the deployer of the AI system to mitigate. In any case, it is not necessary for the provider\nor the deployer to have the intention to cause significant harm, provided that such harm results from the\nmanipulative or exploitative AI-enabled practices. The prohibitions for such AI practices are complementary to the\nprovisions contained in Directive 2005/29/EC of the European Parliament and of the Council (^17 ), in particular unfair\ncommercial practices leading to economic or financial harms to consumers are prohibited under all circumstances,\nirrespective of whether they are put in place through AI systems or otherwise. The prohibitions of manipulative and\nexploitative practices in this Regulation should not affect lawful practices in the context of medical treatment such as\npsychological treatment of a mental disease or physical rehabilitation, when those practices are carried out in\naccordance with the applicable law and me...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0005", "generated_at": "2025-05-14T16:26:34.386629", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "7936f574-341c-4dc8-a6e8-2d4473e0a1cd", "question": "Under what exceptions is real-time remote biometric identification allowed for law enforcement?", "answer": "Real-time remote biometric identification is allowed only in narrowly defined situations, such as searching for missing persons, threats to life or physical safety, or locating perpetrators of serious crimes punishable by custodial sentences.", "context": "intention to distort behaviour where the distortion results from factors external to the AI system which are outside\nthe control of the provider or the deployer, namely factors that may not be reasonably foreseeable and therefore not\npossible for the provider or the deployer of the AI system to mitigate. In any case, it is not necessary for the provider\nor the deployer to have the intention to cause significant harm, provided that such harm results from the\nmanipulative or exploitative AI-enabled practices. The prohibitions for such AI practices are complementary to the\nprovisions contained in Directive 2005/29/EC of the European Parliament and of the Council (^17 ), in particular unfair\ncommercial practices leading to economic or financial harms to consumers are prohibited under all circumstances,\nirrespective of whether they are put in place through AI systems or otherwise. The prohibitions of manipulative and\nexploitative practices in this Regulation should not affect lawful practices in the context of medical treatment such as\npsychological treatment of a mental disease or physical rehabilitation, when those practices are carried out in\naccordance with the applicable law and me...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0005", "generated_at": "2025-05-14T16:26:34.388062", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "74192ff3-671b-4ecf-abd9-0f6a6afcc47d", "question": "What conditions must be met for using real-time biometric systems in law enforcement?", "answer": "Use requires judicial authorization, a fundamental rights impact assessment, and registration in a database, with restrictions to the minimum necessary scope and time period.", "context": "intention to distort behaviour where the distortion results from factors external to the AI system which are outside\nthe control of the provider or the deployer, namely factors that may not be reasonably foreseeable and therefore not\npossible for the provider or the deployer of the AI system to mitigate. In any case, it is not necessary for the provider\nor the deployer to have the intention to cause significant harm, provided that such harm results from the\nmanipulative or exploitative AI-enabled practices. The prohibitions for such AI practices are complementary to the\nprovisions contained in Directive 2005/29/EC of the European Parliament and of the Council (^17 ), in particular unfair\ncommercial practices leading to economic or financial harms to consumers are prohibited under all circumstances,\nirrespective of whether they are put in place through AI systems or otherwise. The prohibitions of manipulative and\nexploitative practices in this Regulation should not affect lawful practices in the context of medical treatment such as\npsychological treatment of a mental disease or physical rehabilitation, when those practices are carried out in\naccordance with the applicable law and me...", "source": "EU AI Act", "category": "Compliance", "metadata": {"chunk_id": "chunk_0005", "generated_at": "2025-05-14T16:26:34.389269", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "ce0ce8a6-5ce5-4cec-a42e-93b02d599af9", "question": "What data is excluded from real-time biometric identification use?", "answer": "Input data legally acquired under other Union or national laws is excluded, and decisions with adverse legal effects must not rely solely on the output of the system.", "context": "intention to distort behaviour where the distortion results from factors external to the AI system which are outside\nthe control of the provider or the deployer, namely factors that may not be reasonably foreseeable and therefore not\npossible for the provider or the deployer of the AI system to mitigate. In any case, it is not necessary for the provider\nor the deployer to have the intention to cause significant harm, provided that such harm results from the\nmanipulative or exploitative AI-enabled practices. The prohibitions for such AI practices are complementary to the\nprovisions contained in Directive 2005/29/EC of the European Parliament and of the Council (^17 ), in particular unfair\ncommercial practices leading to economic or financial harms to consumers are prohibited under all circumstances,\nirrespective of whether they are put in place through AI systems or otherwise. The prohibitions of manipulative and\nexploitative practices in this Regulation should not affect lawful practices in the context of medical treatment such as\npsychological treatment of a mental disease or physical rehabilitation, when those practices are carried out in\naccordance with the applicable law and me...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0005", "generated_at": "2025-05-14T16:26:34.390466", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "eebd94d2-744c-4e5a-8ffe-72827cfdcf8d", "question": "What notification requirements apply to the use of real-time biometric identification systems?", "answer": "The relevant market surveillance authority and national data protection authority must be notified of each use, and they must submit an annual report to the Commission.", "context": "(36) In order to carry out their tasks in accordance with the requirements set out in this Regulation as well as in national\nrules, the relevant market surveillance authority and the national data protection authority should be notified of each\nuse of the real-time biometric identification system. Market surveillance authorities and the national data protection\nauthorities that have been notified should submit to the Commission an annual report on the use of real-time\nbiometric identification systems.\n\n(37) Furthermore, it is appropriate to provide, within the exhaustive framework set by this Regulation that such use in\nthe territory of a Member State in accordance with this Regulation should only be possible where and in as far as the\nMember State concerned has decided to expressly provide for the possibility to authorise such use in its detailed rules\nof national law. Consequently, Member States remain free under this Regulation not to provide for such a possibility\nat all or to only provide for such a possibility in respect of some of the objectives capable of justifying authorised use\nidentified in this Regulation. Such national rules should be notified to the Commission within...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0006", "generated_at": "2025-05-14T16:25:58.623937", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "c2f087e3-8099-437f-839f-dc3c9095cc01", "question": "How long do Member States have to notify the Commission of their national rules regarding biometric identification?", "answer": "Member States must notify the Commission within 30 days of adopting their national rules.", "context": "(36) In order to carry out their tasks in accordance with the requirements set out in this Regulation as well as in national\nrules, the relevant market surveillance authority and the national data protection authority should be notified of each\nuse of the real-time biometric identification system. Market surveillance authorities and the national data protection\nauthorities that have been notified should submit to the Commission an annual report on the use of real-time\nbiometric identification systems.\n\n(37) Furthermore, it is appropriate to provide, within the exhaustive framework set by this Regulation that such use in\nthe territory of a Member State in accordance with this Regulation should only be possible where and in as far as the\nMember State concerned has decided to expressly provide for the possibility to authorise such use in its detailed rules\nof national law. Consequently, Member States remain free under this Regulation not to provide for such a possibility\nat all or to only provide for such a possibility in respect of some of the objectives capable of justifying authorised use\nidentified in this Regulation. Such national rules should be notified to the Commission within...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0006", "generated_at": "2025-05-14T16:25:58.625370", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "beac621d-685c-45f3-91cc-d2d7b7036416", "question": "What legal framework applies to biometric data processing for law enforcement under this Regulation?", "answer": "This Regulation serves as lex specialis over Directive (EU) 2016/680, prohibiting biometric data processing unless compatible with the Regulation's framework.", "context": "(36) In order to carry out their tasks in accordance with the requirements set out in this Regulation as well as in national\nrules, the relevant market surveillance authority and the national data protection authority should be notified of each\nuse of the real-time biometric identification system. Market surveillance authorities and the national data protection\nauthorities that have been notified should submit to the Commission an annual report on the use of real-time\nbiometric identification systems.\n\n(37) Furthermore, it is appropriate to provide, within the exhaustive framework set by this Regulation that such use in\nthe territory of a Member State in accordance with this Regulation should only be possible where and in as far as the\nMember State concerned has decided to expressly provide for the possibility to authorise such use in its detailed rules\nof national law. Consequently, Member States remain free under this Regulation not to provide for such a possibility\nat all or to only provide for such a possibility in respect of some of the objectives capable of justifying authorised use\nidentified in this Regulation. Such national rules should be notified to the Commission within...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0006", "generated_at": "2025-05-14T16:25:58.626548", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "2102a745-8113-42f0-b615-dcd67e91a2e7", "question": "Are there exceptions for using biometric identification systems outside law enforcement purposes?", "answer": "No, use for purposes other than law enforcement is not subject to this Regulation's authorisation requirements or national rules.", "context": "(36) In order to carry out their tasks in accordance with the requirements set out in this Regulation as well as in national\nrules, the relevant market surveillance authority and the national data protection authority should be notified of each\nuse of the real-time biometric identification system. Market surveillance authorities and the national data protection\nauthorities that have been notified should submit to the Commission an annual report on the use of real-time\nbiometric identification systems.\n\n(37) Furthermore, it is appropriate to provide, within the exhaustive framework set by this Regulation that such use in\nthe territory of a Member State in accordance with this Regulation should only be possible where and in as far as the\nMember State concerned has decided to expressly provide for the possibility to authorise such use in its detailed rules\nof national law. Consequently, Member States remain free under this Regulation not to provide for such a possibility\nat all or to only provide for such a possibility in respect of some of the objectives capable of justifying authorised use\nidentified in this Regulation. Such national rules should be notified to the Commission within...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0006", "generated_at": "2025-05-14T16:25:58.627706", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "1e935c3d-f69a-4d5e-8526-9d02e21b4103", "question": "What AI systems are prohibited regarding facial recognition databases?", "answer": "AI systems creating or expanding facial recognition databases through untargeted scraping of facial images from the internet or CCTV footage are prohibited.", "context": "(36) In order to carry out their tasks in accordance with the requirements set out in this Regulation as well as in national\nrules, the relevant market surveillance authority and the national data protection authority should be notified of each\nuse of the real-time biometric identification system. Market surveillance authorities and the national data protection\nauthorities that have been notified should submit to the Commission an annual report on the use of real-time\nbiometric identification systems.\n\n(37) Furthermore, it is appropriate to provide, within the exhaustive framework set by this Regulation that such use in\nthe territory of a Member State in accordance with this Regulation should only be possible where and in as far as the\nMember State concerned has decided to expressly provide for the possibility to authorise such use in its detailed rules\nof national law. Consequently, Member States remain free under this Regulation not to provide for such a possibility\nat all or to only provide for such a possibility in respect of some of the objectives capable of justifying authorised use\nidentified in this Regulation. Such national rules should be notified to the Commission within...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0006", "generated_at": "2025-05-14T16:25:58.628860", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "269bae1f-74b1-4061-816a-ab87fed64c07", "question": "What are the potential adverse impacts of AI systems on health and safety, particularly when they operate as safety components of products?", "answer": "AI systems could have an adverse impact on the health and safety of persons, especially when they operate as safety components of products, requiring prevention and mitigation of risks to ensure safe operation in complex environments.", "context": "(47) AI systems could have an adverse impact on the health and safety of persons, in particular when such systems\noperate as safety components of products. Consistent with the objectives of Union harmonisation legislation to\nfacilitate the free movement of products in the internal market and to ensure that only safe and otherwise compliant\nproducts find their way into the market, it is important that the safety risks that may be generated by a product as\na whole due to its digital components, including AI systems, are duly prevented and mitigated. For instance,\nincreasingly autonomous robots, whether in the context of manufacturing or personal assistance and care should be\nable to safely operate and performs their functions in complex environments. Similarly, in the health sector where\nthe stakes for life and health are particularly high, increasingly sophisticated diagnostics systems and systems\nsupporting human decisions should be reliable and accurate.\n\n(48) The extent of the adverse impact caused by the AI system on the fundamental rights protected by the Charter is of\nparticular relevance when classifying an AI system as high risk. Those rights include the right to human digni...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0007", "generated_at": "2025-05-14T16:28:02.010244", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "9633f9d7-8e9e-4726-9db0-d9347afafc46", "question": "Which fundamental rights are considered when classifying an AI system as high risk?", "answer": "The right to human dignity, privacy, freedom of expression, non-discrimination, consumer protection, workers' rights, environmental protection, and children's rights under the Charter and UNCRC are considered.", "context": "(47) AI systems could have an adverse impact on the health and safety of persons, in particular when such systems\noperate as safety components of products. Consistent with the objectives of Union harmonisation legislation to\nfacilitate the free movement of products in the internal market and to ensure that only safe and otherwise compliant\nproducts find their way into the market, it is important that the safety risks that may be generated by a product as\na whole due to its digital components, including AI systems, are duly prevented and mitigated. For instance,\nincreasingly autonomous robots, whether in the context of manufacturing or personal assistance and care should be\nable to safely operate and performs their functions in complex environments. Similarly, in the health sector where\nthe stakes for life and health are particularly high, increasingly sophisticated diagnostics systems and systems\nsupporting human decisions should be reliable and accurate.\n\n(48) The extent of the adverse impact caused by the AI system on the fundamental rights protected by the Charter is of\nparticular relevance when classifying an AI system as high risk. Those rights include the right to human digni...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0007", "generated_at": "2025-05-14T16:28:02.011718", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "6a90e05f-54f2-40a5-aaff-00b582230171", "question": "Which EU regulations are being amended to incorporate high-risk AI system requirements?", "answer": "Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, and (EU) 2019/2144 are being amended to include mandatory requirements for high-risk AI systems.", "context": "(47) AI systems could have an adverse impact on the health and safety of persons, in particular when such systems\noperate as safety components of products. Consistent with the objectives of Union harmonisation legislation to\nfacilitate the free movement of products in the internal market and to ensure that only safe and otherwise compliant\nproducts find their way into the market, it is important that the safety risks that may be generated by a product as\na whole due to its digital components, including AI systems, are duly prevented and mitigated. For instance,\nincreasingly autonomous robots, whether in the context of manufacturing or personal assistance and care should be\nable to safely operate and performs their functions in complex environments. Similarly, in the health sector where\nthe stakes for life and health are particularly high, increasingly sophisticated diagnostics systems and systems\nsupporting human decisions should be reliable and accurate.\n\n(48) The extent of the adverse impact caused by the AI system on the fundamental rights protected by the Charter is of\nparticular relevance when classifying an AI system as high risk. Those rights include the right to human digni...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0007", "generated_at": "2025-05-14T16:28:02.012892", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "8506171f-74d0-4097-8591-29a5194b12af", "question": "Under what conditions are AI systems classified as high-risk when they are safety components of products?", "answer": "AI systems are classified as high-risk if the product undergoes conformity assessment with a third-party body under relevant Union harmonisation legislation, such as machinery, medical devices, or automotive systems.", "context": "(47) AI systems could have an adverse impact on the health and safety of persons, in particular when such systems\noperate as safety components of products. Consistent with the objectives of Union harmonisation legislation to\nfacilitate the free movement of products in the internal market and to ensure that only safe and otherwise compliant\nproducts find their way into the market, it is important that the safety risks that may be generated by a product as\na whole due to its digital components, including AI systems, are duly prevented and mitigated. For instance,\nincreasingly autonomous robots, whether in the context of manufacturing or personal assistance and care should be\nable to safely operate and performs their functions in complex environments. Similarly, in the health sector where\nthe stakes for life and health are particularly high, increasingly sophisticated diagnostics systems and systems\nsupporting human decisions should be reliable and accurate.\n\n(48) The extent of the adverse impact caused by the AI system on the fundamental rights protected by the Charter is of\nparticular relevance when classifying an AI system as high risk. Those rights include the right to human digni...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0007", "generated_at": "2025-05-14T16:28:02.014050", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "6c6b17ef-2e45-4edc-865b-7e19d16c7b21", "question": "What criteria define stand-alone high-risk AI systems?", "answer": "Stand-alone AI systems are classified as high-risk if they pose significant harm to health, safety, or fundamental rights, considering the severity and probability of harm, and are used in pre-defined high-risk areas.", "context": "(47) AI systems could have an adverse impact on the health and safety of persons, in particular when such systems\noperate as safety components of products. Consistent with the objectives of Union harmonisation legislation to\nfacilitate the free movement of products in the internal market and to ensure that only safe and otherwise compliant\nproducts find their way into the market, it is important that the safety risks that may be generated by a product as\na whole due to its digital components, including AI systems, are duly prevented and mitigated. For instance,\nincreasingly autonomous robots, whether in the context of manufacturing or personal assistance and care should be\nable to safely operate and performs their functions in complex environments. Similarly, in the health sector where\nthe stakes for life and health are particularly high, increasingly sophisticated diagnostics systems and systems\nsupporting human decisions should be reliable and accurate.\n\n(48) The extent of the adverse impact caused by the AI system on the fundamental rights protected by the Charter is of\nparticular relevance when classifying an AI system as high risk. Those rights include the right to human digni...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0007", "generated_at": "2025-05-14T16:28:02.015309", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "1ed029af-511d-4a04-80b4-00c8f300dae8", "question": "What are the conditions for an AI system to be considered non-high-risk under the Regulation?", "answer": "The AI system must (1) improve previously completed human activities with lowered risk, (2) detect decision-making patterns without replacing human review, (3) perform preparatory tasks for assessments, or (4) be used in low-impact preparatory functions like file handling or translation.", "context": "that the task performed by the AI system is intended to improve the result of a previously completed human activity\nthat may be relevant for the purposes of the high-risk uses listed in an annex to this Regulation. Considering those\ncharacteristics, the AI system provides only an additional layer to a human activity with consequently lowered risk.\nThat condition would, for example, apply to AI systems that are intended to improve the language used in previously\ndrafted documents, for example in relation to professional tone, academic style of language or by aligning text to\na certain brand messaging. The third condition should be that the AI system is intended to detect decision-making\npatterns or deviations from prior decision-making patterns. The risk would be lowered because the use of the AI\nsystem follows a previously completed human assessment which it is not meant to replace or influence, without\nproper human review. Such AI systems include for instance those that, given a certain grading pattern of a teacher,\ncan be used to check ex post whether the teacher may have deviated from the grading pattern so as to flag potential\ninconsistencies or anomalies. The fourth condition ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0008", "generated_at": "2025-05-14T16:28:57.679615", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "3f964afb-eae0-41b5-bed6-a16693b337a6", "question": "Which AI systems are classified as high-risk due to biometric data use?", "answer": "AI systems for remote biometric identification, biometric categorisation based on sensitive attributes, and emotion recognition (unless prohibited) are high-risk. Verification systems for authentication or cybersecurity purposes are excluded.", "context": "that the task performed by the AI system is intended to improve the result of a previously completed human activity\nthat may be relevant for the purposes of the high-risk uses listed in an annex to this Regulation. Considering those\ncharacteristics, the AI system provides only an additional layer to a human activity with consequently lowered risk.\nThat condition would, for example, apply to AI systems that are intended to improve the language used in previously\ndrafted documents, for example in relation to professional tone, academic style of language or by aligning text to\na certain brand messaging. The third condition should be that the AI system is intended to detect decision-making\npatterns or deviations from prior decision-making patterns. The risk would be lowered because the use of the AI\nsystem follows a previously completed human assessment which it is not meant to replace or influence, without\nproper human review. Such AI systems include for instance those that, given a certain grading pattern of a teacher,\ncan be used to check ex post whether the teacher may have deviated from the grading pattern so as to flag potential\ninconsistencies or anomalies. The fourth condition ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0008", "generated_at": "2025-05-14T16:28:57.680986", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "7544904d-2700-4093-81f7-1b9994353907", "question": "What are examples of safety components in critical infrastructure?", "answer": "Safety components include systems for monitoring water pressure, fire alarm controlling systems in cloud computing centres, and systems directly protecting the physical integrity of critical infrastructure.", "context": "that the task performed by the AI system is intended to improve the result of a previously completed human activity\nthat may be relevant for the purposes of the high-risk uses listed in an annex to this Regulation. Considering those\ncharacteristics, the AI system provides only an additional layer to a human activity with consequently lowered risk.\nThat condition would, for example, apply to AI systems that are intended to improve the language used in previously\ndrafted documents, for example in relation to professional tone, academic style of language or by aligning text to\na certain brand messaging. The third condition should be that the AI system is intended to detect decision-making\npatterns or deviations from prior decision-making patterns. The risk would be lowered because the use of the AI\nsystem follows a previously completed human assessment which it is not meant to replace or influence, without\nproper human review. Such AI systems include for instance those that, given a certain grading pattern of a teacher,\ncan be used to check ex post whether the teacher may have deviated from the grading pattern so as to flag potential\ninconsistencies or anomalies. The fourth condition ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0008", "generated_at": "2025-05-14T16:28:57.682186", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "2467836a-c45a-4b50-8f39-432653eb47a4", "question": "Why are AI systems in education classified as high-risk?", "answer": "They may determine educational and professional paths, affect livelihoods, and perpetuate discrimination against groups like women, persons with disabilities, or racial minorities if improperly designed.", "context": "that the task performed by the AI system is intended to improve the result of a previously completed human activity\nthat may be relevant for the purposes of the high-risk uses listed in an annex to this Regulation. Considering those\ncharacteristics, the AI system provides only an additional layer to a human activity with consequently lowered risk.\nThat condition would, for example, apply to AI systems that are intended to improve the language used in previously\ndrafted documents, for example in relation to professional tone, academic style of language or by aligning text to\na certain brand messaging. The third condition should be that the AI system is intended to detect decision-making\npatterns or deviations from prior decision-making patterns. The risk would be lowered because the use of the AI\nsystem follows a previously completed human assessment which it is not meant to replace or influence, without\nproper human review. Such AI systems include for instance those that, given a certain grading pattern of a teacher,\ncan be used to check ex post whether the teacher may have deviated from the grading pattern so as to flag potential\ninconsistencies or anomalies. The fourth condition ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0008", "generated_at": "2025-05-14T16:28:57.683359", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "f15682c5-a954-42bb-83bf-d1c10db69280", "question": "What are the implications of using AI systems for public assistance benefits?", "answer": "Such systems may significantly impact livelihoods and infringe on rights like social protection and non-discrimination, requiring classification as high-risk due to potential discriminatory impacts.", "context": "that the task performed by the AI system is intended to improve the result of a previously completed human activity\nthat may be relevant for the purposes of the high-risk uses listed in an annex to this Regulation. Considering those\ncharacteristics, the AI system provides only an additional layer to a human activity with consequently lowered risk.\nThat condition would, for example, apply to AI systems that are intended to improve the language used in previously\ndrafted documents, for example in relation to professional tone, academic style of language or by aligning text to\na certain brand messaging. The third condition should be that the AI system is intended to detect decision-making\npatterns or deviations from prior decision-making patterns. The risk would be lowered because the use of the AI\nsystem follows a previously completed human assessment which it is not meant to replace or influence, without\nproper human review. Such AI systems include for instance those that, given a certain grading pattern of a teacher,\ncan be used to check ex post whether the teacher may have deviated from the grading pattern so as to flag potential\ninconsistencies or anomalies. The fourth condition ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0008", "generated_at": "2025-05-14T16:28:57.684526", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "4ed3801d-b636-4543-a7b3-f829a4cdc65e", "question": "What is the purpose of the risk-management system for high-risk AI systems?", "answer": "The risk-management system should be a continuous, iterative process throughout the entire lifecycle of a high-risk AI system, aimed at identifying and mitigating risks to health, safety, and fundamental rights.", "context": "requirements set out in the relevant Union harmonised legislation, as that sectoral law does not deal with risks\nspecific to AI systems. This calls for a simultaneous and complementary application of the various legislative acts. To\nensure consistency and to avoid an unnecessary administrative burden and unnecessary costs, providers of a product\nthat contains one or more high-risk AI system, to which the requirements of this Regulation and of the Union\nharmonisation legislation based on the New Legislative Framework and listed in an annex to this Regulation apply,\nshould have flexibility with regard to operational decisions on how to ensure compliance of a product that contains\none or more AI systems with all the applicable requirements of that Union harmonised legislation in an optimal\nmanner. That flexibility could mean, for example a decision by the provider to integrate a part of the necessary\ntesting and reporting processes, information and documentation required under this Regulation into already existing\ndocumentation and procedures required under existing Union harmonisation legislation based on the New\nLegislative Framework and listed in an annex to this Regulation. This s...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0010", "generated_at": "2025-05-14T16:30:41.183805", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "10822559-9258-47a4-a122-5eff08096a8d", "question": "What requirements apply to high-risk AI systems regarding data and transparency?", "answer": "Requirements include ensuring high-quality, representative data sets, transparency in data governance, and providing deployers with instructions for use that address risks and foreseeable misuse.", "context": "requirements set out in the relevant Union harmonised legislation, as that sectoral law does not deal with risks\nspecific to AI systems. This calls for a simultaneous and complementary application of the various legislative acts. To\nensure consistency and to avoid an unnecessary administrative burden and unnecessary costs, providers of a product\nthat contains one or more high-risk AI system, to which the requirements of this Regulation and of the Union\nharmonisation legislation based on the New Legislative Framework and listed in an annex to this Regulation apply,\nshould have flexibility with regard to operational decisions on how to ensure compliance of a product that contains\none or more AI systems with all the applicable requirements of that Union harmonised legislation in an optimal\nmanner. That flexibility could mean, for example a decision by the provider to integrate a part of the necessary\ntesting and reporting processes, information and documentation required under this Regulation into already existing\ndocumentation and procedures required under existing Union harmonisation legislation based on the New\nLegislative Framework and listed in an annex to this Regulation. This s...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0010", "generated_at": "2025-05-14T16:30:41.185365", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "a6a52028-64f5-44a8-918a-406ed92ca11f", "question": "How should providers ensure compliance with Union harmonised legislation for AI systems?", "answer": "Providers should integrate testing, reporting, and documentation processes required under this Regulation into existing Union harmonisation legislation procedures to ensure optimal compliance without duplicative efforts.", "context": "requirements set out in the relevant Union harmonised legislation, as that sectoral law does not deal with risks\nspecific to AI systems. This calls for a simultaneous and complementary application of the various legislative acts. To\nensure consistency and to avoid an unnecessary administrative burden and unnecessary costs, providers of a product\nthat contains one or more high-risk AI system, to which the requirements of this Regulation and of the Union\nharmonisation legislation based on the New Legislative Framework and listed in an annex to this Regulation apply,\nshould have flexibility with regard to operational decisions on how to ensure compliance of a product that contains\none or more AI systems with all the applicable requirements of that Union harmonised legislation in an optimal\nmanner. That flexibility could mean, for example a decision by the provider to integrate a part of the necessary\ntesting and reporting processes, information and documentation required under this Regulation into already existing\ndocumentation and procedures required under existing Union harmonisation legislation based on the New\nLegislative Framework and listed in an annex to this Regulation. This s...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0010", "generated_at": "2025-05-14T16:30:41.186843", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "c7dd708c-2039-488b-ac7d-cb9929a177c8", "question": "What role do European common data spaces play in AI system development?", "answer": "European common data spaces facilitate non-discriminatory, privacy-preserving access to high-quality data for training, validation, and testing AI systems, supported by institutional governance and data sharing frameworks.", "context": "requirements set out in the relevant Union harmonised legislation, as that sectoral law does not deal with risks\nspecific to AI systems. This calls for a simultaneous and complementary application of the various legislative acts. To\nensure consistency and to avoid an unnecessary administrative burden and unnecessary costs, providers of a product\nthat contains one or more high-risk AI system, to which the requirements of this Regulation and of the Union\nharmonisation legislation based on the New Legislative Framework and listed in an annex to this Regulation apply,\nshould have flexibility with regard to operational decisions on how to ensure compliance of a product that contains\none or more AI systems with all the applicable requirements of that Union harmonised legislation in an optimal\nmanner. That flexibility could mean, for example a decision by the provider to integrate a part of the necessary\ntesting and reporting processes, information and documentation required under this Regulation into already existing\ndocumentation and procedures required under existing Union harmonisation legislation based on the New\nLegislative Framework and listed in an annex to this Regulation. This s...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0010", "generated_at": "2025-05-14T16:30:41.188234", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "3ad02674-101b-41d8-8429-a79c8d1b2a8d", "question": "What technical documentation is required for high-risk AI systems?", "answer": "Technical documentation must include system characteristics, algorithms, data sources, risk-management processes, and testing procedures, maintained up to date throughout the system's lifecycle to enable compliance verification and post-market monitoring.", "context": "requirements set out in the relevant Union harmonised legislation, as that sectoral law does not deal with risks\nspecific to AI systems. This calls for a simultaneous and complementary application of the various legislative acts. To\nensure consistency and to avoid an unnecessary administrative burden and unnecessary costs, providers of a product\nthat contains one or more high-risk AI system, to which the requirements of this Regulation and of the Union\nharmonisation legislation based on the New Legislative Framework and listed in an annex to this Regulation apply,\nshould have flexibility with regard to operational decisions on how to ensure compliance of a product that contains\none or more AI systems with all the applicable requirements of that Union harmonised legislation in an optimal\nmanner. That flexibility could mean, for example a decision by the provider to integrate a part of the necessary\ntesting and reporting processes, information and documentation required under this Regulation into already existing\ndocumentation and procedures required under existing Union harmonisation legislation based on the New\nLegislative Framework and listed in an annex to this Regulation. This s...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0010", "generated_at": "2025-05-14T16:30:41.189533", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "14cbf7fb-5fc2-415f-8e96-5568ec9052dc", "question": "What transparency requirements are imposed on high-risk AI systems under the Regulation?", "answer": "High-risk AI systems must be designed to enable deployers to understand their functionality, strengths, and limitations. They must include instructions for use detailing characteristics, capabilities, limitations, potential risks, human oversight measures, and pre-determined changes. Information should be clear, accessible, and available in the language understood by target deployers.", "context": "(72) To address concerns related to opacity and complexity of certain AI systems and help deployers to fulfil their\nobligations under this Regulation, transparency should be required for high-risk AI systems before they are placed\non the market or put it into service. High-risk AI systems should be designed in a manner to enable deployers to\nunderstand how the AI system works, evaluate its functionality, and comprehend its strengths and limitations.\nHigh-risk AI systems should be accompanied by appropriate information in the form of instructions of use. Such\ninformation should include the characteristics, capabilities and limitations of performance of the AI system. Those\nwould cover information on possible known and foreseeable circumstances related to the use of the high-risk AI\nsystem, including deployer action that may influence system behaviour and performance, under which the AI system\ncan lead to risks to health, safety, and fundamental rights, on the changes that have been pre-determined and\nassessed for conformity by the provider and on the relevant human oversight measures, including the measures to\nfacilitate the interpretation of the outputs of the AI system by the depl...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0011", "generated_at": "2025-05-14T16:31:42.325418", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "90d4c46e-3e3e-428b-b291-e975627c87da", "question": "What human oversight measures are required for high-risk AI systems?", "answer": "Providers must implement measures ensuring natural persons can oversee system functioning, including in-built operational constraints that cannot be overridden, training for operators, and mechanisms to guide intervention. Biometric systems must have decisions verified by at least two natural persons unless disproportionate under Union or national law.", "context": "(72) To address concerns related to opacity and complexity of certain AI systems and help deployers to fulfil their\nobligations under this Regulation, transparency should be required for high-risk AI systems before they are placed\non the market or put it into service. High-risk AI systems should be designed in a manner to enable deployers to\nunderstand how the AI system works, evaluate its functionality, and comprehend its strengths and limitations.\nHigh-risk AI systems should be accompanied by appropriate information in the form of instructions of use. Such\ninformation should include the characteristics, capabilities and limitations of performance of the AI system. Those\nwould cover information on possible known and foreseeable circumstances related to the use of the high-risk AI\nsystem, including deployer action that may influence system behaviour and performance, under which the AI system\ncan lead to risks to health, safety, and fundamental rights, on the changes that have been pre-determined and\nassessed for conformity by the provider and on the relevant human oversight measures, including the measures to\nfacilitate the interpretation of the outputs of the AI system by the depl...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0011", "generated_at": "2025-05-14T16:31:42.326814", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "b120b8a8-08e6-4cad-82ea-fff76026c975", "question": "How should performance metrics for high-risk AI systems be communicated?", "answer": "Providers must declare expected performance metrics in instructions for use, ensuring clarity and avoiding misunderstandings. This includes accuracy, robustness, and cybersecurity standards aligned with the state of the art, with collaboration on benchmarks and measurement methodologies.", "context": "(72) To address concerns related to opacity and complexity of certain AI systems and help deployers to fulfil their\nobligations under this Regulation, transparency should be required for high-risk AI systems before they are placed\non the market or put it into service. High-risk AI systems should be designed in a manner to enable deployers to\nunderstand how the AI system works, evaluate its functionality, and comprehend its strengths and limitations.\nHigh-risk AI systems should be accompanied by appropriate information in the form of instructions of use. Such\ninformation should include the characteristics, capabilities and limitations of performance of the AI system. Those\nwould cover information on possible known and foreseeable circumstances related to the use of the high-risk AI\nsystem, including deployer action that may influence system behaviour and performance, under which the AI system\ncan lead to risks to health, safety, and fundamental rights, on the changes that have been pre-determined and\nassessed for conformity by the provider and on the relevant human oversight measures, including the measures to\nfacilitate the interpretation of the outputs of the AI system by the depl...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0011", "generated_at": "2025-05-14T16:31:42.328211", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "0326ad21-52a3-4a71-91b0-fd0bdfd12666", "question": "What technical measures are required to ensure robustness of high-risk AI systems?", "answer": "Technical and organisational measures must prevent harmful behaviour, such as errors or biases. Systems should include fail-safe mechanisms to safely interrupt operations when anomalies occur or boundaries are exceeded.", "context": "(72) To address concerns related to opacity and complexity of certain AI systems and help deployers to fulfil their\nobligations under this Regulation, transparency should be required for high-risk AI systems before they are placed\non the market or put it into service. High-risk AI systems should be designed in a manner to enable deployers to\nunderstand how the AI system works, evaluate its functionality, and comprehend its strengths and limitations.\nHigh-risk AI systems should be accompanied by appropriate information in the form of instructions of use. Such\ninformation should include the characteristics, capabilities and limitations of performance of the AI system. Those\nwould cover information on possible known and foreseeable circumstances related to the use of the high-risk AI\nsystem, including deployer action that may influence system behaviour and performance, under which the AI system\ncan lead to risks to health, safety, and fundamental rights, on the changes that have been pre-determined and\nassessed for conformity by the provider and on the relevant human oversight measures, including the measures to\nfacilitate the interpretation of the outputs of the AI system by the depl...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0011", "generated_at": "2025-05-14T16:31:42.329567", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "5fb17638-66d8-4692-ac4c-672755fc1d1c", "question": "What cybersecurity requirements apply to high-risk AI systems?", "answer": "Providers must implement security controls to protect against cyberattacks exploiting vulnerabilities, including data poisoning and adversarial attacks. Cybersecurity compliance under horizontal regulations may satisfy this Regulation's requirements, with conformity assessments considering risks to AI systems' resilience and fundamental rights.", "context": "(72) To address concerns related to opacity and complexity of certain AI systems and help deployers to fulfil their\nobligations under this Regulation, transparency should be required for high-risk AI systems before they are placed\non the market or put it into service. High-risk AI systems should be designed in a manner to enable deployers to\nunderstand how the AI system works, evaluate its functionality, and comprehend its strengths and limitations.\nHigh-risk AI systems should be accompanied by appropriate information in the form of instructions of use. Such\ninformation should include the characteristics, capabilities and limitations of performance of the AI system. Those\nwould cover information on possible known and foreseeable circumstances related to the use of the high-risk AI\nsystem, including deployer action that may influence system behaviour and performance, under which the AI system\ncan lead to risks to health, safety, and fundamental rights, on the changes that have been pre-determined and\nassessed for conformity by the provider and on the relevant human oversight measures, including the measures to\nfacilitate the interpretation of the outputs of the AI system by the depl...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0011", "generated_at": "2025-05-14T16:31:42.331174", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "0850a34d-66a4-4501-8c7a-867de3ef4f3b", "question": "Who is responsible for ensuring compliance with accessibility requirements for high-risk AI systems?", "answer": "Providers of high-risk AI systems must ensure compliance with accessibility requirements, including Directive (EU) 2016/2102 and Directive (EU) 2019/882, by design.", "context": "(^37 ) Regulation (EU) 2019/881 of the European Parliament and of the Council of 17 April 2019 on ENISA (the European Union Agency\nfor Cybersecurity) and on information and communications technology cybersecurity certification and repealing Regulation\n(EU) No 526/2013 (Cybersecurity Act) (OJ L 151, 7.6.2019, p. 15).\n\n(79) It is appropriate that a specific natural or legal person, defined as the provider, takes responsibility for the placing on\nthe market or the putting into service of a high-risk AI system, regardless of whether that natural or legal person is\nthe person who designed or developed the system.\n\n(80) As signatories to the United Nations Convention on the Rights of Persons with Disabilities, the Union and the\nMember States are legally obliged to protect persons with disabilities from discrimination and promote their equality,\nto ensure that persons with disabilities have access, on an equal basis with others, to information and\ncommunications technologies and systems, and to ensure respect for privacy for persons with disabilities. Given the\ngrowing importance and use of AI systems, the application of universal design principles to all new technologies and\nservices sho...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0012", "generated_at": "2025-05-14T16:32:21.450667", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "31b69bd8-69b7-4b30-8f8a-164818731729", "question": "What is the role of an authorized representative for non-EU providers of high-risk AI systems?", "answer": "An authorized representative established in the Union must be appointed by providers in third countries to ensure compliance with the Regulation and act as their contact person within the Union.", "context": "(^37 ) Regulation (EU) 2019/881 of the European Parliament and of the Council of 17 April 2019 on ENISA (the European Union Agency\nfor Cybersecurity) and on information and communications technology cybersecurity certification and repealing Regulation\n(EU) No 526/2013 (Cybersecurity Act) (OJ L 151, 7.6.2019, p. 15).\n\n(79) It is appropriate that a specific natural or legal person, defined as the provider, takes responsibility for the placing on\nthe market or the putting into service of a high-risk AI system, regardless of whether that natural or legal person is\nthe person who designed or developed the system.\n\n(80) As signatories to the United Nations Convention on the Rights of Persons with Disabilities, the Union and the\nMember States are legally obliged to protect persons with disabilities from discrimination and promote their equality,\nto ensure that persons with disabilities have access, on an equal basis with others, to information and\ncommunications technologies and systems, and to ensure respect for privacy for persons with disabilities. Given the\ngrowing importance and use of AI systems, the application of universal design principles to all new technologies and\nservices sho...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0012", "generated_at": "2025-05-14T16:32:21.452096", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "e99c2ba1-b760-4d30-80a2-746a40284e9a", "question": "Under what conditions can distributors or importers be considered providers of high-risk AI systems?", "answer": "Distributors, importers, deployers, or other third parties may be considered providers if they modify a high-risk AI system, change its intended purpose, or affix their name/trademark to it, unless contractual arrangements specify otherwise.", "context": "(^37 ) Regulation (EU) 2019/881 of the European Parliament and of the Council of 17 April 2019 on ENISA (the European Union Agency\nfor Cybersecurity) and on information and communications technology cybersecurity certification and repealing Regulation\n(EU) No 526/2013 (Cybersecurity Act) (OJ L 151, 7.6.2019, p. 15).\n\n(79) It is appropriate that a specific natural or legal person, defined as the provider, takes responsibility for the placing on\nthe market or the putting into service of a high-risk AI system, regardless of whether that natural or legal person is\nthe person who designed or developed the system.\n\n(80) As signatories to the United Nations Convention on the Rights of Persons with Disabilities, the Union and the\nMember States are legally obliged to protect persons with disabilities from discrimination and promote their equality,\nto ensure that persons with disabilities have access, on an equal basis with others, to information and\ncommunications technologies and systems, and to ensure respect for privacy for persons with disabilities. Given the\ngrowing importance and use of AI systems, the application of universal design principles to all new technologies and\nservices sho...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0012", "generated_at": "2025-05-14T16:32:21.453286", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "3a8e3b72-df47-4ed1-96ea-1b9603dd8c9f", "question": "What obligations do deployers of high-risk AI systems have regarding monitoring and record-keeping?", "answer": "Deployers must take technical and organizational measures to ensure proper use of high-risk AI systems, monitor their functioning, maintain records, and ensure personnel have adequate AI literacy and authority to fulfill tasks.", "context": "(^37 ) Regulation (EU) 2019/881 of the European Parliament and of the Council of 17 April 2019 on ENISA (the European Union Agency\nfor Cybersecurity) and on information and communications technology cybersecurity certification and repealing Regulation\n(EU) No 526/2013 (Cybersecurity Act) (OJ L 151, 7.6.2019, p. 15).\n\n(79) It is appropriate that a specific natural or legal person, defined as the provider, takes responsibility for the placing on\nthe market or the putting into service of a high-risk AI system, regardless of whether that natural or legal person is\nthe person who designed or developed the system.\n\n(80) As signatories to the United Nations Convention on the Rights of Persons with Disabilities, the Union and the\nMember States are legally obliged to protect persons with disabilities from discrimination and promote their equality,\nto ensure that persons with disabilities have access, on an equal basis with others, to information and\ncommunications technologies and systems, and to ensure respect for privacy for persons with disabilities. Given the\ngrowing importance and use of AI systems, the application of universal design principles to all new technologies and\nservices sho...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0012", "generated_at": "2025-05-14T16:32:21.454469", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "a4589802-5d82-4a7a-a9fa-7d76d5733995", "question": "How should third parties contribute to the compliance of high-risk AI systems along the value chain?", "answer": "Third parties must provide necessary information, technical access, and assistance to providers through written agreements, without compromising intellectual property rights, to enable compliance with the Regulation.", "context": "(^37 ) Regulation (EU) 2019/881 of the European Parliament and of the Council of 17 April 2019 on ENISA (the European Union Agency\nfor Cybersecurity) and on information and communications technology cybersecurity certification and repealing Regulation\n(EU) No 526/2013 (Cybersecurity Act) (OJ L 151, 7.6.2019, p. 15).\n\n(79) It is appropriate that a specific natural or legal person, defined as the provider, takes responsibility for the placing on\nthe market or the putting into service of a high-risk AI system, regardless of whether that natural or legal person is\nthe person who designed or developed the system.\n\n(80) As signatories to the United Nations Convention on the Rights of Persons with Disabilities, the Union and the\nMember States are legally obliged to protect persons with disabilities from discrimination and promote their equality,\nto ensure that persons with disabilities have access, on an equal basis with others, to information and\ncommunications technologies and systems, and to ensure respect for privacy for persons with disabilities. Given the\ngrowing importance and use of AI systems, the application of universal design principles to all new technologies and\nservices sho...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0012", "generated_at": "2025-05-14T16:32:21.455794", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "81a68151-0a3c-4dba-a57a-fb27d8ccb312", "question": "What obligations does the Regulation impose on employers regarding workers' information on high-risk AI systems?", "answer": "The Regulation requires employers to inform workers and their representatives about the planned deployment of high-risk AI systems, especially when other legal instruments do not fulfill the information or consultation obligations.", "context": "(92) This Regulation is without prejudice to obligations for employers to inform or to inform and consult workers or\ntheir representatives under Union or national law and practice, including Directive 2002/14/EC of the European\nParliament and of the Council (^39 ), on decisions to put into service or use AI systems. It remains necessary to ensure\ninformation of workers and their representatives on the planned deployment of high-risk AI systems at the\nworkplace where the conditions for those information or information and consultation obligations in other legal\ninstruments are not fulfilled. Moreover, such information right is ancillary and necessary to the objective of\nprotecting fundamental rights that underlies this Regulation. Therefore, an information requirement to that effect\nshould be laid down in this Regulation, without affecting any existing rights of workers.\n\n(93) Whilst risks related to AI systems can result from the way such systems are designed, risks can as well stem from\nhow such AI systems are used. Deployers of high-risk AI system therefore play a critical role in ensuring that\nfundamental rights are protected, complementing the obligations of the provider when d...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0013", "generated_at": "2025-05-14T16:34:29.983935", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "25fa472f-23ef-42c7-a9ef-57144d0f4d1f", "question": "What role do deployers play in ensuring fundamental rights when using high-risk AI systems?", "answer": "Deployers are critical in ensuring fundamental rights by identifying potential risks not foreseen during development and informing natural persons about the AI system's use and their right to explanation, especially in law enforcement contexts.", "context": "(92) This Regulation is without prejudice to obligations for employers to inform or to inform and consult workers or\ntheir representatives under Union or national law and practice, including Directive 2002/14/EC of the European\nParliament and of the Council (^39 ), on decisions to put into service or use AI systems. It remains necessary to ensure\ninformation of workers and their representatives on the planned deployment of high-risk AI systems at the\nworkplace where the conditions for those information or information and consultation obligations in other legal\ninstruments are not fulfilled. Moreover, such information right is ancillary and necessary to the objective of\nprotecting fundamental rights that underlies this Regulation. Therefore, an information requirement to that effect\nshould be laid down in this Regulation, without affecting any existing rights of workers.\n\n(93) Whilst risks related to AI systems can result from the way such systems are designed, risks can as well stem from\nhow such AI systems are used. Deployers of high-risk AI system therefore play a critical role in ensuring that\nfundamental rights are protected, complementing the obligations of the provider when d...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0013", "generated_at": "2025-05-14T16:34:29.985272", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "2373fcd1-b4b1-4cf1-bf40-8d88f24ba371", "question": "What compliance requirements apply to biometric data processing in law enforcement AI systems?", "answer": "Processing biometric data for law enforcement must comply with Article 10 of Directive (EU) 2016/680, ensuring it's strictly necessary, lawful, and with appropriate safeguards, and authorized by Union or Member State law.", "context": "(92) This Regulation is without prejudice to obligations for employers to inform or to inform and consult workers or\ntheir representatives under Union or national law and practice, including Directive 2002/14/EC of the European\nParliament and of the Council (^39 ), on decisions to put into service or use AI systems. It remains necessary to ensure\ninformation of workers and their representatives on the planned deployment of high-risk AI systems at the\nworkplace where the conditions for those information or information and consultation obligations in other legal\ninstruments are not fulfilled. Moreover, such information right is ancillary and necessary to the objective of\nprotecting fundamental rights that underlies this Regulation. Therefore, an information requirement to that effect\nshould be laid down in this Regulation, without affecting any existing rights of workers.\n\n(93) Whilst risks related to AI systems can result from the way such systems are designed, risks can as well stem from\nhow such AI systems are used. Deployers of high-risk AI system therefore play a critical role in ensuring that\nfundamental rights are protected, complementing the obligations of the provider when d...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0013", "generated_at": "2025-05-14T16:34:29.986448", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "938aa9cc-f654-4234-9ccf-92403d3ef31b", "question": "What safeguards are required for the use of post-remote biometric identification systems?", "answer": "Post-remote systems must be used proportionately, legitimately, and strictly necessary, targeting specific individuals and locations, avoiding indiscriminate surveillance, and not circumventing real-time remote biometric identification restrictions.", "context": "(92) This Regulation is without prejudice to obligations for employers to inform or to inform and consult workers or\ntheir representatives under Union or national law and practice, including Directive 2002/14/EC of the European\nParliament and of the Council (^39 ), on decisions to put into service or use AI systems. It remains necessary to ensure\ninformation of workers and their representatives on the planned deployment of high-risk AI systems at the\nworkplace where the conditions for those information or information and consultation obligations in other legal\ninstruments are not fulfilled. Moreover, such information right is ancillary and necessary to the objective of\nprotecting fundamental rights that underlies this Regulation. Therefore, an information requirement to that effect\nshould be laid down in this Regulation, without affecting any existing rights of workers.\n\n(93) Whilst risks related to AI systems can result from the way such systems are designed, risks can as well stem from\nhow such AI systems are used. Deployers of high-risk AI system therefore play a critical role in ensuring that\nfundamental rights are protected, complementing the obligations of the provider when d...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0013", "generated_at": "2025-05-14T16:34:29.987596", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "bd0bcd94-54d5-4086-9cbd-054092265506", "question": "What is the purpose of a fundamental rights impact assessment for high-risk AI systems?", "answer": "The impact assessment aims to identify risks to fundamental rights, determine mitigation measures, and ensure compliance with the Regulation, including human oversight and redress procedures, and must be updated when relevant factors change.", "context": "(92) This Regulation is without prejudice to obligations for employers to inform or to inform and consult workers or\ntheir representatives under Union or national law and practice, including Directive 2002/14/EC of the European\nParliament and of the Council (^39 ), on decisions to put into service or use AI systems. It remains necessary to ensure\ninformation of workers and their representatives on the planned deployment of high-risk AI systems at the\nworkplace where the conditions for those information or information and consultation obligations in other legal\ninstruments are not fulfilled. Moreover, such information right is ancillary and necessary to the objective of\nprotecting fundamental rights that underlies this Regulation. Therefore, an information requirement to that effect\nshould be laid down in this Regulation, without affecting any existing rights of workers.\n\n(93) Whilst risks related to AI systems can result from the way such systems are designed, risks can as well stem from\nhow such AI systems are used. Deployers of high-risk AI system therefore play a critical role in ensuring that\nfundamental rights are protected, complementing the obligations of the provider when d...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0013", "generated_at": "2025-05-14T16:34:29.988783", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "56d497e4-1c89-43bb-93a1-0f4ca5ca2a46", "question": "What transparency measures are required for providers of general-purpose AI models?", "answer": "Providers must prepare and maintain up-to-date technical documentation, including information on model architecture, usage, and parameters. This documentation should be available to the AI Office and national authorities, with specific annexes defining the minimal elements. The Commission can update these annexes via delegated acts.", "context": "(101) Providers of general-purpose AI models have a particular role and responsibility along the AI value chain, as the\nmodels they provide may form the basis for a range of downstream systems, often provided by downstream\nproviders that necessitate a good understanding of the models and their capabilities, both to enable the integration of\nsuch models into their products, and to fulfil their obligations under this or other regulations. Therefore,\nproportionate transparency measures should be laid down, including the drawing up and keeping up to date of\ndocumentation, and the provision of information on the general-purpose AI model for its usage by the downstream\nproviders. Technical documentation should be prepared and kept up to date by the general-purpose AI model\nprovider for the purpose of making it available, upon request, to the AI Office and the national competent\nauthorities. The minimal set of elements to be included in such documentation should be set out in specific annexes\nto this Regulation. The Commission should be empowered to amend those annexes by means of delegated acts in\nlight of evolving technological developments.\n\n(102) Software and data, including models, r...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0014", "generated_at": "2025-05-14T16:33:54.489455", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "4829d84a-5a1b-4579-8304-c513ae435605", "question": "Under what conditions can free and open-source licenses exempt AI models from transparency obligations?", "answer": "Models released under free and open-source licenses with publicly available parameters, architecture, and usage info are exempt from some transparency requirements, except for obligations related to training data summaries and copyright compliance policies.", "context": "(101) Providers of general-purpose AI models have a particular role and responsibility along the AI value chain, as the\nmodels they provide may form the basis for a range of downstream systems, often provided by downstream\nproviders that necessitate a good understanding of the models and their capabilities, both to enable the integration of\nsuch models into their products, and to fulfil their obligations under this or other regulations. Therefore,\nproportionate transparency measures should be laid down, including the drawing up and keeping up to date of\ndocumentation, and the provision of information on the general-purpose AI model for its usage by the downstream\nproviders. Technical documentation should be prepared and kept up to date by the general-purpose AI model\nprovider for the purpose of making it available, upon request, to the AI Office and the national competent\nauthorities. The minimal set of elements to be included in such documentation should be set out in specific annexes\nto this Regulation. The Commission should be empowered to amend those annexes by means of delegated acts in\nlight of evolving technological developments.\n\n(102) Software and data, including models, r...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0014", "generated_at": "2025-05-14T16:33:54.491942", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "978280a3-ed7b-4f2e-bdf7-b47485a841b1", "question": "What obligations do AI model providers have regarding copyright compliance?", "answer": "Providers must implement policies to comply with Union copyright law, including respecting rights reservations under Directive (EU) 2019/790. They must also provide summaries of training data used, even if the data includes copyrighted content.", "context": "(101) Providers of general-purpose AI models have a particular role and responsibility along the AI value chain, as the\nmodels they provide may form the basis for a range of downstream systems, often provided by downstream\nproviders that necessitate a good understanding of the models and their capabilities, both to enable the integration of\nsuch models into their products, and to fulfil their obligations under this or other regulations. Therefore,\nproportionate transparency measures should be laid down, including the drawing up and keeping up to date of\ndocumentation, and the provision of information on the general-purpose AI model for its usage by the downstream\nproviders. Technical documentation should be prepared and kept up to date by the general-purpose AI model\nprovider for the purpose of making it available, upon request, to the AI Office and the national competent\nauthorities. The minimal set of elements to be included in such documentation should be set out in specific annexes\nto this Regulation. The Commission should be empowered to amend those annexes by means of delegated acts in\nlight of evolving technological developments.\n\n(102) Software and data, including models, r...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0014", "generated_at": "2025-05-14T16:33:54.496599", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "bc045c51-26b1-48f5-8b3c-bf0c64e474df", "question": "How should AI model providers address systemic risks?", "answer": "Providers must assess risks like disruptions to critical sectors, public safety threats, and misuse potential. Systemic risks increase with model capabilities and reach, requiring attention to factors like model reliability, security, and alignment with human intent.", "context": "(101) Providers of general-purpose AI models have a particular role and responsibility along the AI value chain, as the\nmodels they provide may form the basis for a range of downstream systems, often provided by downstream\nproviders that necessitate a good understanding of the models and their capabilities, both to enable the integration of\nsuch models into their products, and to fulfil their obligations under this or other regulations. Therefore,\nproportionate transparency measures should be laid down, including the drawing up and keeping up to date of\ndocumentation, and the provision of information on the general-purpose AI model for its usage by the downstream\nproviders. Technical documentation should be prepared and kept up to date by the general-purpose AI model\nprovider for the purpose of making it available, upon request, to the AI Office and the national competent\nauthorities. The minimal set of elements to be included in such documentation should be set out in specific annexes\nto this Regulation. The Commission should be empowered to amend those annexes by means of delegated acts in\nlight of evolving technological developments.\n\n(102) Software and data, including models, r...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0014", "generated_at": "2025-05-14T16:33:54.499117", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "f0f45670-04cf-447d-93f6-7171bc92304e", "question": "What compliance adjustments are made for SMEs and non-professional users of AI models?", "answer": "SMEs and non-professional users are exempt from full compliance obligations but are encouraged to voluntarily comply. Simplified methods should be allowed to avoid excessive costs, with obligations limited to modifications or fine-tuning of models.", "context": "(101) Providers of general-purpose AI models have a particular role and responsibility along the AI value chain, as the\nmodels they provide may form the basis for a range of downstream systems, often provided by downstream\nproviders that necessitate a good understanding of the models and their capabilities, both to enable the integration of\nsuch models into their products, and to fulfil their obligations under this or other regulations. Therefore,\nproportionate transparency measures should be laid down, including the drawing up and keeping up to date of\ndocumentation, and the provision of information on the general-purpose AI model for its usage by the downstream\nproviders. Technical documentation should be prepared and kept up to date by the general-purpose AI model\nprovider for the purpose of making it available, upon request, to the AI Office and the national competent\nauthorities. The minimal set of elements to be included in such documentation should be set out in specific annexes\nto this Regulation. The Commission should be empowered to amend those annexes by means of delegated acts in\nlight of evolving technological developments.\n\n(102) Software and data, including models, r...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0014", "generated_at": "2025-05-14T16:33:54.502660", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "4316f6e4-1209-4269-b9ad-e5374dda8660", "question": "What criteria determine if a general-purpose AI model presents systemic risks?", "answer": "A general-purpose AI model is considered to present systemic risks if it has high-impact capabilities, evaluated using technical tools, or significant impact on the internal market due to its reach. High-impact capabilities are defined as matching or exceeding the capabilities of the most advanced general-purpose AI models.", "context": "(111) It is appropriate to establish a methodology for the classification of general-purpose AI models as general-purpose\nAI model with systemic risks. Since systemic risks result from particularly high capabilities, a general-purpose AI\nmodel should be considered to present systemic risks if it has high-impact capabilities, evaluated on the basis of\nappropriate technical tools and methodologies, or significant impact on the internal market due to its reach.\nHigh-impact capabilities in general-purpose AI models means capabilities that match or exceed the capabilities\nrecorded in the most advanced general-purpose AI models. The full range of capabilities in a model could be better\nunderstood after its placing on the market or when deployers interact with the model. According to the state of the\nart at the time of entry into force of this Regulation, the cumulative amount of computation used for the training of\nthe general-purpose AI model measured in floating point operations is one of the relevant approximations for model\ncapabilities. The cumulative amount of computation used for training includes the computation used across the\nactivities and methods that are intended to enhance ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0015", "generated_at": "2025-05-14T16:35:36.027234", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "75268191-399c-4227-9504-8cd7284ce3b5", "question": "What is the initial threshold for computational resources to classify a model as having systemic risks?", "answer": "The cumulative amount of computation used for training, measured in floating point operations, serves as an approximation. An initial threshold is set, which may be adjusted over time to reflect technological and industrial changes.", "context": "(111) It is appropriate to establish a methodology for the classification of general-purpose AI models as general-purpose\nAI model with systemic risks. Since systemic risks result from particularly high capabilities, a general-purpose AI\nmodel should be considered to present systemic risks if it has high-impact capabilities, evaluated on the basis of\nappropriate technical tools and methodologies, or significant impact on the internal market due to its reach.\nHigh-impact capabilities in general-purpose AI models means capabilities that match or exceed the capabilities\nrecorded in the most advanced general-purpose AI models. The full range of capabilities in a model could be better\nunderstood after its placing on the market or when deployers interact with the model. According to the state of the\nart at the time of entry into force of this Regulation, the cumulative amount of computation used for the training of\nthe general-purpose AI model measured in floating point operations is one of the relevant approximations for model\ncapabilities. The cumulative amount of computation used for training includes the computation used across the\nactivities and methods that are intended to enhance ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0015", "generated_at": "2025-05-14T16:35:36.029426", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "14264b0e-0d8d-4412-9144-4e5bae6b7dbc", "question": "What is the notification procedure for providers meeting the systemic risk threshold?", "answer": "Providers must notify the AI Office at least two weeks after the model meets the threshold or becomes known to do so. They may also demonstrate exceptional reasons why the model does not present systemic risks.", "context": "(111) It is appropriate to establish a methodology for the classification of general-purpose AI models as general-purpose\nAI model with systemic risks. Since systemic risks result from particularly high capabilities, a general-purpose AI\nmodel should be considered to present systemic risks if it has high-impact capabilities, evaluated on the basis of\nappropriate technical tools and methodologies, or significant impact on the internal market due to its reach.\nHigh-impact capabilities in general-purpose AI models means capabilities that match or exceed the capabilities\nrecorded in the most advanced general-purpose AI models. The full range of capabilities in a model could be better\nunderstood after its placing on the market or when deployers interact with the model. According to the state of the\nart at the time of entry into force of this Regulation, the cumulative amount of computation used for the training of\nthe general-purpose AI model measured in floating point operations is one of the relevant approximations for model\ncapabilities. The cumulative amount of computation used for training includes the computation used across the\nactivities and methods that are intended to enhance ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0015", "generated_at": "2025-05-14T16:35:36.030651", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "5382d271-ea7f-4bae-a70e-7f44c6b315df", "question": "What cybersecurity obligations apply to providers of general-purpose AI models with systemic risks?", "answer": "Providers must perform model evaluations, including adversarial testing, implement risk-management policies, conduct post-market monitoring, and ensure cybersecurity protection throughout the model's lifecycle, addressing threats like model theft and unauthorized access.", "context": "(111) It is appropriate to establish a methodology for the classification of general-purpose AI models as general-purpose\nAI model with systemic risks. Since systemic risks result from particularly high capabilities, a general-purpose AI\nmodel should be considered to present systemic risks if it has high-impact capabilities, evaluated on the basis of\nappropriate technical tools and methodologies, or significant impact on the internal market due to its reach.\nHigh-impact capabilities in general-purpose AI models means capabilities that match or exceed the capabilities\nrecorded in the most advanced general-purpose AI models. The full range of capabilities in a model could be better\nunderstood after its placing on the market or when deployers interact with the model. According to the state of the\nart at the time of entry into force of this Regulation, the cumulative amount of computation used for the training of\nthe general-purpose AI model measured in floating point operations is one of the relevant approximations for model\ncapabilities. The cumulative amount of computation used for training includes the computation used across the\nactivities and methods that are intended to enhance ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0015", "generated_at": "2025-05-14T16:35:36.032089", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "1ffa1871-a7de-4181-9084-3979708cbb33", "question": "What role do codes of practice play in compliance with AI regulations?", "answer": "Codes of practice are central tools for compliance, covering obligations for providers and systemic risk mitigation. They may be approved by the Commission as harmonized standards, granting presumption of conformity if they meet AI Office criteria.", "context": "(111) It is appropriate to establish a methodology for the classification of general-purpose AI models as general-purpose\nAI model with systemic risks. Since systemic risks result from particularly high capabilities, a general-purpose AI\nmodel should be considered to present systemic risks if it has high-impact capabilities, evaluated on the basis of\nappropriate technical tools and methodologies, or significant impact on the internal market due to its reach.\nHigh-impact capabilities in general-purpose AI models means capabilities that match or exceed the capabilities\nrecorded in the most advanced general-purpose AI models. The full range of capabilities in a model could be better\nunderstood after its placing on the market or when deployers interact with the model. According to the state of the\nart at the time of entry into force of this Regulation, the cumulative amount of computation used for the training of\nthe general-purpose AI model measured in floating point operations is one of the relevant approximations for model\ncapabilities. The cumulative amount of computation used for training includes the computation used across the\nactivities and methods that are intended to enhance ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0015", "generated_at": "2025-05-14T16:35:36.033451", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "350e4456-68b4-45ea-83a2-242c2f4f39cd", "question": "How can AI systems be classified as intermediary services under Union law?", "answer": "AI systems may be provided as intermediary services or parts thereof within the meaning of Regulation (EU) 2022/2065, interpreted in a technology-neutral manner. For example, chatbots performing searches across websites and generating combined outputs qualify as such.", "context": "(119) Considering the quick pace of innovation and the technological evolution of digital services in scope of different\ninstruments of Union law in particular having in mind the usage and the perception of their recipients, the AI\nsystems subject to this Regulation may be provided as intermediary services or parts thereof within the meaning of\nRegulation (EU) 2022/2065, which should be interpreted in a technology-neutral manner. For example, AI systems\nmay be used to provide online search engines, in particular, to the extent that an AI system such as an online chatbot\nperforms searches of, in principle, all websites, then incorporates the results into its existing knowledge and uses the\nupdated knowledge to generate a single output that combines different sources of information.\n\n(120) Furthermore, obligations placed on providers and deployers of certain AI systems in this Regulation to enable the\ndetection and disclosure that the outputs of those systems are artificially generated or manipulated are particularly\nrelevant to facilitate the effective implementation of Regulation (EU) 2022/2065. This applies in particular as regards\nthe obligations of providers of very large online...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0016", "generated_at": "2025-05-14T16:37:17.993492", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "97c84152-6c65-4b4b-a775-9c5f2a43ca20", "question": "What obligations do AI providers have regarding artificially generated content?", "answer": "Providers must enable detection and disclosure of AI-generated or manipulated outputs to facilitate Regulation (EU) 2022/2065. This includes mitigating risks like disinformation affecting democratic processes and civic discourse.", "context": "(119) Considering the quick pace of innovation and the technological evolution of digital services in scope of different\ninstruments of Union law in particular having in mind the usage and the perception of their recipients, the AI\nsystems subject to this Regulation may be provided as intermediary services or parts thereof within the meaning of\nRegulation (EU) 2022/2065, which should be interpreted in a technology-neutral manner. For example, AI systems\nmay be used to provide online search engines, in particular, to the extent that an AI system such as an online chatbot\nperforms searches of, in principle, all websites, then incorporates the results into its existing knowledge and uses the\nupdated knowledge to generate a single output that combines different sources of information.\n\n(120) Furthermore, obligations placed on providers and deployers of certain AI systems in this Regulation to enable the\ndetection and disclosure that the outputs of those systems are artificially generated or manipulated are particularly\nrelevant to facilitate the effective implementation of Regulation (EU) 2022/2065. This applies in particular as regards\nthe obligations of providers of very large online...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0016", "generated_at": "2025-05-14T16:37:17.994843", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "d2ad72ed-fc2c-45ad-b188-b28d345a79d4", "question": "What role does standardization play in ensuring compliance with AI regulations?", "answer": "Standardization provides technical solutions for compliance with this Regulation, using harmonized standards under Regulation (EU) No 1025/2012. The Commission should issue standardization requests promptly and consult stakeholders.", "context": "(119) Considering the quick pace of innovation and the technological evolution of digital services in scope of different\ninstruments of Union law in particular having in mind the usage and the perception of their recipients, the AI\nsystems subject to this Regulation may be provided as intermediary services or parts thereof within the meaning of\nRegulation (EU) 2022/2065, which should be interpreted in a technology-neutral manner. For example, AI systems\nmay be used to provide online search engines, in particular, to the extent that an AI system such as an online chatbot\nperforms searches of, in principle, all websites, then incorporates the results into its existing knowledge and uses the\nupdated knowledge to generate a single output that combines different sources of information.\n\n(120) Furthermore, obligations placed on providers and deployers of certain AI systems in this Regulation to enable the\ndetection and disclosure that the outputs of those systems are artificially generated or manipulated are particularly\nrelevant to facilitate the effective implementation of Regulation (EU) 2022/2065. This applies in particular as regards\nthe obligations of providers of very large online...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0016", "generated_at": "2025-05-14T16:37:17.996044", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "27b84477-acdd-48f1-8fa1-9f1183f4c726", "question": "Under what conditions can high-risk AI systems be presumed compliant?", "answer": "High-risk AI systems trained on specific data reflecting their intended use context are presumed compliant with data governance requirements. Cybersecurity-certified systems are also presumed compliant if their certificates cover relevant requirements.", "context": "(119) Considering the quick pace of innovation and the technological evolution of digital services in scope of different\ninstruments of Union law in particular having in mind the usage and the perception of their recipients, the AI\nsystems subject to this Regulation may be provided as intermediary services or parts thereof within the meaning of\nRegulation (EU) 2022/2065, which should be interpreted in a technology-neutral manner. For example, AI systems\nmay be used to provide online search engines, in particular, to the extent that an AI system such as an online chatbot\nperforms searches of, in principle, all websites, then incorporates the results into its existing knowledge and uses the\nupdated knowledge to generate a single output that combines different sources of information.\n\n(120) Furthermore, obligations placed on providers and deployers of certain AI systems in this Regulation to enable the\ndetection and disclosure that the outputs of those systems are artificially generated or manipulated are particularly\nrelevant to facilitate the effective implementation of Regulation (EU) 2022/2065. This applies in particular as regards\nthe obligations of providers of very large online...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0016", "generated_at": "2025-05-14T16:37:17.997226", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "a8ab5773-01e1-4f48-832d-c7b0fabb3a98", "question": "What is the significance of the CE marking for high-risk AI systems?", "answer": "High-risk AI systems must bear the CE marking to indicate conformity with this Regulation, enabling free movement within the internal market. Physical or digital CE markings are required, and Member States must not create unjustified obstacles.", "context": "(119) Considering the quick pace of innovation and the technological evolution of digital services in scope of different\ninstruments of Union law in particular having in mind the usage and the perception of their recipients, the AI\nsystems subject to this Regulation may be provided as intermediary services or parts thereof within the meaning of\nRegulation (EU) 2022/2065, which should be interpreted in a technology-neutral manner. For example, AI systems\nmay be used to provide online search engines, in particular, to the extent that an AI system such as an online chatbot\nperforms searches of, in principle, all websites, then incorporates the results into its existing knowledge and uses the\nupdated knowledge to generate a single output that combines different sources of information.\n\n(120) Furthermore, obligations placed on providers and deployers of certain AI systems in this Regulation to enable the\ndetection and disclosure that the outputs of those systems are artificially generated or manipulated are particularly\nrelevant to facilitate the effective implementation of Regulation (EU) 2022/2065. This applies in particular as regards\nthe obligations of providers of very large online...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0016", "generated_at": "2025-05-14T16:37:17.998420", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "65ac41bd-03c2-4599-a26a-04080fb89ccb", "question": "What is the purpose of the EU database for high-risk AI systems?", "answer": "To facilitate the work of the Commission and Member States, increase transparency, and require providers and deployers to register AI systems, with public access and specific security measures for law enforcement and critical infrastructure.", "context": "(131) In order to facilitate the work of the Commission and the Member States in the AI field as well as to increase the\ntransparency towards the public, providers of high-risk AI systems other than those related to products falling within\nthe scope of relevant existing Union harmonisation legislation, as well as providers who consider that an AI system\nlisted in the high-risk use cases in an annex to this Regulation is not high-risk on the basis of a derogation, should be\nrequired to register themselves and information about their AI system in an EU database, to be established and\nmanaged by the Commission. Before using an AI system listed in the high-risk use cases in an annex to this\nRegulation, deployers of high-risk AI systems that are public authorities, agencies or bodies, should register\nthemselves in such database and select the system that they envisage to use. Other deployers should be entitled to do\nso voluntarily. This section of the EU database should be publicly accessible, free of charge, the information should\nbe easily navigable, understandable and machine-readable. The EU database should also be user-friendly, for example\nby providing search functionalities, incl...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0017", "generated_at": "2025-05-14T16:36:59.124772", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "28ad8874-b273-4604-942f-fd073953d586", "question": "What transparency obligations apply to AI systems interacting with natural persons?", "answer": "Providers must notify users they are interacting with an AI system, unless obvious, and disclose AI-generated content that infers emotions or categories, with accessible formats for persons with disabilities.", "context": "(131) In order to facilitate the work of the Commission and the Member States in the AI field as well as to increase the\ntransparency towards the public, providers of high-risk AI systems other than those related to products falling within\nthe scope of relevant existing Union harmonisation legislation, as well as providers who consider that an AI system\nlisted in the high-risk use cases in an annex to this Regulation is not high-risk on the basis of a derogation, should be\nrequired to register themselves and information about their AI system in an EU database, to be established and\nmanaged by the Commission. Before using an AI system listed in the high-risk use cases in an annex to this\nRegulation, deployers of high-risk AI systems that are public authorities, agencies or bodies, should register\nthemselves in such database and select the system that they envisage to use. Other deployers should be entitled to do\nso voluntarily. This section of the EU database should be publicly accessible, free of charge, the information should\nbe easily navigable, understandable and machine-readable. The EU database should also be user-friendly, for example\nby providing search functionalities, incl...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0017", "generated_at": "2025-05-14T16:36:59.126098", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "7c44adb2-bfe0-4c40-aaa9-078bd951f5d6", "question": "What technical solutions are required for marking synthetic AI-generated content?", "answer": "Providers must embed reliable, interoperable methods like watermarks, metadata, cryptographic techniques, or fingerprints to detect AI-generated content, excluding assistive or non-altering systems.", "context": "(131) In order to facilitate the work of the Commission and the Member States in the AI field as well as to increase the\ntransparency towards the public, providers of high-risk AI systems other than those related to products falling within\nthe scope of relevant existing Union harmonisation legislation, as well as providers who consider that an AI system\nlisted in the high-risk use cases in an annex to this Regulation is not high-risk on the basis of a derogation, should be\nrequired to register themselves and information about their AI system in an EU database, to be established and\nmanaged by the Commission. Before using an AI system listed in the high-risk use cases in an annex to this\nRegulation, deployers of high-risk AI systems that are public authorities, agencies or bodies, should register\nthemselves in such database and select the system that they envisage to use. Other deployers should be entitled to do\nso voluntarily. This section of the EU database should be publicly accessible, free of charge, the information should\nbe easily navigable, understandable and machine-readable. The EU database should also be user-friendly, for example\nby providing search functionalities, incl...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0017", "generated_at": "2025-05-14T16:36:59.127277", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "496622c2-ba07-4216-8236-11c9f65d26db", "question": "What disclosure requirements apply to AI-generated deep fakes?", "answer": "Content resembling real persons or events must be clearly labeled as artificial, with exceptions for creative works, ensuring transparency without hindering artistic expression.", "context": "(131) In order to facilitate the work of the Commission and the Member States in the AI field as well as to increase the\ntransparency towards the public, providers of high-risk AI systems other than those related to products falling within\nthe scope of relevant existing Union harmonisation legislation, as well as providers who consider that an AI system\nlisted in the high-risk use cases in an annex to this Regulation is not high-risk on the basis of a derogation, should be\nrequired to register themselves and information about their AI system in an EU database, to be established and\nmanaged by the Commission. Before using an AI system listed in the high-risk use cases in an annex to this\nRegulation, deployers of high-risk AI systems that are public authorities, agencies or bodies, should register\nthemselves in such database and select the system that they envisage to use. Other deployers should be entitled to do\nso voluntarily. This section of the EU database should be publicly accessible, free of charge, the information should\nbe easily navigable, understandable and machine-readable. The EU database should also be user-friendly, for example\nby providing search functionalities, incl...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0017", "generated_at": "2025-05-14T16:36:59.128450", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "1f613f50-8aae-4399-9000-31f684395913", "question": "What role does the Commission play in promoting AI content detection?", "answer": "The Commission may encourage Union-level codes of practice to support detection mechanisms, cooperation, and public awareness of AI-generated content authenticity.", "context": "(131) In order to facilitate the work of the Commission and the Member States in the AI field as well as to increase the\ntransparency towards the public, providers of high-risk AI systems other than those related to products falling within\nthe scope of relevant existing Union harmonisation legislation, as well as providers who consider that an AI system\nlisted in the high-risk use cases in an annex to this Regulation is not high-risk on the basis of a derogation, should be\nrequired to register themselves and information about their AI system in an EU database, to be established and\nmanaged by the Commission. Before using an AI system listed in the high-risk use cases in an annex to this\nRegulation, deployers of high-risk AI systems that are public authorities, agencies or bodies, should register\nthemselves in such database and select the system that they envisage to use. Other deployers should be entitled to do\nso voluntarily. This section of the EU database should be publicly accessible, free of charge, the information should\nbe easily navigable, understandable and machine-readable. The EU database should also be user-friendly, for example\nby providing search functionalities, incl...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0017", "generated_at": "2025-05-14T16:36:59.129604", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "27c65ce8-ae2c-4743-882d-81ccec51941c", "question": "What are the main objectives of AI regulatory sandboxes according to the text?", "answer": "The objectives include fostering AI innovation, ensuring compliance with regulations, enhancing legal certainty, facilitating regulatory learning, supporting cooperation, and accelerating market access for SMEs and start-ups.", "context": "(139) The objectives of the AI regulatory sandboxes should be to foster AI innovation by establishing a controlled\nexperimentation and testing environment in the development and pre-marketing phase with a view to ensuring\ncompliance of the innovative AI systems with this Regulation and other relevant Union and national law. Moreover,\nthe AI regulatory sandboxes should aim to enhance legal certainty for innovators and the competent authorities \noversight and understanding of the opportunities, emerging risks and the impacts of AI use, to facilitate regulatory\nlearning for authorities and undertakings, including with a view to future adaptions of the legal framework, to\nsupport cooperation and the sharing of best practices with the authorities involved in the AI regulatory sandbox,\nand to accelerate access to markets, including by removing barriers for SMEs, including start-ups. AI regulatory\nsandboxes should be widely available throughout the Union, and particular attention should be given to their\naccessibility for SMEs, including start-ups. The participation in the AI regulatory sandbox should focus on issues that\nraise legal uncertainty for providers and prospective providers to ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0018", "generated_at": "2025-05-14T16:38:41.385204", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "bc5fefc7-c925-421f-808c-de04f799e0e5", "question": "Under what conditions can personal data be used in AI regulatory sandboxes?", "answer": "Personal data collected for other purposes can be used for developing AI systems in the public interest within the sandbox, under specified conditions in accordance with GDPR and other data protection regulations, without prejudice to existing legal bases.", "context": "(139) The objectives of the AI regulatory sandboxes should be to foster AI innovation by establishing a controlled\nexperimentation and testing environment in the development and pre-marketing phase with a view to ensuring\ncompliance of the innovative AI systems with this Regulation and other relevant Union and national law. Moreover,\nthe AI regulatory sandboxes should aim to enhance legal certainty for innovators and the competent authorities \noversight and understanding of the opportunities, emerging risks and the impacts of AI use, to facilitate regulatory\nlearning for authorities and undertakings, including with a view to future adaptions of the legal framework, to\nsupport cooperation and the sharing of best practices with the authorities involved in the AI regulatory sandbox,\nand to accelerate access to markets, including by removing barriers for SMEs, including start-ups. AI regulatory\nsandboxes should be widely available throughout the Union, and particular attention should be given to their\naccessibility for SMEs, including start-ups. The participation in the AI regulatory sandbox should focus on issues that\nraise legal uncertainty for providers and prospective providers to ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0018", "generated_at": "2025-05-14T16:38:41.387045", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "4e126c74-defb-47bb-b5f7-1103a434ec4d", "question": "What safeguards are required for real-world testing of high-risk AI systems outside sandboxes?", "answer": "Safeguards include informed consent (except for law enforcement), a real-world testing plan, registration in the EU database, limitations on testing duration, additional safeguards for vulnerable groups, and data transfer compliance with Union law.", "context": "(139) The objectives of the AI regulatory sandboxes should be to foster AI innovation by establishing a controlled\nexperimentation and testing environment in the development and pre-marketing phase with a view to ensuring\ncompliance of the innovative AI systems with this Regulation and other relevant Union and national law. Moreover,\nthe AI regulatory sandboxes should aim to enhance legal certainty for innovators and the competent authorities \noversight and understanding of the opportunities, emerging risks and the impacts of AI use, to facilitate regulatory\nlearning for authorities and undertakings, including with a view to future adaptions of the legal framework, to\nsupport cooperation and the sharing of best practices with the authorities involved in the AI regulatory sandbox,\nand to accelerate access to markets, including by removing barriers for SMEs, including start-ups. AI regulatory\nsandboxes should be widely available throughout the Union, and particular attention should be given to their\naccessibility for SMEs, including start-ups. The participation in the AI regulatory sandbox should focus on issues that\nraise legal uncertainty for providers and prospective providers to ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0018", "generated_at": "2025-05-14T16:38:41.388230", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "ad9817cc-bd58-4710-b0fe-229af3ab2274", "question": "How should Member States support SMEs in AI development?", "answer": "Member States should provide priority access to AI regulatory sandboxes for SMEs, establish communication channels, offer guidance, and facilitate participation in standardisation processes.", "context": "(139) The objectives of the AI regulatory sandboxes should be to foster AI innovation by establishing a controlled\nexperimentation and testing environment in the development and pre-marketing phase with a view to ensuring\ncompliance of the innovative AI systems with this Regulation and other relevant Union and national law. Moreover,\nthe AI regulatory sandboxes should aim to enhance legal certainty for innovators and the competent authorities \noversight and understanding of the opportunities, emerging risks and the impacts of AI use, to facilitate regulatory\nlearning for authorities and undertakings, including with a view to future adaptions of the legal framework, to\nsupport cooperation and the sharing of best practices with the authorities involved in the AI regulatory sandbox,\nand to accelerate access to markets, including by removing barriers for SMEs, including start-ups. AI regulatory\nsandboxes should be widely available throughout the Union, and particular attention should be given to their\naccessibility for SMEs, including start-ups. The participation in the AI regulatory sandbox should focus on issues that\nraise legal uncertainty for providers and prospective providers to ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0018", "generated_at": "2025-05-14T16:38:41.389388", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "bd362d9a-2402-40ed-af55-05789dc73559", "question": "What is the purpose of promoting AI solutions for socially and environmentally beneficial outcomes?", "answer": "To encourage research and development of AI solutions addressing accessibility, socio-economic inequalities, and environmental targets, through public funding and interdisciplinary collaboration.", "context": "(139) The objectives of the AI regulatory sandboxes should be to foster AI innovation by establishing a controlled\nexperimentation and testing environment in the development and pre-marketing phase with a view to ensuring\ncompliance of the innovative AI systems with this Regulation and other relevant Union and national law. Moreover,\nthe AI regulatory sandboxes should aim to enhance legal certainty for innovators and the competent authorities \noversight and understanding of the opportunities, emerging risks and the impacts of AI use, to facilitate regulatory\nlearning for authorities and undertakings, including with a view to future adaptions of the legal framework, to\nsupport cooperation and the sharing of best practices with the authorities involved in the AI regulatory sandbox,\nand to accelerate access to markets, including by removing barriers for SMEs, including start-ups. AI regulatory\nsandboxes should be widely available throughout the Union, and particular attention should be given to their\naccessibility for SMEs, including start-ups. The participation in the AI regulatory sandbox should focus on issues that\nraise legal uncertainty for providers and prospective providers to ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0018", "generated_at": "2025-05-14T16:38:41.390556", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "524a8993-c05d-48d3-9e3c-fb2309ed7048", "question": "What measures should the Commission take to address certification and compliance costs for SMEs, including start-ups?", "answer": "The Commission should regularly assess certification and compliance costs for SMEs through transparent consultations, work with Member States to lower costs, and provide standardised templates for the Regulation upon request of the Board.", "context": "including start-ups, should be taken into account when notified bodies set conformity assessment fees. The\nCommission should regularly assess the certification and compliance costs for SMEs, including start-ups, through\ntransparent consultations and should work with Member States to lower such costs. For example, translation costs\nrelated to mandatory documentation and communication with authorities may constitute a significant cost for\nproviders and other operators, in particular those of a smaller scale. Member States should possibly ensure that one\nof the languages determined and accepted by them for relevant providers  documentation and for communication\nwith operators is one which is broadly understood by the largest possible number of cross-border deployers. In order\nto address the specific needs of SMEs, including start-ups, the Commission should provide standardised templates for\nthe areas covered by this Regulation, upon request of the Board. Additionally, the Commission should complement\nMember States  efforts by providing a single information platform with easy-to-use information with regards to this\nRegulation for all providers and deployers, by organising appropriate c...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0019", "generated_at": "2025-05-14T16:39:02.144516", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "c16be89d-9228-481a-8351-230aaa360406", "question": "How can Member States reduce translation costs for SMEs and start-ups?", "answer": "Member States should ensure that one of the accepted languages for documentation and communication is a broadly understood language to minimize translation costs for cross-border deployers.", "context": "including start-ups, should be taken into account when notified bodies set conformity assessment fees. The\nCommission should regularly assess the certification and compliance costs for SMEs, including start-ups, through\ntransparent consultations and should work with Member States to lower such costs. For example, translation costs\nrelated to mandatory documentation and communication with authorities may constitute a significant cost for\nproviders and other operators, in particular those of a smaller scale. Member States should possibly ensure that one\nof the languages determined and accepted by them for relevant providers  documentation and for communication\nwith operators is one which is broadly understood by the largest possible number of cross-border deployers. In order\nto address the specific needs of SMEs, including start-ups, the Commission should provide standardised templates for\nthe areas covered by this Regulation, upon request of the Board. Additionally, the Commission should complement\nMember States  efforts by providing a single information platform with easy-to-use information with regards to this\nRegulation for all providers and deployers, by organising appropriate c...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0019", "generated_at": "2025-05-14T16:39:02.145832", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "6dc981f3-180e-48e8-b9f0-d675777cc553", "question": "What support measures are available for new medium-sized enterprises under this Regulation?", "answer": "New medium-sized enterprises should have access to support measures, including legal resources and training, to ensure proper understanding and compliance with the Regulation.", "context": "including start-ups, should be taken into account when notified bodies set conformity assessment fees. The\nCommission should regularly assess the certification and compliance costs for SMEs, including start-ups, through\ntransparent consultations and should work with Member States to lower such costs. For example, translation costs\nrelated to mandatory documentation and communication with authorities may constitute a significant cost for\nproviders and other operators, in particular those of a smaller scale. Member States should possibly ensure that one\nof the languages determined and accepted by them for relevant providers  documentation and for communication\nwith operators is one which is broadly understood by the largest possible number of cross-border deployers. In order\nto address the specific needs of SMEs, including start-ups, the Commission should provide standardised templates for\nthe areas covered by this Regulation, upon request of the Board. Additionally, the Commission should complement\nMember States  efforts by providing a single information platform with easy-to-use information with regards to this\nRegulation for all providers and deployers, by organising appropriate c...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0019", "generated_at": "2025-05-14T16:39:02.147022", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "86f4afb5-e460-4913-999c-177585733f9a", "question": "What is proposed for microenterprises regarding quality management systems?", "answer": "Microenterprises should be allowed to fulfill the quality management system obligation in a simplified manner, with the Commission developing guidelines to specify the elements of this simplified system.", "context": "including start-ups, should be taken into account when notified bodies set conformity assessment fees. The\nCommission should regularly assess the certification and compliance costs for SMEs, including start-ups, through\ntransparent consultations and should work with Member States to lower such costs. For example, translation costs\nrelated to mandatory documentation and communication with authorities may constitute a significant cost for\nproviders and other operators, in particular those of a smaller scale. Member States should possibly ensure that one\nof the languages determined and accepted by them for relevant providers  documentation and for communication\nwith operators is one which is broadly understood by the largest possible number of cross-border deployers. In order\nto address the specific needs of SMEs, including start-ups, the Commission should provide standardised templates for\nthe areas covered by this Regulation, upon request of the Board. Additionally, the Commission should complement\nMember States  efforts by providing a single information platform with easy-to-use information with regards to this\nRegulation for all providers and deployers, by organising appropriate c...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0019", "generated_at": "2025-05-14T16:39:02.148197", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "dfe45943-7ed9-4e4c-9cca-66ecd4a02c99", "question": "What role does the AI-on-demand platform play in the implementation of this Regulation?", "answer": "The AI-on-demand platform, along with Digital Innovation Hubs and testing facilities, should provide technical and scientific support to providers and notified bodies to facilitate compliance and implementation of the Regulation.", "context": "including start-ups, should be taken into account when notified bodies set conformity assessment fees. The\nCommission should regularly assess the certification and compliance costs for SMEs, including start-ups, through\ntransparent consultations and should work with Member States to lower such costs. For example, translation costs\nrelated to mandatory documentation and communication with authorities may constitute a significant cost for\nproviders and other operators, in particular those of a smaller scale. Member States should possibly ensure that one\nof the languages determined and accepted by them for relevant providers  documentation and for communication\nwith operators is one which is broadly understood by the largest possible number of cross-border deployers. In order\nto address the specific needs of SMEs, including start-ups, the Commission should provide standardised templates for\nthe areas covered by this Regulation, upon request of the Board. Additionally, the Commission should complement\nMember States  efforts by providing a single information platform with easy-to-use information with regards to this\nRegulation for all providers and deployers, by organising appropriate c...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0019", "generated_at": "2025-05-14T16:39:02.149344", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "4e3a7a3d-0fe6-470c-9dc4-8b88ed5d7c06", "question": "What is the role of the AI Office in monitoring general-purpose AI models?", "answer": "The AI Office should monitor the effective implementation of obligations for providers of general-purpose AI models, investigate possible infringements, and ensure compliance through measures like risk mitigation, market withdrawal, or recalls.", "context": "take place at Union level through the AI Office, which should have the powers of a market surveillance authority\nwithin the meaning of Regulation (EU) 2019/1020 for this purpose. In all other cases, national market surveillance\nauthorities remain responsible for the supervision of AI systems. However, for general-purpose AI systems that can\nbe used directly by deployers for at least one purpose that is classified as high-risk, market surveillance authorities\nshould cooperate with the AI Office to carry out evaluations of compliance and inform the Board and other market\nsurveillance authorities accordingly. Furthermore, market surveillance authorities should be able to request\nassistance from the AI Office where the market surveillance authority is unable to conclude an investigation on\na high-risk AI system because of its inability to access certain information related to the general-purpose AI model\non which the high-risk AI system is built. In such cases, the procedure regarding mutual assistance in cross-border\ncases in Chapter VI of Regulation (EU) 2019/1020 should apply mutatis mutandis.\n\n(162) To make best use of the centralised Union expertise and synergies at Union level, t...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0021", "generated_at": "2025-05-14T16:55:30.321224", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "625739e4-46d3-4746-a6ff-1ed9ba03e7ed", "question": "How do national market surveillance authorities cooperate with the AI Office for high-risk AI systems?", "answer": "National authorities should cooperate with the AI Office for evaluations of compliance, inform the Board and other authorities, and request assistance from the AI Office when unable to access information on general-purpose AI models, applying mutual assistance procedures from Regulation (EU) 2019/1020.", "context": "take place at Union level through the AI Office, which should have the powers of a market surveillance authority\nwithin the meaning of Regulation (EU) 2019/1020 for this purpose. In all other cases, national market surveillance\nauthorities remain responsible for the supervision of AI systems. However, for general-purpose AI systems that can\nbe used directly by deployers for at least one purpose that is classified as high-risk, market surveillance authorities\nshould cooperate with the AI Office to carry out evaluations of compliance and inform the Board and other market\nsurveillance authorities accordingly. Furthermore, market surveillance authorities should be able to request\nassistance from the AI Office where the market surveillance authority is unable to conclude an investigation on\na high-risk AI system because of its inability to access certain information related to the general-purpose AI model\non which the high-risk AI system is built. In such cases, the procedure regarding mutual assistance in cross-border\ncases in Chapter VI of Regulation (EU) 2019/1020 should apply mutatis mutandis.\n\n(162) To make best use of the centralised Union expertise and synergies at Union level, t...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0021", "generated_at": "2025-05-14T16:55:30.322557", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "e214983b-c25a-4756-8db3-ee1cd5e07996", "question": "Under what conditions can the scientific panel trigger follow-ups with the AI Office?", "answer": "The scientific panel may provide alerts to the AI Office if it suspects a general-purpose AI model poses a concrete risk at Union level or meets criteria for systemic risk, enabling investigations or other actions.", "context": "take place at Union level through the AI Office, which should have the powers of a market surveillance authority\nwithin the meaning of Regulation (EU) 2019/1020 for this purpose. In all other cases, national market surveillance\nauthorities remain responsible for the supervision of AI systems. However, for general-purpose AI systems that can\nbe used directly by deployers for at least one purpose that is classified as high-risk, market surveillance authorities\nshould cooperate with the AI Office to carry out evaluations of compliance and inform the Board and other market\nsurveillance authorities accordingly. Furthermore, market surveillance authorities should be able to request\nassistance from the AI Office where the market surveillance authority is unable to conclude an investigation on\na high-risk AI system because of its inability to access certain information related to the general-purpose AI model\non which the high-risk AI system is built. In such cases, the procedure regarding mutual assistance in cross-border\ncases in Chapter VI of Regulation (EU) 2019/1020 should apply mutatis mutandis.\n\n(162) To make best use of the centralised Union expertise and synergies at Union level, t...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0021", "generated_at": "2025-05-14T16:55:30.323730", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "8594bab7-9361-4447-844e-6f1b1b9455e6", "question": "What enforcement measures are available for non-compliance with AI regulations?", "answer": "Enforcement includes penalties, administrative fines, and measures like restricting market availability or recalling models. The Commission may impose fines, and Member States must ensure proportionate penalties, respecting the ne bis in idem principle.", "context": "take place at Union level through the AI Office, which should have the powers of a market surveillance authority\nwithin the meaning of Regulation (EU) 2019/1020 for this purpose. In all other cases, national market surveillance\nauthorities remain responsible for the supervision of AI systems. However, for general-purpose AI systems that can\nbe used directly by deployers for at least one purpose that is classified as high-risk, market surveillance authorities\nshould cooperate with the AI Office to carry out evaluations of compliance and inform the Board and other market\nsurveillance authorities accordingly. Furthermore, market surveillance authorities should be able to request\nassistance from the AI Office where the market surveillance authority is unable to conclude an investigation on\na high-risk AI system because of its inability to access certain information related to the general-purpose AI model\non which the high-risk AI system is built. In such cases, the procedure regarding mutual assistance in cross-border\ncases in Chapter VI of Regulation (EU) 2019/1020 should apply mutatis mutandis.\n\n(162) To make best use of the centralised Union expertise and synergies at Union level, t...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0021", "generated_at": "2025-05-14T16:55:30.324891", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "63f2b69b-61d3-4267-90a7-0267854773fe", "question": "What are the voluntary measures encouraged for non-high-risk AI systems?", "answer": "Providers are encouraged to create codes of conduct and apply voluntary requirements such as ethical guidelines, environmental sustainability, AI literacy, inclusive design, and stakeholder participation, with clear objectives and performance indicators.", "context": "take place at Union level through the AI Office, which should have the powers of a market surveillance authority\nwithin the meaning of Regulation (EU) 2019/1020 for this purpose. In all other cases, national market surveillance\nauthorities remain responsible for the supervision of AI systems. However, for general-purpose AI systems that can\nbe used directly by deployers for at least one purpose that is classified as high-risk, market surveillance authorities\nshould cooperate with the AI Office to carry out evaluations of compliance and inform the Board and other market\nsurveillance authorities accordingly. Furthermore, market surveillance authorities should be able to request\nassistance from the AI Office where the market surveillance authority is unable to conclude an investigation on\na high-risk AI system because of its inability to access certain information related to the general-purpose AI model\non which the high-risk AI system is built. In such cases, the procedure regarding mutual assistance in cross-border\ncases in Chapter VI of Regulation (EU) 2019/1020 should apply mutatis mutandis.\n\n(162) To make best use of the centralised Union expertise and synergies at Union level, t...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0021", "generated_at": "2025-05-14T16:55:30.326048", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "a2ede2b0-5de8-4642-af6a-0d05e16085d7", "question": "What powers should be conferred on the Commission to ensure uniform conditions for implementing the Regulation?", "answer": "Implementing powers should be conferred on the Commission in accordance with Regulation (EU) No 182/2011 of the European Parliament and of the Council.", "context": "(173) In order to ensure that the regulatory framework can be adapted where necessary, the power to adopt acts in\naccordance with Article 290 TFEU should be delegated to the Commission to amend the conditions under which an\nAI system is not to be considered to be high-risk, the list of high-risk AI systems, the provisions regarding technical\ndocumentation, the content of the EU declaration of conformity the provisions regarding the conformity assessment\nprocedures, the provisions establishing the high-risk AI systems to which the conformity assessment procedure\nbased on assessment of the quality management system and assessment of the technical documentation should\napply, the threshold, benchmarks and indicators, including by supplementing those benchmarks and indicators, in\nthe rules for the classification of general-purpose AI models with systemic risk, the criteria for the designation of\ngeneral-purpose AI models with systemic risk, the technical documentation for providers of general-purpose AI\nmodels and the transparency information for providers of general-purpose AI models. It is of particular importance\nthat the Commission carry out appropriate consultations during its prep...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0022", "generated_at": "2025-05-14T17:12:35.486455", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "45c5e3f0-ebef-4c9d-b6ac-4d1f1e420420", "question": "When must the Commission evaluate and review this Regulation?", "answer": "The Commission should evaluate and review this Regulation by 2 August 2029 and every four years thereafter.", "context": "(173) In order to ensure that the regulatory framework can be adapted where necessary, the power to adopt acts in\naccordance with Article 290 TFEU should be delegated to the Commission to amend the conditions under which an\nAI system is not to be considered to be high-risk, the list of high-risk AI systems, the provisions regarding technical\ndocumentation, the content of the EU declaration of conformity the provisions regarding the conformity assessment\nprocedures, the provisions establishing the high-risk AI systems to which the conformity assessment procedure\nbased on assessment of the quality management system and assessment of the technical documentation should\napply, the threshold, benchmarks and indicators, including by supplementing those benchmarks and indicators, in\nthe rules for the classification of general-purpose AI models with systemic risk, the criteria for the designation of\ngeneral-purpose AI models with systemic risk, the technical documentation for providers of general-purpose AI\nmodels and the transparency information for providers of general-purpose AI models. It is of particular importance\nthat the Commission carry out appropriate consultations during its prep...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0022", "generated_at": "2025-05-14T17:12:35.487910", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "9ce324ae-daee-4858-afb8-565ff62ed2b0", "question": "When does this Regulation apply to high-risk AI systems already on the market?", "answer": "This Regulation applies to high-risk AI systems placed on the market or put into service before the general date of application only if they undergo significant changes in design or intended purpose.", "context": "(173) In order to ensure that the regulatory framework can be adapted where necessary, the power to adopt acts in\naccordance with Article 290 TFEU should be delegated to the Commission to amend the conditions under which an\nAI system is not to be considered to be high-risk, the list of high-risk AI systems, the provisions regarding technical\ndocumentation, the content of the EU declaration of conformity the provisions regarding the conformity assessment\nprocedures, the provisions establishing the high-risk AI systems to which the conformity assessment procedure\nbased on assessment of the quality management system and assessment of the technical documentation should\napply, the threshold, benchmarks and indicators, including by supplementing those benchmarks and indicators, in\nthe rules for the classification of general-purpose AI models with systemic risk, the criteria for the designation of\ngeneral-purpose AI models with systemic risk, the technical documentation for providers of general-purpose AI\nmodels and the transparency information for providers of general-purpose AI models. It is of particular importance\nthat the Commission carry out appropriate consultations during its prep...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0022", "generated_at": "2025-05-14T17:12:35.489101", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "82f750af-fb9f-41f8-9ff7-34be29a01a23", "question": "What is the effective date for the full application of the Regulation?", "answer": "This Regulation should apply from 2 August 2026, with prohibitions and general provisions applying from 2 February 2025.", "context": "(173) In order to ensure that the regulatory framework can be adapted where necessary, the power to adopt acts in\naccordance with Article 290 TFEU should be delegated to the Commission to amend the conditions under which an\nAI system is not to be considered to be high-risk, the list of high-risk AI systems, the provisions regarding technical\ndocumentation, the content of the EU declaration of conformity the provisions regarding the conformity assessment\nprocedures, the provisions establishing the high-risk AI systems to which the conformity assessment procedure\nbased on assessment of the quality management system and assessment of the technical documentation should\napply, the threshold, benchmarks and indicators, including by supplementing those benchmarks and indicators, in\nthe rules for the classification of general-purpose AI models with systemic risk, the criteria for the designation of\ngeneral-purpose AI models with systemic risk, the technical documentation for providers of general-purpose AI\nmodels and the transparency information for providers of general-purpose AI models. It is of particular importance\nthat the Commission carry out appropriate consultations during its prep...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0022", "generated_at": "2025-05-14T17:12:35.490268", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "bb87fc0d-e3ba-441a-b50c-fedad2dcfb0f", "question": "What is the basis for the Commission's delegated powers under this Regulation?", "answer": "The Commission's delegated powers are based on Article 290 TFEU, allowing it to amend conditions, lists, and provisions related to high-risk AI systems.", "context": "(173) In order to ensure that the regulatory framework can be adapted where necessary, the power to adopt acts in\naccordance with Article 290 TFEU should be delegated to the Commission to amend the conditions under which an\nAI system is not to be considered to be high-risk, the list of high-risk AI systems, the provisions regarding technical\ndocumentation, the content of the EU declaration of conformity the provisions regarding the conformity assessment\nprocedures, the provisions establishing the high-risk AI systems to which the conformity assessment procedure\nbased on assessment of the quality management system and assessment of the technical documentation should\napply, the threshold, benchmarks and indicators, including by supplementing those benchmarks and indicators, in\nthe rules for the classification of general-purpose AI models with systemic risk, the criteria for the designation of\ngeneral-purpose AI models with systemic risk, the technical documentation for providers of general-purpose AI\nmodels and the transparency information for providers of general-purpose AI models. It is of particular importance\nthat the Commission carry out appropriate consultations during its prep...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0022", "generated_at": "2025-05-14T17:12:35.491439", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "29ef3282-a334-4770-97b4-b632808e5cff", "question": "Which entities are excluded from the Regulation's application under Article 4?", "answer": "Public authorities in a third country or international organisations using AI systems in international cooperation or agreements for law enforcement and judicial cooperation with the Union, provided they provide adequate safeguards for fundamental rights.", "context": "4. This Regulation applies neither to public authorities in a third country nor to international organisations falling\nwithin the scope of this Regulation pursuant to paragraph 1, where those authorities or organisations use AI systems in the\nframework of international cooperation or agreements for law enforcement and judicial cooperation with the Union or\nwith one or more Member States, provided that such a third country or international organisation provides adequate\nsafeguards with respect to the protection of fundamental rights and freedoms of individuals.\n5. This Regulation shall not affect the application of the provisions on the liability of providers of intermediary services\nas set out in Chapter II of Regulation (EU) 2022/2065.\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 45/144\n\n6. This Regulation does not apply to AI systems or AI models, including their output, specifically developed and put into\nservice for the sole purpose of scientific research and development.\n7. Union law on the protection of personal data, privacy and the confidentiality of communications applies to personal\ndata processed in ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0023", "generated_at": "2025-05-14T17:13:07.438874", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "adfeeb5f-1eab-4d60-a7f9-e66b7184df24", "question": "How does the Regulation handle liability of intermediary services?", "answer": "It does not affect the application of liability provisions for providers of intermediary services as set out in Regulation (EU) 2022/2065.", "context": "4. This Regulation applies neither to public authorities in a third country nor to international organisations falling\nwithin the scope of this Regulation pursuant to paragraph 1, where those authorities or organisations use AI systems in the\nframework of international cooperation or agreements for law enforcement and judicial cooperation with the Union or\nwith one or more Member States, provided that such a third country or international organisation provides adequate\nsafeguards with respect to the protection of fundamental rights and freedoms of individuals.\n5. This Regulation shall not affect the application of the provisions on the liability of providers of intermediary services\nas set out in Chapter II of Regulation (EU) 2022/2065.\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 45/144\n\n6. This Regulation does not apply to AI systems or AI models, including their output, specifically developed and put into\nservice for the sole purpose of scientific research and development.\n7. Union law on the protection of personal data, privacy and the confidentiality of communications applies to personal\ndata processed in ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0023", "generated_at": "2025-05-14T17:13:07.440335", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "fcd1f4a1-764b-4fa3-800d-1095848f60e4", "question": "Are AI systems developed for scientific research exempt from the Regulation?", "answer": "Yes, if they are specifically developed and put into service for the sole purpose of scientific research and development.", "context": "4. This Regulation applies neither to public authorities in a third country nor to international organisations falling\nwithin the scope of this Regulation pursuant to paragraph 1, where those authorities or organisations use AI systems in the\nframework of international cooperation or agreements for law enforcement and judicial cooperation with the Union or\nwith one or more Member States, provided that such a third country or international organisation provides adequate\nsafeguards with respect to the protection of fundamental rights and freedoms of individuals.\n5. This Regulation shall not affect the application of the provisions on the liability of providers of intermediary services\nas set out in Chapter II of Regulation (EU) 2022/2065.\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 45/144\n\n6. This Regulation does not apply to AI systems or AI models, including their output, specifically developed and put into\nservice for the sole purpose of scientific research and development.\n7. Union law on the protection of personal data, privacy and the confidentiality of communications applies to personal\ndata processed in ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0023", "generated_at": "2025-05-14T17:13:07.441518", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "6056b976-a293-4d62-acca-dd8bd1c73a95", "question": "What is the scope of Union law on personal data regarding this Regulation?", "answer": "Union law on personal data protection, privacy, and communications confidentiality applies to data processed under this Regulation, without affecting Regulation (EU) 2016/679 or other relevant data protection laws.", "context": "4. This Regulation applies neither to public authorities in a third country nor to international organisations falling\nwithin the scope of this Regulation pursuant to paragraph 1, where those authorities or organisations use AI systems in the\nframework of international cooperation or agreements for law enforcement and judicial cooperation with the Union or\nwith one or more Member States, provided that such a third country or international organisation provides adequate\nsafeguards with respect to the protection of fundamental rights and freedoms of individuals.\n5. This Regulation shall not affect the application of the provisions on the liability of providers of intermediary services\nas set out in Chapter II of Regulation (EU) 2022/2065.\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 45/144\n\n6. This Regulation does not apply to AI systems or AI models, including their output, specifically developed and put into\nservice for the sole purpose of scientific research and development.\n7. Union law on the protection of personal data, privacy and the confidentiality of communications applies to personal\ndata processed in ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0023", "generated_at": "2025-05-14T17:13:07.442680", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "2a196a93-4ead-4ae9-a20a-cae0fa6f13d8", "question": "Does the Regulation apply to research and testing activities before market placement?", "answer": "No, but such activities must comply with applicable Union law. Testing in real-world conditions is not excluded.", "context": "4. This Regulation applies neither to public authorities in a third country nor to international organisations falling\nwithin the scope of this Regulation pursuant to paragraph 1, where those authorities or organisations use AI systems in the\nframework of international cooperation or agreements for law enforcement and judicial cooperation with the Union or\nwith one or more Member States, provided that such a third country or international organisation provides adequate\nsafeguards with respect to the protection of fundamental rights and freedoms of individuals.\n5. This Regulation shall not affect the application of the provisions on the liability of providers of intermediary services\nas set out in Chapter II of Regulation (EU) 2022/2065.\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 45/144\n\n6. This Regulation does not apply to AI systems or AI models, including their output, specifically developed and put into\nservice for the sole purpose of scientific research and development.\n7. Union law on the protection of personal data, privacy and the confidentiality of communications applies to personal\ndata processed in ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0023", "generated_at": "2025-05-14T17:13:07.443882", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "e8a6fb4c-9798-4a76-b2de-a22a61cda82d", "question": "Under what circumstances is the use of AI systems to infer emotions in workplaces or education institutions allowed?", "answer": "The use of AI systems to infer emotions in workplaces or education institutions is allowed only if it is intended for medical or safety reasons.", "context": "(f) the placing on the market, the putting into service for this specific purpose, or the use of AI systems to infer emotions\nof a natural person in the areas of workplace and education institutions, except where the use of the AI system is\nintended to be put in place or into the market for medical or safety reasons;\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 51/144\n\n(g) the placing on the market, the putting into service for this specific purpose, or the use of biometric categorisation\nsystems that categorise individually natural persons based on their biometric data to deduce or infer their race, political\nopinions, trade union membership, religious or philosophical beliefs, sex life or sexual orientation; this prohibition does\nnot cover any labelling or filtering of lawfully acquired biometric datasets, such as images, based on biometric data or\ncategorizing of biometric data in the area of law enforcement;\n\n(h)the use of  real-time  remote biometric identification systems in publicly accessible spaces for the purposes of law\nenforcement, unless and in so far as such use is strictly necessary for one of th...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0025", "generated_at": "2025-05-14T17:29:41.601160", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "145fff5c-bb8e-4175-8a8a-a9caf60a0752", "question": "What is prohibited regarding biometric categorisation systems?", "answer": "Biometric categorisation systems that categorise individuals based on biometric data to deduce race, political opinions, trade union membership, religious beliefs, sex life, or sexual orientation are prohibited, except for labelling or filtering lawfully acquired biometric datasets in law enforcement.", "context": "(f) the placing on the market, the putting into service for this specific purpose, or the use of AI systems to infer emotions\nof a natural person in the areas of workplace and education institutions, except where the use of the AI system is\nintended to be put in place or into the market for medical or safety reasons;\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 51/144\n\n(g) the placing on the market, the putting into service for this specific purpose, or the use of biometric categorisation\nsystems that categorise individually natural persons based on their biometric data to deduce or infer their race, political\nopinions, trade union membership, religious or philosophical beliefs, sex life or sexual orientation; this prohibition does\nnot cover any labelling or filtering of lawfully acquired biometric datasets, such as images, based on biometric data or\ncategorizing of biometric data in the area of law enforcement;\n\n(h)the use of  real-time  remote biometric identification systems in publicly accessible spaces for the purposes of law\nenforcement, unless and in so far as such use is strictly necessary for one of th...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0025", "generated_at": "2025-05-14T17:29:41.602629", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "cb601852-5fc5-4070-b078-21920275ffe2", "question": "What are the permitted objectives for using real-time remote biometric identification systems in public spaces for law enforcement?", "answer": "Permitted objectives include targeted search for missing persons, prevention of imminent threats to life or safety, and localization of suspects for criminal investigations involving serious offenses.", "context": "(f) the placing on the market, the putting into service for this specific purpose, or the use of AI systems to infer emotions\nof a natural person in the areas of workplace and education institutions, except where the use of the AI system is\nintended to be put in place or into the market for medical or safety reasons;\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 51/144\n\n(g) the placing on the market, the putting into service for this specific purpose, or the use of biometric categorisation\nsystems that categorise individually natural persons based on their biometric data to deduce or infer their race, political\nopinions, trade union membership, religious or philosophical beliefs, sex life or sexual orientation; this prohibition does\nnot cover any labelling or filtering of lawfully acquired biometric datasets, such as images, based on biometric data or\ncategorizing of biometric data in the area of law enforcement;\n\n(h)the use of  real-time  remote biometric identification systems in publicly accessible spaces for the purposes of law\nenforcement, unless and in so far as such use is strictly necessary for one of th...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0025", "generated_at": "2025-05-14T17:29:41.603757", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "09d4549b-03b9-4e4f-8b57-a45a112f4dfd", "question": "What requirements must be met when deploying real-time remote biometric identification systems for law enforcement?", "answer": "Deployment must be limited to confirming the identity of targeted individuals, consider the nature of the situation and potential harm, and comply with proportionality safeguards, including a fundamental rights impact assessment and EU database registration.", "context": "(f) the placing on the market, the putting into service for this specific purpose, or the use of AI systems to infer emotions\nof a natural person in the areas of workplace and education institutions, except where the use of the AI system is\nintended to be put in place or into the market for medical or safety reasons;\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 51/144\n\n(g) the placing on the market, the putting into service for this specific purpose, or the use of biometric categorisation\nsystems that categorise individually natural persons based on their biometric data to deduce or infer their race, political\nopinions, trade union membership, religious or philosophical beliefs, sex life or sexual orientation; this prohibition does\nnot cover any labelling or filtering of lawfully acquired biometric datasets, such as images, based on biometric data or\ncategorizing of biometric data in the area of law enforcement;\n\n(h)the use of  real-time  remote biometric identification systems in publicly accessible spaces for the purposes of law\nenforcement, unless and in so far as such use is strictly necessary for one of th...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0025", "generated_at": "2025-05-14T17:29:41.604893", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "ee1282cd-3c16-4ce0-97fc-a5a0d40a412f", "question": "What authorisation is required for the use of real-time remote biometric identification systems in public spaces?", "answer": "Prior authorisation by a judicial or independent administrative authority is required, issued upon a reasoned request, with exceptions for urgent cases where authorisation is sought within 24 hours.", "context": "(f) the placing on the market, the putting into service for this specific purpose, or the use of AI systems to infer emotions\nof a natural person in the areas of workplace and education institutions, except where the use of the AI system is\nintended to be put in place or into the market for medical or safety reasons;\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 51/144\n\n(g) the placing on the market, the putting into service for this specific purpose, or the use of biometric categorisation\nsystems that categorise individually natural persons based on their biometric data to deduce or infer their race, political\nopinions, trade union membership, religious or philosophical beliefs, sex life or sexual orientation; this prohibition does\nnot cover any labelling or filtering of lawfully acquired biometric datasets, such as images, based on biometric data or\ncategorizing of biometric data in the area of law enforcement;\n\n(h)the use of  real-time  remote biometric identification systems in publicly accessible spaces for the purposes of law\nenforcement, unless and in so far as such use is strictly necessary for one of th...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0025", "generated_at": "2025-05-14T17:29:41.606059", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "1869ff99-19c9-4ae0-b94b-d571623c5014", "question": "What must a provider do if they determine an AI system is not high-risk before placing it on the market?", "answer": "The provider must document their assessment and be subject to the registration obligation in Article 49(2).", "context": "4. A provider who considers that an AI system referred to in Annex III is not high-risk shall document its assessment\nbefore that system is placed on the market or put into service. Such provider shall be subject to the registration obligation\nset out in Article 49(2). Upon request of national competent authorities, the provider shall provide the documentation of\nthe assessment.\n5. The Commission shall, after consulting the European Artificial Intelligence Board (the  Board ), and no later than\n2 February 2026, provide guidelines specifying the practical implementation of this Article in line with Article 96 together\nwith a comprehensive list of practical examples of use cases of AI systems that are high-risk and not high-risk.\n6. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend paragraph 3,\nsecond subparagraph, of this Article by adding new conditions to those laid down therein, or by modifying them, where\nthere is concrete and reliable evidence of the existence of AI systems that fall under the scope of Annex III, but do not pose\na significant risk of harm to the health, safety or fundamental rights of natural persons.\n7. The Com...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0026", "generated_at": "2025-05-14T17:31:19.298774", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "5a86c9a8-38fc-471d-a627-74cf5b0f4601", "question": "When does the Commission need to provide guidelines on the practical implementation of Article 4?", "answer": "No later than 2 February 2026, after consulting the European Artificial Intelligence Board.", "context": "4. A provider who considers that an AI system referred to in Annex III is not high-risk shall document its assessment\nbefore that system is placed on the market or put into service. Such provider shall be subject to the registration obligation\nset out in Article 49(2). Upon request of national competent authorities, the provider shall provide the documentation of\nthe assessment.\n5. The Commission shall, after consulting the European Artificial Intelligence Board (the  Board ), and no later than\n2 February 2026, provide guidelines specifying the practical implementation of this Article in line with Article 96 together\nwith a comprehensive list of practical examples of use cases of AI systems that are high-risk and not high-risk.\n6. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend paragraph 3,\nsecond subparagraph, of this Article by adding new conditions to those laid down therein, or by modifying them, where\nthere is concrete and reliable evidence of the existence of AI systems that fall under the scope of Annex III, but do not pose\na significant risk of harm to the health, safety or fundamental rights of natural persons.\n7. The Com...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0026", "generated_at": "2025-05-14T17:31:19.301228", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "61c5baa7-a09d-42dd-b5e7-cdb4cd34a95b", "question": "Under what conditions can the Commission amend Annex III through delegated acts?", "answer": "When there is concrete evidence that AI systems fall under Annex III but do not pose significant risks to health, safety, or fundamental rights.", "context": "4. A provider who considers that an AI system referred to in Annex III is not high-risk shall document its assessment\nbefore that system is placed on the market or put into service. Such provider shall be subject to the registration obligation\nset out in Article 49(2). Upon request of national competent authorities, the provider shall provide the documentation of\nthe assessment.\n5. The Commission shall, after consulting the European Artificial Intelligence Board (the  Board ), and no later than\n2 February 2026, provide guidelines specifying the practical implementation of this Article in line with Article 96 together\nwith a comprehensive list of practical examples of use cases of AI systems that are high-risk and not high-risk.\n6. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend paragraph 3,\nsecond subparagraph, of this Article by adding new conditions to those laid down therein, or by modifying them, where\nthere is concrete and reliable evidence of the existence of AI systems that fall under the scope of Annex III, but do not pose\na significant risk of harm to the health, safety or fundamental rights of natural persons.\n7. The Com...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0026", "generated_at": "2025-05-14T17:31:19.302901", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "bbb0bc79-4ba1-4202-a087-bbcbe5770650", "question": "What are the key components of a risk management system for high-risk AI systems?", "answer": "It includes identifying risks, estimating and evaluating risks, post-market monitoring, and adopting targeted risk management measures.", "context": "4. A provider who considers that an AI system referred to in Annex III is not high-risk shall document its assessment\nbefore that system is placed on the market or put into service. Such provider shall be subject to the registration obligation\nset out in Article 49(2). Upon request of national competent authorities, the provider shall provide the documentation of\nthe assessment.\n5. The Commission shall, after consulting the European Artificial Intelligence Board (the  Board ), and no later than\n2 February 2026, provide guidelines specifying the practical implementation of this Article in line with Article 96 together\nwith a comprehensive list of practical examples of use cases of AI systems that are high-risk and not high-risk.\n6. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend paragraph 3,\nsecond subparagraph, of this Article by adding new conditions to those laid down therein, or by modifying them, where\nthere is concrete and reliable evidence of the existence of AI systems that fall under the scope of Annex III, but do not pose\na significant risk of harm to the health, safety or fundamental rights of natural persons.\n7. The Com...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0026", "generated_at": "2025-05-14T17:31:19.304763", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "3439a13f-f0c1-4ab6-b753-04f867080b46", "question": "What data governance practices are required for high-risk AI systems using training data?", "answer": "Training, validation, and testing data sets must meet quality criteria and be managed with practices appropriate to the system's intended purpose.", "context": "4. A provider who considers that an AI system referred to in Annex III is not high-risk shall document its assessment\nbefore that system is placed on the market or put into service. Such provider shall be subject to the registration obligation\nset out in Article 49(2). Upon request of national competent authorities, the provider shall provide the documentation of\nthe assessment.\n5. The Commission shall, after consulting the European Artificial Intelligence Board (the  Board ), and no later than\n2 February 2026, provide guidelines specifying the practical implementation of this Article in line with Article 96 together\nwith a comprehensive list of practical examples of use cases of AI systems that are high-risk and not high-risk.\n6. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend paragraph 3,\nsecond subparagraph, of this Article by adding new conditions to those laid down therein, or by modifying them, where\nthere is concrete and reliable evidence of the existence of AI systems that fall under the scope of Annex III, but do not pose\na significant risk of harm to the health, safety or fundamental rights of natural persons.\n7. The Com...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0026", "generated_at": "2025-05-14T17:31:19.306480", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "7a60a747-2a9f-40f3-8555-1cc5d532ccc0", "question": "What aspects must be considered regarding data collection processes for high-risk AI systems?", "answer": "Data collection processes and the origin of data, and in the case of personal data, the original purpose of the data collection.", "context": "(b)data collection processes and the origin of data, and in the case of personal data, the original purpose of the data\ncollection;\n\n(c) relevant data-preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and\naggregation;\n\n(d)the formulation of assumptions, in particular with respect to the information that the data are supposed to measure and\nrepresent;\n\n(e) an assessment of the availability, quantity and suitability of the data sets that are needed;\n\n(f) examination in view of possible biases that are likely to affect the health and safety of persons, have a negative impact\non fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence\ninputs for future operations;\n\n(g) appropriate measures to detect, prevent and mitigate possible biases identified according to point (f);\n\n(h)the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how those\ngaps and shortcomings can be addressed.\n\n3. Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible,\nfree of errors and complete in...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0027", "generated_at": "2025-05-14T17:32:16.902840", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "66ea3704-3368-4a6f-b118-c2bd0cfb573b", "question": "What measures are required to address potential biases in high-risk AI systems?", "answer": "Appropriate measures to detect, prevent and mitigate possible biases identified according to point (f).", "context": "(b)data collection processes and the origin of data, and in the case of personal data, the original purpose of the data\ncollection;\n\n(c) relevant data-preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and\naggregation;\n\n(d)the formulation of assumptions, in particular with respect to the information that the data are supposed to measure and\nrepresent;\n\n(e) an assessment of the availability, quantity and suitability of the data sets that are needed;\n\n(f) examination in view of possible biases that are likely to affect the health and safety of persons, have a negative impact\non fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence\ninputs for future operations;\n\n(g) appropriate measures to detect, prevent and mitigate possible biases identified according to point (f);\n\n(h)the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how those\ngaps and shortcomings can be addressed.\n\n3. Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible,\nfree of errors and complete in...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0027", "generated_at": "2025-05-14T17:32:16.904663", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "6228fc4d-de8d-45e9-942c-f7df8bc2038f", "question": "What are the requirements for training, validation, and testing data sets?", "answer": "Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible, free of errors and complete in view of the intended purpose.", "context": "(b)data collection processes and the origin of data, and in the case of personal data, the original purpose of the data\ncollection;\n\n(c) relevant data-preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and\naggregation;\n\n(d)the formulation of assumptions, in particular with respect to the information that the data are supposed to measure and\nrepresent;\n\n(e) an assessment of the availability, quantity and suitability of the data sets that are needed;\n\n(f) examination in view of possible biases that are likely to affect the health and safety of persons, have a negative impact\non fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence\ninputs for future operations;\n\n(g) appropriate measures to detect, prevent and mitigate possible biases identified according to point (f);\n\n(h)the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how those\ngaps and shortcomings can be addressed.\n\n3. Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible,\nfree of errors and complete in...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0027", "generated_at": "2025-05-14T17:32:16.906345", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "ef813686-4a37-466e-b20b-f74bf33b01ec", "question": "What information must be included in the technical documentation of a high-risk AI system?", "answer": "The technical documentation shall contain, at a minimum, the elements set out in Annex IV, including characteristics, capabilities, limitations, and instructions for use.", "context": "(b)data collection processes and the origin of data, and in the case of personal data, the original purpose of the data\ncollection;\n\n(c) relevant data-preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and\naggregation;\n\n(d)the formulation of assumptions, in particular with respect to the information that the data are supposed to measure and\nrepresent;\n\n(e) an assessment of the availability, quantity and suitability of the data sets that are needed;\n\n(f) examination in view of possible biases that are likely to affect the health and safety of persons, have a negative impact\non fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence\ninputs for future operations;\n\n(g) appropriate measures to detect, prevent and mitigate possible biases identified according to point (f);\n\n(h)the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how those\ngaps and shortcomings can be addressed.\n\n3. Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible,\nfree of errors and complete in...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0027", "generated_at": "2025-05-14T17:32:16.908040", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "263867bb-f5b1-474f-a14b-01b1499fb679", "question": "What are the requirements for human oversight of high-risk AI systems?", "answer": "High-risk AI systems shall be designed to enable effective human oversight, including measures to prevent or minimize risks to health, safety, or fundamental rights.", "context": "(b)data collection processes and the origin of data, and in the case of personal data, the original purpose of the data\ncollection;\n\n(c) relevant data-preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and\naggregation;\n\n(d)the formulation of assumptions, in particular with respect to the information that the data are supposed to measure and\nrepresent;\n\n(e) an assessment of the availability, quantity and suitability of the data sets that are needed;\n\n(f) examination in view of possible biases that are likely to affect the health and safety of persons, have a negative impact\non fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence\ninputs for future operations;\n\n(g) appropriate measures to detect, prevent and mitigate possible biases identified according to point (f);\n\n(h)the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how those\ngaps and shortcomings can be addressed.\n\n3. Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible,\nfree of errors and complete in...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0027", "generated_at": "2025-05-14T17:32:16.909693", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "4d27d9f4-f63d-4a18-a380-ee388677c9a3", "question": "What measures must users take to avoid automation bias when using high-risk AI systems?", "answer": "Users must remain aware of the possible tendency to over-rely on AI outputs, correctly interpret the system's output using available tools, decide not to use the system in certain situations, and intervene in its operation through a stop button or similar procedure.", "context": "(b)to remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk\nAI system (automation bias), in particular for high-risk AI systems used to provide information or recommendations for\ndecisions to be taken by natural persons;\n\n(c)to correctly interpret the high-risk AI system s output, taking into account, for example, the interpretation tools and\nmethods available;\n\n# EN OJ L, 12.7.2024\n\n60/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\n(d)to decide, in any particular situation, not to use the high-risk AI system or to otherwise disregard, override or reverse\nthe output of the high-risk AI system;\n\n(e)to intervene in the operation of the high-risk AI system or interrupt the system through a  stop  button or a similar\nprocedure that allows the system to come to a halt in a safe state.\n\n5. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 of this Article\nshall be such as to ensure that, in addition, no action or decision is taken by the deployer on the basis of the identification\nresulting from the system unless that ide...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0028", "generated_at": "2025-05-14T17:33:05.981671", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "2943aaa4-0842-493f-b353-9ebfbba052b8", "question": "What are the accuracy, robustness, and cybersecurity requirements for high-risk AI systems?", "answer": "High-risk AI systems must achieve an appropriate level of accuracy, robustness, and cybersecurity throughout their lifecycle, with technical solutions to address vulnerabilities and prevent manipulation of training data or inputs.", "context": "(b)to remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk\nAI system (automation bias), in particular for high-risk AI systems used to provide information or recommendations for\ndecisions to be taken by natural persons;\n\n(c)to correctly interpret the high-risk AI system s output, taking into account, for example, the interpretation tools and\nmethods available;\n\n# EN OJ L, 12.7.2024\n\n60/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\n(d)to decide, in any particular situation, not to use the high-risk AI system or to otherwise disregard, override or reverse\nthe output of the high-risk AI system;\n\n(e)to intervene in the operation of the high-risk AI system or interrupt the system through a  stop  button or a similar\nprocedure that allows the system to come to a halt in a safe state.\n\n5. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 of this Article\nshall be such as to ensure that, in addition, no action or decision is taken by the deployer on the basis of the identification\nresulting from the system unless that ide...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0028", "generated_at": "2025-05-14T17:33:05.983494", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "bc51a425-fbdb-4e64-946b-0d153a4ccf30", "question": "What obligations do providers of high-risk AI systems have under the regulation?", "answer": "Providers must ensure compliance with Section 2 requirements, maintain quality management systems, keep documentation, ensure conformity assessments, affix CE marking, and comply with registration and corrective action obligations.", "context": "(b)to remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk\nAI system (automation bias), in particular for high-risk AI systems used to provide information or recommendations for\ndecisions to be taken by natural persons;\n\n(c)to correctly interpret the high-risk AI system s output, taking into account, for example, the interpretation tools and\nmethods available;\n\n# EN OJ L, 12.7.2024\n\n60/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\n(d)to decide, in any particular situation, not to use the high-risk AI system or to otherwise disregard, override or reverse\nthe output of the high-risk AI system;\n\n(e)to intervene in the operation of the high-risk AI system or interrupt the system through a  stop  button or a similar\nprocedure that allows the system to come to a halt in a safe state.\n\n5. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 of this Article\nshall be such as to ensure that, in addition, no action or decision is taken by the deployer on the basis of the identification\nresulting from the system unless that ide...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0028", "generated_at": "2025-05-14T17:33:05.985149", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "52368fde-e207-42ea-b85c-6e8480346155", "question": "What aspects must a quality management system for high-risk AI systems include?", "answer": "The system must include regulatory compliance strategies, design and development procedures, risk management, post-market monitoring, incident reporting, data management, and accountability frameworks.", "context": "(b)to remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk\nAI system (automation bias), in particular for high-risk AI systems used to provide information or recommendations for\ndecisions to be taken by natural persons;\n\n(c)to correctly interpret the high-risk AI system s output, taking into account, for example, the interpretation tools and\nmethods available;\n\n# EN OJ L, 12.7.2024\n\n60/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\n(d)to decide, in any particular situation, not to use the high-risk AI system or to otherwise disregard, override or reverse\nthe output of the high-risk AI system;\n\n(e)to intervene in the operation of the high-risk AI system or interrupt the system through a  stop  button or a similar\nprocedure that allows the system to come to a halt in a safe state.\n\n5. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 of this Article\nshall be such as to ensure that, in addition, no action or decision is taken by the deployer on the basis of the identification\nresulting from the system unless that ide...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0028", "generated_at": "2025-05-14T17:33:05.986789", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "733dc0e7-da4c-4f0b-a995-8cefbb5e7e22", "question": "How long must providers retain technical documentation for high-risk AI systems?", "answer": "Providers must retain technical documentation, quality management records, and other relevant documents for 10 years after the system is placed on the market or put into service.", "context": "(b)to remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk\nAI system (automation bias), in particular for high-risk AI systems used to provide information or recommendations for\ndecisions to be taken by natural persons;\n\n(c)to correctly interpret the high-risk AI system s output, taking into account, for example, the interpretation tools and\nmethods available;\n\n# EN OJ L, 12.7.2024\n\n60/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\n(d)to decide, in any particular situation, not to use the high-risk AI system or to otherwise disregard, override or reverse\nthe output of the high-risk AI system;\n\n(e)to intervene in the operation of the high-risk AI system or interrupt the system through a  stop  button or a similar\nprocedure that allows the system to come to a halt in a safe state.\n\n5. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 of this Article\nshall be such as to ensure that, in addition, no action or decision is taken by the deployer on the basis of the identification\nresulting from the system unless that ide...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0028", "generated_at": "2025-05-14T17:33:05.988426", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "1fa4e966-46b7-48d9-9def-f1df6cec0709", "question": "What actions must providers take if a high-risk AI system is not in conformity with the Regulation?", "answer": "Providers must immediately take corrective actions to bring the system into conformity, withdraw, disable, or recall it. They must also inform distributors, deployers, authorised representatives, and importers.", "context": "1. Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system that they have\nplaced on the market or put into service is not in conformity with this Regulation shall immediately take the necessary\ncorrective actions to bring that system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They shall\ninform the distributors of the high-risk AI system concerned and, where applicable, the deployers, the authorised\nrepresentative and importers accordingly.\n2. Where the high-risk AI system presents a risk within the meaning of Article 79(1) and the provider becomes aware of\nthat risk, it shall immediately investigate the causes, in collaboration with the reporting deployer, where applicable, and\ninform the market surveillance authorities competent for the high-risk AI system concerned and, where applicable, the\nnotified body that issued a certificate for that high-risk AI system in accordance with Article 44, in particular, of the nature\nof the non-compliance and of any relevant corrective action taken.\n\nArticle 21\n\nCooperation with competent authorities\n\n1. Providers of high-risk AI systems shall, upon a reasoned r...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0029", "generated_at": "2025-05-14T17:34:25.657489", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "6c9d0f45-24b6-48ca-9ea3-929ad0d1f350", "question": "What must providers do upon a request by a competent authority regarding conformity documentation?", "answer": "Providers must provide all necessary information and documentation in an official Union language to demonstrate conformity, including access to automatically generated logs.", "context": "1. Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system that they have\nplaced on the market or put into service is not in conformity with this Regulation shall immediately take the necessary\ncorrective actions to bring that system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They shall\ninform the distributors of the high-risk AI system concerned and, where applicable, the deployers, the authorised\nrepresentative and importers accordingly.\n2. Where the high-risk AI system presents a risk within the meaning of Article 79(1) and the provider becomes aware of\nthat risk, it shall immediately investigate the causes, in collaboration with the reporting deployer, where applicable, and\ninform the market surveillance authorities competent for the high-risk AI system concerned and, where applicable, the\nnotified body that issued a certificate for that high-risk AI system in accordance with Article 44, in particular, of the nature\nof the non-compliance and of any relevant corrective action taken.\n\nArticle 21\n\nCooperation with competent authorities\n\n1. Providers of high-risk AI systems shall, upon a reasoned r...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0029", "generated_at": "2025-05-14T17:34:25.659310", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "73f38613-8eb7-4c72-a65f-ff1739928073", "question": "What tasks must an authorised representative of a provider perform?", "answer": "The authorised representative must verify conformity documentation, keep provider contact details for 10 years, provide information to authorities, cooperate in risk mitigation, and ensure correct registration details.", "context": "1. Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system that they have\nplaced on the market or put into service is not in conformity with this Regulation shall immediately take the necessary\ncorrective actions to bring that system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They shall\ninform the distributors of the high-risk AI system concerned and, where applicable, the deployers, the authorised\nrepresentative and importers accordingly.\n2. Where the high-risk AI system presents a risk within the meaning of Article 79(1) and the provider becomes aware of\nthat risk, it shall immediately investigate the causes, in collaboration with the reporting deployer, where applicable, and\ninform the market surveillance authorities competent for the high-risk AI system concerned and, where applicable, the\nnotified body that issued a certificate for that high-risk AI system in accordance with Article 44, in particular, of the nature\nof the non-compliance and of any relevant corrective action taken.\n\nArticle 21\n\nCooperation with competent authorities\n\n1. Providers of high-risk AI systems shall, upon a reasoned r...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0029", "generated_at": "2025-05-14T17:34:25.660971", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "be1cdaea-8aec-4364-8e98-bde01dc11d03", "question": "What obligations do importers have before placing a high-risk AI system on the market?", "answer": "Importers must verify the provider has completed conformity assessments, technical documentation, CE marking, and appointed an authorised representative.", "context": "1. Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system that they have\nplaced on the market or put into service is not in conformity with this Regulation shall immediately take the necessary\ncorrective actions to bring that system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They shall\ninform the distributors of the high-risk AI system concerned and, where applicable, the deployers, the authorised\nrepresentative and importers accordingly.\n2. Where the high-risk AI system presents a risk within the meaning of Article 79(1) and the provider becomes aware of\nthat risk, it shall immediately investigate the causes, in collaboration with the reporting deployer, where applicable, and\ninform the market surveillance authorities competent for the high-risk AI system concerned and, where applicable, the\nnotified body that issued a certificate for that high-risk AI system in accordance with Article 44, in particular, of the nature\nof the non-compliance and of any relevant corrective action taken.\n\nArticle 21\n\nCooperation with competent authorities\n\n1. Providers of high-risk AI systems shall, upon a reasoned r...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0029", "generated_at": "2025-05-14T17:34:25.662621", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "aca69f04-b760-4321-b24e-7cb882e9cdb3", "question": "Under what circumstances are distributors considered providers of a high-risk AI system?", "answer": "Distributors are considered providers if they alter the system's purpose, make substantial modifications, or affix their name/trademark to a system already on the market.", "context": "1. Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system that they have\nplaced on the market or put into service is not in conformity with this Regulation shall immediately take the necessary\ncorrective actions to bring that system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They shall\ninform the distributors of the high-risk AI system concerned and, where applicable, the deployers, the authorised\nrepresentative and importers accordingly.\n2. Where the high-risk AI system presents a risk within the meaning of Article 79(1) and the provider becomes aware of\nthat risk, it shall immediately investigate the causes, in collaboration with the reporting deployer, where applicable, and\ninform the market surveillance authorities competent for the high-risk AI system concerned and, where applicable, the\nnotified body that issued a certificate for that high-risk AI system in accordance with Article 44, in particular, of the nature\nof the non-compliance and of any relevant corrective action taken.\n\nArticle 21\n\nCooperation with competent authorities\n\n1. Providers of high-risk AI systems shall, upon a reasoned r...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0029", "generated_at": "2025-05-14T17:34:25.664264", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "bdc70033-0777-44bb-b5bc-79c985122400", "question": "What happens to the initial provider of a high-risk AI system when the system is modified or repurposed?", "answer": "The initial provider is no longer considered a provider of that specific AI system for the purposes of the Regulation. They must cooperate with new providers and provide necessary information and technical access to ensure compliance with obligations, unless they explicitly state the system is not intended to become high-risk.", "context": "2. Where the circumstances referred to in paragraph 1 occur, the provider that initially placed the AI system on the\nmarket or put it into service shall no longer be considered to be a provider of that specific AI system for the purposes of\nthis Regulation. That initial provider shall closely cooperate with new providers and shall make available the necessary\ninformation and provide the reasonably expected technical access and other assistance that are required for the fulfilment of\nthe obligations set out in this Regulation, in particular regarding the compliance with the conformity assessment of\nhigh-risk AI systems. This paragraph shall not apply in cases where the initial provider has clearly specified that its AI\nsystem is not to be changed into a high-risk AI system and therefore does not fall under the obligation to hand over the\ndocumentation.\n3. In the case of high-risk AI systems that are safety components of products covered by the Union harmonisation\nlegislation listed in Section A of Annex I, the product manufacturer shall be considered to be the provider of the high-risk\nAI system, and shall be subject to the obligations under Article 16 under either of the following ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0030", "generated_at": "2025-05-14T17:36:19.848343", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "265b8b4a-0951-4c4f-85a1-2e8793ff6815", "question": "Under what circumstances is the product manufacturer considered the provider of a high-risk AI system?", "answer": "The product manufacturer is considered the provider if the high-risk AI system is placed on the market or put into service under their name or trademark, either alongside the product or after it has been placed on the market.", "context": "2. Where the circumstances referred to in paragraph 1 occur, the provider that initially placed the AI system on the\nmarket or put it into service shall no longer be considered to be a provider of that specific AI system for the purposes of\nthis Regulation. That initial provider shall closely cooperate with new providers and shall make available the necessary\ninformation and provide the reasonably expected technical access and other assistance that are required for the fulfilment of\nthe obligations set out in this Regulation, in particular regarding the compliance with the conformity assessment of\nhigh-risk AI systems. This paragraph shall not apply in cases where the initial provider has clearly specified that its AI\nsystem is not to be changed into a high-risk AI system and therefore does not fall under the obligation to hand over the\ndocumentation.\n3. In the case of high-risk AI systems that are safety components of products covered by the Union harmonisation\nlegislation listed in Section A of Annex I, the product manufacturer shall be considered to be the provider of the high-risk\nAI system, and shall be subject to the obligations under Article 16 under either of the following ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0030", "generated_at": "2025-05-14T17:36:19.850823", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "dcdada59-3055-4e1b-aa43-78c67f0cd10f", "question": "What obligations do providers and third parties have regarding information and technical access for high-risk AI systems?", "answer": "Providers and third parties must enter into written agreements specifying necessary information, capabilities, technical access, and other assistance to ensure compliance with the Regulation, except for third parties offering open-source tools under free licenses.", "context": "2. Where the circumstances referred to in paragraph 1 occur, the provider that initially placed the AI system on the\nmarket or put it into service shall no longer be considered to be a provider of that specific AI system for the purposes of\nthis Regulation. That initial provider shall closely cooperate with new providers and shall make available the necessary\ninformation and provide the reasonably expected technical access and other assistance that are required for the fulfilment of\nthe obligations set out in this Regulation, in particular regarding the compliance with the conformity assessment of\nhigh-risk AI systems. This paragraph shall not apply in cases where the initial provider has clearly specified that its AI\nsystem is not to be changed into a high-risk AI system and therefore does not fall under the obligation to hand over the\ndocumentation.\n3. In the case of high-risk AI systems that are safety components of products covered by the Union harmonisation\nlegislation listed in Section A of Annex I, the product manufacturer shall be considered to be the provider of the high-risk\nAI system, and shall be subject to the obligations under Article 16 under either of the following ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0030", "generated_at": "2025-05-14T17:36:19.852486", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "c80c3c0a-2f77-4381-b068-e756adaec01d", "question": "What responsibility does a deployer have regarding the input data used by a high-risk AI system?", "answer": "Deployers must ensure input data is relevant and sufficiently representative for the intended purpose of the high-risk AI system, provided they exercise control over the data.", "context": "2. Where the circumstances referred to in paragraph 1 occur, the provider that initially placed the AI system on the\nmarket or put it into service shall no longer be considered to be a provider of that specific AI system for the purposes of\nthis Regulation. That initial provider shall closely cooperate with new providers and shall make available the necessary\ninformation and provide the reasonably expected technical access and other assistance that are required for the fulfilment of\nthe obligations set out in this Regulation, in particular regarding the compliance with the conformity assessment of\nhigh-risk AI systems. This paragraph shall not apply in cases where the initial provider has clearly specified that its AI\nsystem is not to be changed into a high-risk AI system and therefore does not fall under the obligation to hand over the\ndocumentation.\n3. In the case of high-risk AI systems that are safety components of products covered by the Union harmonisation\nlegislation listed in Section A of Annex I, the product manufacturer shall be considered to be the provider of the high-risk\nAI system, and shall be subject to the obligations under Article 16 under either of the following ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0030", "generated_at": "2025-05-14T17:36:19.854126", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "e420ec2d-36c4-4eb7-be37-5fed7c531a08", "question": "What is required for deployers of high-risk AI systems regarding fundamental rights impact assessments?", "answer": "Deployers must perform a fundamental rights impact assessment prior to deployment, including descriptions of usage processes, affected groups, potential risks, and human oversight measures, for systems listed in Annex III and used by public law bodies or entities providing public services.", "context": "2. Where the circumstances referred to in paragraph 1 occur, the provider that initially placed the AI system on the\nmarket or put it into service shall no longer be considered to be a provider of that specific AI system for the purposes of\nthis Regulation. That initial provider shall closely cooperate with new providers and shall make available the necessary\ninformation and provide the reasonably expected technical access and other assistance that are required for the fulfilment of\nthe obligations set out in this Regulation, in particular regarding the compliance with the conformity assessment of\nhigh-risk AI systems. This paragraph shall not apply in cases where the initial provider has clearly specified that its AI\nsystem is not to be changed into a high-risk AI system and therefore does not fall under the obligation to hand over the\ndocumentation.\n3. In the case of high-risk AI systems that are safety components of products covered by the Union harmonisation\nlegislation listed in Section A of Annex I, the product manufacturer shall be considered to be the provider of the high-risk\nAI system, and shall be subject to the obligations under Article 16 under either of the following ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0030", "generated_at": "2025-05-14T17:36:19.855780", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "03b52e74-eebe-4917-9325-a764e95ff8d4", "question": "What are the obligations of deployers regarding high-risk AI systems under Article 27(1)?", "answer": "Deployers must perform a fundamental rights impact assessment and notify the market surveillance authority of the results.", "context": "(f) the measures to be taken in the case of the materialisation of those risks, including the arrangements for internal\ngovernance and complaint mechanisms.\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 69/144\n\n2. The obligation laid down in paragraph 1 applies to the first use of the high-risk AI system. The deployer may, in\nsimilar cases, rely on previously conducted fundamental rights impact assessments or existing impact assessments carried\nout by provider. If, during the use of the high-risk AI system, the deployer considers that any of the elements listed in\nparagraph 1 has changed or is no longer up to date, the deployer shall take the necessary steps to update the information.\n3. Once the assessment referred to in paragraph 1 of this Article has been performed, the deployer shall notify the\nmarket surveillance authority of its results, submitting the filled-out template referred to in paragraph 5 of this Article as\npart of the notification. In the case referred to in Article 46(1), deployers may be exempt from that obligation to notify.\n4. If any of the obligations laid down in this Article is already me...", "source": "EU AI Act", "category": "Obligations", "metadata": {"chunk_id": "chunk_0031", "generated_at": "2025-05-14T17:41:59.893906", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "51207c43-5d0a-4b9d-ad66-3586e6e2d1e2", "question": "How can deployers rely on previous assessments for high-risk AI systems?", "answer": "Deployers may rely on previously conducted fundamental rights impact assessments or existing ones by the provider, provided they update information if changes occur.", "context": "(f) the measures to be taken in the case of the materialisation of those risks, including the arrangements for internal\ngovernance and complaint mechanisms.\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 69/144\n\n2. The obligation laid down in paragraph 1 applies to the first use of the high-risk AI system. The deployer may, in\nsimilar cases, rely on previously conducted fundamental rights impact assessments or existing impact assessments carried\nout by provider. If, during the use of the high-risk AI system, the deployer considers that any of the elements listed in\nparagraph 1 has changed or is no longer up to date, the deployer shall take the necessary steps to update the information.\n3. Once the assessment referred to in paragraph 1 of this Article has been performed, the deployer shall notify the\nmarket surveillance authority of its results, submitting the filled-out template referred to in paragraph 5 of this Article as\npart of the notification. In the case referred to in Article 46(1), deployers may be exempt from that obligation to notify.\n4. If any of the obligations laid down in this Article is already me...", "source": "EU AI Act", "category": "Compliance", "metadata": {"chunk_id": "chunk_0031", "generated_at": "2025-05-14T17:41:59.896446", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "6d596a65-f690-4f10-96d3-0f3384d860da", "question": "What is required for a conformity assessment body to apply for notification?", "answer": "The application must include a description of conformity assessment activities, modules, AI system types, an accreditation certificate, and relevant designations from other Union legislation.", "context": "(f) the measures to be taken in the case of the materialisation of those risks, including the arrangements for internal\ngovernance and complaint mechanisms.\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 69/144\n\n2. The obligation laid down in paragraph 1 applies to the first use of the high-risk AI system. The deployer may, in\nsimilar cases, rely on previously conducted fundamental rights impact assessments or existing impact assessments carried\nout by provider. If, during the use of the high-risk AI system, the deployer considers that any of the elements listed in\nparagraph 1 has changed or is no longer up to date, the deployer shall take the necessary steps to update the information.\n3. Once the assessment referred to in paragraph 1 of this Article has been performed, the deployer shall notify the\nmarket surveillance authority of its results, submitting the filled-out template referred to in paragraph 5 of this Article as\npart of the notification. In the case referred to in Article 46(1), deployers may be exempt from that obligation to notify.\n4. If any of the obligations laid down in this Article is already me...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0031", "generated_at": "2025-05-14T17:41:59.898150", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "0a929a38-06bb-4328-a94d-2434481af0fb", "question": "What is the timeline for objections to a conformity assessment body's notification?", "answer": "Objections must be raised within two weeks if an accreditation certificate is provided, or within two months if documentary evidence is submitted.", "context": "(f) the measures to be taken in the case of the materialisation of those risks, including the arrangements for internal\ngovernance and complaint mechanisms.\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 69/144\n\n2. The obligation laid down in paragraph 1 applies to the first use of the high-risk AI system. The deployer may, in\nsimilar cases, rely on previously conducted fundamental rights impact assessments or existing impact assessments carried\nout by provider. If, during the use of the high-risk AI system, the deployer considers that any of the elements listed in\nparagraph 1 has changed or is no longer up to date, the deployer shall take the necessary steps to update the information.\n3. Once the assessment referred to in paragraph 1 of this Article has been performed, the deployer shall notify the\nmarket surveillance authority of its results, submitting the filled-out template referred to in paragraph 5 of this Article as\npart of the notification. In the case referred to in Article 46(1), deployers may be exempt from that obligation to notify.\n4. If any of the obligations laid down in this Article is already me...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0031", "generated_at": "2025-05-14T17:41:59.899834", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "8060aaf0-a4de-4e7e-bdca-0a0485446bbb", "question": "What independence requirements apply to notified bodies?", "answer": "Notified bodies must be independent of providers, competitors, and any entities with economic interests in high-risk AI systems, ensuring impartiality and avoiding conflicts of interest.", "context": "(f) the measures to be taken in the case of the materialisation of those risks, including the arrangements for internal\ngovernance and complaint mechanisms.\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 69/144\n\n2. The obligation laid down in paragraph 1 applies to the first use of the high-risk AI system. The deployer may, in\nsimilar cases, rely on previously conducted fundamental rights impact assessments or existing impact assessments carried\nout by provider. If, during the use of the high-risk AI system, the deployer considers that any of the elements listed in\nparagraph 1 has changed or is no longer up to date, the deployer shall take the necessary steps to update the information.\n3. Once the assessment referred to in paragraph 1 of this Article has been performed, the deployer shall notify the\nmarket surveillance authority of its results, submitting the filled-out template referred to in paragraph 5 of this Article as\npart of the notification. In the case referred to in Article 46(1), deployers may be exempt from that obligation to notify.\n4. If any of the obligations laid down in this Article is already me...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0031", "generated_at": "2025-05-14T17:41:59.901490", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "20c2683a-7b24-414a-8551-1c202e4b16b9", "question": "What must a notified body ensure when subcontracting tasks connected with the conformity assessment?", "answer": "The notified body must ensure that the subcontractor or subsidiary meets the requirements laid down in Article 31 and inform the notifying authority accordingly.", "context": "Where a conformity assessment body demonstrates its conformity with the criteria laid down in the relevant harmonised\nstandards or parts thereof, the references of which have been published in the Official Journal of the European Union, it shall\nbe presumed to comply with the requirements set out in Article 31 in so far as the applicable harmonised standards cover\nthose requirements.\n\n# EN OJ L, 12.7.2024\n\n72/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\nArticle 33\n\nSubsidiaries of notified bodies and subcontracting\n\n1. Where a notified body subcontracts specific tasks connected with the conformity assessment or has recourse to\na subsidiary, it shall ensure that the subcontractor or the subsidiary meets the requirements laid down in Article 31, and\nshall inform the notifying authority accordingly.\n2. Notified bodies shall take full responsibility for the tasks performed by any subcontractors or subsidiaries.\n3. Activities may be subcontracted or carried out by a subsidiary only with the agreement of the provider. Notified\nbodies shall make a list of their subsidiaries publicly available.\n4. The relevant documents concerning the a...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0032", "generated_at": "2025-05-14T17:38:05.849323", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "12328389-af93-489c-b42f-952bc400849c", "question": "What obligations do notified bodies have regarding high-risk AI systems under Article 34?", "answer": "Notified bodies must verify the conformity of high-risk AI systems, avoid unnecessary burdens for providers, and make relevant documentation available to the notifying authority.", "context": "Where a conformity assessment body demonstrates its conformity with the criteria laid down in the relevant harmonised\nstandards or parts thereof, the references of which have been published in the Official Journal of the European Union, it shall\nbe presumed to comply with the requirements set out in Article 31 in so far as the applicable harmonised standards cover\nthose requirements.\n\n# EN OJ L, 12.7.2024\n\n72/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\nArticle 33\n\nSubsidiaries of notified bodies and subcontracting\n\n1. Where a notified body subcontracts specific tasks connected with the conformity assessment or has recourse to\na subsidiary, it shall ensure that the subcontractor or the subsidiary meets the requirements laid down in Article 31, and\nshall inform the notifying authority accordingly.\n2. Notified bodies shall take full responsibility for the tasks performed by any subcontractors or subsidiaries.\n3. Activities may be subcontracted or carried out by a subsidiary only with the agreement of the provider. Notified\nbodies shall make a list of their subsidiaries publicly available.\n4. The relevant documents concerning the a...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0032", "generated_at": "2025-05-14T17:38:05.852210", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "c0639b9c-ace8-4d55-ab50-d934efe43470", "question": "How does the Commission assign identification numbers to notified bodies?", "answer": "The Commission shall assign a single identification number to each notified body, even if notified under multiple Union acts.", "context": "Where a conformity assessment body demonstrates its conformity with the criteria laid down in the relevant harmonised\nstandards or parts thereof, the references of which have been published in the Official Journal of the European Union, it shall\nbe presumed to comply with the requirements set out in Article 31 in so far as the applicable harmonised standards cover\nthose requirements.\n\n# EN OJ L, 12.7.2024\n\n72/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\nArticle 33\n\nSubsidiaries of notified bodies and subcontracting\n\n1. Where a notified body subcontracts specific tasks connected with the conformity assessment or has recourse to\na subsidiary, it shall ensure that the subcontractor or the subsidiary meets the requirements laid down in Article 31, and\nshall inform the notifying authority accordingly.\n2. Notified bodies shall take full responsibility for the tasks performed by any subcontractors or subsidiaries.\n3. Activities may be subcontracted or carried out by a subsidiary only with the agreement of the provider. Notified\nbodies shall make a list of their subsidiaries publicly available.\n4. The relevant documents concerning the a...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0032", "generated_at": "2025-05-14T17:38:05.854224", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "f7f8db56-4649-4710-a282-7ee8828ae69c", "question": "What is the validity period of certificates after a notified body ceases its conformity assessment activities?", "answer": "Certificates may remain valid for nine months after cessation, provided another notified body assumes responsibility and completes a full assessment.", "context": "Where a conformity assessment body demonstrates its conformity with the criteria laid down in the relevant harmonised\nstandards or parts thereof, the references of which have been published in the Official Journal of the European Union, it shall\nbe presumed to comply with the requirements set out in Article 31 in so far as the applicable harmonised standards cover\nthose requirements.\n\n# EN OJ L, 12.7.2024\n\n72/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\nArticle 33\n\nSubsidiaries of notified bodies and subcontracting\n\n1. Where a notified body subcontracts specific tasks connected with the conformity assessment or has recourse to\na subsidiary, it shall ensure that the subcontractor or the subsidiary meets the requirements laid down in Article 31, and\nshall inform the notifying authority accordingly.\n2. Notified bodies shall take full responsibility for the tasks performed by any subcontractors or subsidiaries.\n3. Activities may be subcontracted or carried out by a subsidiary only with the agreement of the provider. Notified\nbodies shall make a list of their subsidiaries publicly available.\n4. The relevant documents concerning the a...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0032", "generated_at": "2025-05-14T17:38:05.856245", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "ad68a866-6a32-4eeb-bdb9-45dbb165ac2e", "question": "What is the Commission's role in investigating doubts about a notified body's competence?", "answer": "The Commission must investigate cases where there are reasons to doubt a notified body's competence or continued fulfillment of Article 31 requirements.", "context": "Where a conformity assessment body demonstrates its conformity with the criteria laid down in the relevant harmonised\nstandards or parts thereof, the references of which have been published in the Official Journal of the European Union, it shall\nbe presumed to comply with the requirements set out in Article 31 in so far as the applicable harmonised standards cover\nthose requirements.\n\n# EN OJ L, 12.7.2024\n\n72/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\nArticle 33\n\nSubsidiaries of notified bodies and subcontracting\n\n1. Where a notified body subcontracts specific tasks connected with the conformity assessment or has recourse to\na subsidiary, it shall ensure that the subcontractor or the subsidiary meets the requirements laid down in Article 31, and\nshall inform the notifying authority accordingly.\n2. Notified bodies shall take full responsibility for the tasks performed by any subcontractors or subsidiaries.\n3. Activities may be subcontracted or carried out by a subsidiary only with the agreement of the provider. Notified\nbodies shall make a list of their subsidiaries publicly available.\n4. The relevant documents concerning the a...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0032", "generated_at": "2025-05-14T17:38:05.858352", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "14ce18f2-48fb-4db1-864a-c48120f70da1", "question": "What conditions allow conformity assessment bodies from third countries to act as notified bodies under this Regulation?", "answer": "They must meet the requirements in Article 31 or ensure an equivalent level of compliance.", "context": "Conformity assessment bodies established under the law of a third country with which the Union has concluded an\nagreement may be authorised to carry out the activities of notified bodies under this Regulation, provided that they meet\nthe requirements laid down in Article 31 or they ensure an equivalent level of compliance.\n\nSECTION 5\nStandards, conformity assessment, certificates, registration\n\nArticle 40\n\nHarmonised standards and standardisation deliverables\n\n1. High-risk AI systems or general-purpose AI models which are in conformity with harmonised standards or parts\nthereof the references of which have been published in the Official Journal of the European Union in accordance with\nRegulation (EU) No 1025/2012 shall be presumed to be in conformity with the requirements set out in Section 2 of this\nChapter or, as applicable, with the obligations set out in of Chapter V, Sections 2 and 3, of this Regulation, to the extent that\nthose standards cover those requirements or obligations.\n2. In accordance with Article 10 of Regulation (EU) No 1025/2012, the Commission shall issue, without undue delay,\nstandardisation requests covering all requirements set out in Section 2 of this Chapte...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0033", "generated_at": "2025-05-14T17:40:21.573979", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "ba76057a-8ad4-487a-bf40-193bef1200df", "question": "What is the presumption of conformity for high-risk AI systems aligned with harmonised standards?", "answer": "They are presumed to comply with Section 2 requirements or Chapter V obligations if the standards cover those aspects.", "context": "Conformity assessment bodies established under the law of a third country with which the Union has concluded an\nagreement may be authorised to carry out the activities of notified bodies under this Regulation, provided that they meet\nthe requirements laid down in Article 31 or they ensure an equivalent level of compliance.\n\nSECTION 5\nStandards, conformity assessment, certificates, registration\n\nArticle 40\n\nHarmonised standards and standardisation deliverables\n\n1. High-risk AI systems or general-purpose AI models which are in conformity with harmonised standards or parts\nthereof the references of which have been published in the Official Journal of the European Union in accordance with\nRegulation (EU) No 1025/2012 shall be presumed to be in conformity with the requirements set out in Section 2 of this\nChapter or, as applicable, with the obligations set out in of Chapter V, Sections 2 and 3, of this Regulation, to the extent that\nthose standards cover those requirements or obligations.\n2. In accordance with Article 10 of Regulation (EU) No 1025/2012, the Commission shall issue, without undue delay,\nstandardisation requests covering all requirements set out in Section 2 of this Chapte...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0033", "generated_at": "2025-05-14T17:40:21.576714", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "f5ccbc49-9587-4bbe-8bee-106ee3e5518f", "question": "Under what conditions can the Commission adopt common specifications?", "answer": "If harmonised standards fail to address requirements, are not delivered on time, or insufficiently cover fundamental rights, and no references have been published in the Official Journal.", "context": "Conformity assessment bodies established under the law of a third country with which the Union has concluded an\nagreement may be authorised to carry out the activities of notified bodies under this Regulation, provided that they meet\nthe requirements laid down in Article 31 or they ensure an equivalent level of compliance.\n\nSECTION 5\nStandards, conformity assessment, certificates, registration\n\nArticle 40\n\nHarmonised standards and standardisation deliverables\n\n1. High-risk AI systems or general-purpose AI models which are in conformity with harmonised standards or parts\nthereof the references of which have been published in the Official Journal of the European Union in accordance with\nRegulation (EU) No 1025/2012 shall be presumed to be in conformity with the requirements set out in Section 2 of this\nChapter or, as applicable, with the obligations set out in of Chapter V, Sections 2 and 3, of this Regulation, to the extent that\nthose standards cover those requirements or obligations.\n2. In accordance with Article 10 of Regulation (EU) No 1025/2012, the Commission shall issue, without undue delay,\nstandardisation requests covering all requirements set out in Section 2 of this Chapte...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0033", "generated_at": "2025-05-14T17:40:21.578500", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "0a8fbc5e-4b72-408a-a11e-b3b7efe8eff6", "question": "What is the presumption of conformity for high-risk AI systems trained on specific data?", "answer": "They are presumed to comply with Article 10(4) requirements if trained and tested on data reflecting their intended use context.", "context": "Conformity assessment bodies established under the law of a third country with which the Union has concluded an\nagreement may be authorised to carry out the activities of notified bodies under this Regulation, provided that they meet\nthe requirements laid down in Article 31 or they ensure an equivalent level of compliance.\n\nSECTION 5\nStandards, conformity assessment, certificates, registration\n\nArticle 40\n\nHarmonised standards and standardisation deliverables\n\n1. High-risk AI systems or general-purpose AI models which are in conformity with harmonised standards or parts\nthereof the references of which have been published in the Official Journal of the European Union in accordance with\nRegulation (EU) No 1025/2012 shall be presumed to be in conformity with the requirements set out in Section 2 of this\nChapter or, as applicable, with the obligations set out in of Chapter V, Sections 2 and 3, of this Regulation, to the extent that\nthose standards cover those requirements or obligations.\n2. In accordance with Article 10 of Regulation (EU) No 1025/2012, the Commission shall issue, without undue delay,\nstandardisation requests covering all requirements set out in Section 2 of this Chapte...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0033", "generated_at": "2025-05-14T17:40:21.580617", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "31e3c9a6-45e1-4547-bb08-3c1eb512fe89", "question": "What conformity assessment procedures apply to high-risk AI systems using harmonised standards or common specifications?", "answer": "Providers must opt for internal control (Annex VI) or a notified body-assisted assessment (Annex VII), depending on the specific circumstances.", "context": "Conformity assessment bodies established under the law of a third country with which the Union has concluded an\nagreement may be authorised to carry out the activities of notified bodies under this Regulation, provided that they meet\nthe requirements laid down in Article 31 or they ensure an equivalent level of compliance.\n\nSECTION 5\nStandards, conformity assessment, certificates, registration\n\nArticle 40\n\nHarmonised standards and standardisation deliverables\n\n1. High-risk AI systems or general-purpose AI models which are in conformity with harmonised standards or parts\nthereof the references of which have been published in the Official Journal of the European Union in accordance with\nRegulation (EU) No 1025/2012 shall be presumed to be in conformity with the requirements set out in Section 2 of this\nChapter or, as applicable, with the obligations set out in of Chapter V, Sections 2 and 3, of this Regulation, to the extent that\nthose standards cover those requirements or obligations.\n2. In accordance with Article 10 of Regulation (EU) No 1025/2012, the Commission shall issue, without undue delay,\nstandardisation requests covering all requirements set out in Section 2 of this Chapte...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0033", "generated_at": "2025-05-14T17:40:21.582368", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "f029c507-8bb3-4bba-92fd-1d3ef84f1b5b", "question": "What constitutes a substantial modification for high-risk AI systems that continue to learn post-market?", "answer": "Changes to the high-risk AI system and its performance that have been pre-determined by the provider at the moment of the initial conformity assessment and are part of the technical documentation referred to in point 2(f) of Annex IV shall not constitute a substantial modification.", "context": "For high-risk AI systems that continue to learn after being placed on the market or put into service, changes to the high-risk\nAI system and its performance that have been pre-determined by the provider at the moment of the initial conformity\nassessment and are part of the information contained in the technical documentation referred to in point 2(f) of Annex IV,\nshall not constitute a substantial modification.\n\n5. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend Annexes VI\nand VII by updating them in light of technical progress.\n\n# EN OJ L, 12.7.2024\n\n78/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\n6. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend paragraphs 1\nand 2 of this Article in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the conformity\nassessment procedure referred to in Annex VII or parts thereof. The Commission shall adopt such delegated acts taking into\naccount the effectiveness of the conformity assessment procedure based on internal control referred to in Annex VI in\npreventi...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0034", "generated_at": "2025-05-14T17:41:26.556077", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "739246db-70a3-46d9-bd2a-a24da11307a8", "question": "What is the Commission's authority regarding Annexes VI and VII?", "answer": "The Commission is empowered to adopt delegated acts in accordance with Article 97 to amend Annexes VI and VII by updating them in light of technical progress.", "context": "For high-risk AI systems that continue to learn after being placed on the market or put into service, changes to the high-risk\nAI system and its performance that have been pre-determined by the provider at the moment of the initial conformity\nassessment and are part of the information contained in the technical documentation referred to in point 2(f) of Annex IV,\nshall not constitute a substantial modification.\n\n5. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend Annexes VI\nand VII by updating them in light of technical progress.\n\n# EN OJ L, 12.7.2024\n\n78/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\n6. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend paragraphs 1\nand 2 of this Article in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the conformity\nassessment procedure referred to in Annex VII or parts thereof. The Commission shall adopt such delegated acts taking into\naccount the effectiveness of the conformity assessment procedure based on internal control referred to in Annex VI in\npreventi...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0034", "generated_at": "2025-05-14T17:41:26.558626", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "c47182df-f16e-4e79-b392-2d8344a74a21", "question": "What are the validity periods for AI system certificates under Annex I and III?", "answer": "Certificates for AI systems covered by Annex I are valid for up to five years, while those for Annex III are valid for up to four years.", "context": "For high-risk AI systems that continue to learn after being placed on the market or put into service, changes to the high-risk\nAI system and its performance that have been pre-determined by the provider at the moment of the initial conformity\nassessment and are part of the information contained in the technical documentation referred to in point 2(f) of Annex IV,\nshall not constitute a substantial modification.\n\n5. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend Annexes VI\nand VII by updating them in light of technical progress.\n\n# EN OJ L, 12.7.2024\n\n78/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\n6. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend paragraphs 1\nand 2 of this Article in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the conformity\nassessment procedure referred to in Annex VII or parts thereof. The Commission shall adopt such delegated acts taking into\naccount the effectiveness of the conformity assessment procedure based on internal control referred to in Annex VI in\npreventi...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0034", "generated_at": "2025-05-14T17:41:26.560294", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "6c73730b-a4a1-4a24-bb25-2d4864f8f89a", "question": "What information must notified bodies provide to the notifying authority?", "answer": "Notified bodies must inform the notifying authority of technical documentation assessments, refusals, restrictions, suspensions, withdrawals, and any requests for information from market surveillance authorities.", "context": "For high-risk AI systems that continue to learn after being placed on the market or put into service, changes to the high-risk\nAI system and its performance that have been pre-determined by the provider at the moment of the initial conformity\nassessment and are part of the information contained in the technical documentation referred to in point 2(f) of Annex IV,\nshall not constitute a substantial modification.\n\n5. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend Annexes VI\nand VII by updating them in light of technical progress.\n\n# EN OJ L, 12.7.2024\n\n78/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\n6. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend paragraphs 1\nand 2 of this Article in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the conformity\nassessment procedure referred to in Annex VII or parts thereof. The Commission shall adopt such delegated acts taking into\naccount the effectiveness of the conformity assessment procedure based on internal control referred to in Annex VI in\npreventi...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0034", "generated_at": "2025-05-14T17:41:26.561971", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "29c32e6a-b842-46e6-bbbe-e243904d2a1d", "question": "Under what conditions can a market surveillance authority authorize high-risk AI systems for exceptional reasons?", "answer": "A market surveillance authority may authorize specific high-risk AI systems for exceptional reasons of public security, protection of life and health, environmental protection, or key industrial assets, provided the system complies with Section 2 requirements and the authorization is limited in duration.", "context": "For high-risk AI systems that continue to learn after being placed on the market or put into service, changes to the high-risk\nAI system and its performance that have been pre-determined by the provider at the moment of the initial conformity\nassessment and are part of the information contained in the technical documentation referred to in point 2(f) of Annex IV,\nshall not constitute a substantial modification.\n\n5. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend Annexes VI\nand VII by updating them in light of technical progress.\n\n# EN OJ L, 12.7.2024\n\n78/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\n6. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend paragraphs 1\nand 2 of this Article in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the conformity\nassessment procedure referred to in Annex VII or parts thereof. The Commission shall adopt such delegated acts taking into\naccount the effectiveness of the conformity assessment procedure based on internal control referred to in Annex VI in\npreventi...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0034", "generated_at": "2025-05-14T17:41:26.563653", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "2f82abc7-bdd9-4dbf-bb22-bd45d4452098", "question": "Which entities must register high-risk AI systems in the EU database before placing on the market or putting into service?", "answer": "The provider or authorised representative, except for high-risk AI systems referred to in point 2 of Annex III.", "context": "1. Before placing on the market or putting into service a high-risk AI system listed in Annex III, with the exception of\nhigh-risk AI systems referred to in point 2 of Annex III, the provider or, where applicable, the authorised representative\nshall register themselves and their system in the EU database referred to in Article 71.\n2. Before placing on the market or putting into service an AI system for which the provider has concluded that it is not\nhigh-risk according to Article 6(3), that provider or, where applicable, the authorised representative shall register\nthemselves and that system in the EU database referred to in Article 71.\n3. Before putting into service or using a high-risk AI system listed in Annex III, with the exception of high-risk AI\nsystems listed in point 2 of Annex III, deployers that are public authorities, Union institutions, bodies, offices or agencies or\npersons acting on their behalf shall register themselves, select the system and register its use in the EU database referred to\nin Article 71.\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 81/144\n\n4. For high-risk AI systems referred to...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0035", "generated_at": "2025-05-14T17:44:16.417397", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "5ad2ab4e-d211-4c4e-b45d-b0324d2bdab9", "question": "What is the registration obligation for AI systems not classified as high-risk?", "answer": "The provider or authorised representative must register themselves and the system in the EU database referred to in Article 71.", "context": "1. Before placing on the market or putting into service a high-risk AI system listed in Annex III, with the exception of\nhigh-risk AI systems referred to in point 2 of Annex III, the provider or, where applicable, the authorised representative\nshall register themselves and their system in the EU database referred to in Article 71.\n2. Before placing on the market or putting into service an AI system for which the provider has concluded that it is not\nhigh-risk according to Article 6(3), that provider or, where applicable, the authorised representative shall register\nthemselves and that system in the EU database referred to in Article 71.\n3. Before putting into service or using a high-risk AI system listed in Annex III, with the exception of high-risk AI\nsystems listed in point 2 of Annex III, deployers that are public authorities, Union institutions, bodies, offices or agencies or\npersons acting on their behalf shall register themselves, select the system and register its use in the EU database referred to\nin Article 71.\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 81/144\n\n4. For high-risk AI systems referred to...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0035", "generated_at": "2025-05-14T17:44:16.419246", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "bb6a116e-a3fa-40bb-8fd3-6d60d4f11ff0", "question": "What registration requirement applies to deployers of high-risk AI systems (excluding point 2 of Annex III)?", "answer": "Deployers (public authorities, institutions, etc.) must register themselves, select the system, and register its use in the EU database.", "context": "1. Before placing on the market or putting into service a high-risk AI system listed in Annex III, with the exception of\nhigh-risk AI systems referred to in point 2 of Annex III, the provider or, where applicable, the authorised representative\nshall register themselves and their system in the EU database referred to in Article 71.\n2. Before placing on the market or putting into service an AI system for which the provider has concluded that it is not\nhigh-risk according to Article 6(3), that provider or, where applicable, the authorised representative shall register\nthemselves and that system in the EU database referred to in Article 71.\n3. Before putting into service or using a high-risk AI system listed in Annex III, with the exception of high-risk AI\nsystems listed in point 2 of Annex III, deployers that are public authorities, Union institutions, bodies, offices or agencies or\npersons acting on their behalf shall register themselves, select the system and register its use in the EU database referred to\nin Article 71.\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 81/144\n\n4. For high-risk AI systems referred to...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0035", "generated_at": "2025-05-14T17:44:16.420927", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "b8484b43-1222-42e2-b314-99ac821ebf40", "question": "How are high-risk AI systems in points 1, 6, and 7 of Annex III registered?", "answer": "They must be registered in a secure non-public section of the EU database, accessible only to the Commission and national authorities specified in Article 74(8).", "context": "1. Before placing on the market or putting into service a high-risk AI system listed in Annex III, with the exception of\nhigh-risk AI systems referred to in point 2 of Annex III, the provider or, where applicable, the authorised representative\nshall register themselves and their system in the EU database referred to in Article 71.\n2. Before placing on the market or putting into service an AI system for which the provider has concluded that it is not\nhigh-risk according to Article 6(3), that provider or, where applicable, the authorised representative shall register\nthemselves and that system in the EU database referred to in Article 71.\n3. Before putting into service or using a high-risk AI system listed in Annex III, with the exception of high-risk AI\nsystems listed in point 2 of Annex III, deployers that are public authorities, Union institutions, bodies, offices or agencies or\npersons acting on their behalf shall register themselves, select the system and register its use in the EU database referred to\nin Article 71.\n\n# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 81/144\n\n4. For high-risk AI systems referred to...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0035", "generated_at": "2025-05-14T17:44:16.422961", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "3b6cfd3e-c3d8-4eef-bcae-644f0d9695ba", "question": "What obligations do providers of general-purpose AI models have regarding information and documentation for AI system integrators?", "answer": "Providers must draw up, keep up-to-date, and make available information and documentation to enable integrators to understand the model's capabilities, limitations, and comply with obligations. This includes the elements in Annex XII.", "context": "(b)draw up, keep up-to-date and make available information and documentation to providers of AI systems who intend to\nintegrate the general-purpose AI model into their AI systems. Without prejudice to the need to observe and protect\nintellectual property rights and confidential business information or trade secrets in accordance with Union and\nnational law, the information and documentation shall:\n\n(i)enable providers of AI systems to have a good understanding of the capabilities and limitations of the\ngeneral-purpose AI model and to comply with their obligations pursuant to this Regulation; and\n\n(ii)contain, at a minimum, the elements set out in Annex XII;\n\n(c)put in place a policy to comply with Union law on copyright and related rights, and in particular to identify and comply\nwith, including through state-of-the-art technologies, a reservation of rights expressed pursuant to Article 4(3) of\nDirective (EU) 2019/790;\n\n(d)draw up and make publicly available a sufficiently detailed summary about the content used for training of the\ngeneral-purpose AI model, according to a template provided by the AI Office.\n\n# EN OJ L, 12.7.2024\n\n84/144 ELI: [http://data.europa.eu/eli/reg/2024/1689...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0036", "generated_at": "2025-05-14T17:43:23.281789", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "30c7fee7-a51d-4ad8-9323-e8aa03760fe2", "question": "What policy must providers implement regarding Union law on copyright and related rights?", "answer": "Providers must put in place a policy to identify and comply with reservations of rights under Article 4(3) of Directive (EU) 2019/790, using state-of-the-art technologies.", "context": "(b)draw up, keep up-to-date and make available information and documentation to providers of AI systems who intend to\nintegrate the general-purpose AI model into their AI systems. Without prejudice to the need to observe and protect\nintellectual property rights and confidential business information or trade secrets in accordance with Union and\nnational law, the information and documentation shall:\n\n(i)enable providers of AI systems to have a good understanding of the capabilities and limitations of the\ngeneral-purpose AI model and to comply with their obligations pursuant to this Regulation; and\n\n(ii)contain, at a minimum, the elements set out in Annex XII;\n\n(c)put in place a policy to comply with Union law on copyright and related rights, and in particular to identify and comply\nwith, including through state-of-the-art technologies, a reservation of rights expressed pursuant to Article 4(3) of\nDirective (EU) 2019/790;\n\n(d)draw up and make publicly available a sufficiently detailed summary about the content used for training of the\ngeneral-purpose AI model, according to a template provided by the AI Office.\n\n# EN OJ L, 12.7.2024\n\n84/144 ELI: [http://data.europa.eu/eli/reg/2024/1689...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0036", "generated_at": "2025-05-14T17:43:23.283826", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "eae58d3b-9f14-4cf4-b56e-62bc79be9c83", "question": "What requirements are there for the summary of training content used in general-purpose AI models?", "answer": "A sufficiently detailed summary of the training content must be drawn up and made publicly available using a template provided by the AI Office.", "context": "(b)draw up, keep up-to-date and make available information and documentation to providers of AI systems who intend to\nintegrate the general-purpose AI model into their AI systems. Without prejudice to the need to observe and protect\nintellectual property rights and confidential business information or trade secrets in accordance with Union and\nnational law, the information and documentation shall:\n\n(i)enable providers of AI systems to have a good understanding of the capabilities and limitations of the\ngeneral-purpose AI model and to comply with their obligations pursuant to this Regulation; and\n\n(ii)contain, at a minimum, the elements set out in Annex XII;\n\n(c)put in place a policy to comply with Union law on copyright and related rights, and in particular to identify and comply\nwith, including through state-of-the-art technologies, a reservation of rights expressed pursuant to Article 4(3) of\nDirective (EU) 2019/790;\n\n(d)draw up and make publicly available a sufficiently detailed summary about the content used for training of the\ngeneral-purpose AI model, according to a template provided by the AI Office.\n\n# EN OJ L, 12.7.2024\n\n84/144 ELI: [http://data.europa.eu/eli/reg/2024/1689...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0036", "generated_at": "2025-05-14T17:43:23.285509", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "5ca2d24c-0a1a-4fc9-9501-481c40558633", "question": "What tasks must an authorized representative of a provider perform under Article 54?", "answer": "The authorized representative must verify technical documentation, keep copies for 10 years, provide information for compliance checks, and cooperate with authorities on AI model-related actions.", "context": "(b)draw up, keep up-to-date and make available information and documentation to providers of AI systems who intend to\nintegrate the general-purpose AI model into their AI systems. Without prejudice to the need to observe and protect\nintellectual property rights and confidential business information or trade secrets in accordance with Union and\nnational law, the information and documentation shall:\n\n(i)enable providers of AI systems to have a good understanding of the capabilities and limitations of the\ngeneral-purpose AI model and to comply with their obligations pursuant to this Regulation; and\n\n(ii)contain, at a minimum, the elements set out in Annex XII;\n\n(c)put in place a policy to comply with Union law on copyright and related rights, and in particular to identify and comply\nwith, including through state-of-the-art technologies, a reservation of rights expressed pursuant to Article 4(3) of\nDirective (EU) 2019/790;\n\n(d)draw up and make publicly available a sufficiently detailed summary about the content used for training of the\ngeneral-purpose AI model, according to a template provided by the AI Office.\n\n# EN OJ L, 12.7.2024\n\n84/144 ELI: [http://data.europa.eu/eli/reg/2024/1689...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0036", "generated_at": "2025-05-14T17:43:23.287188", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "718dfe47-cdde-40cd-a6a7-8a6e2e064e3e", "question": "What evaluation requirements apply to general-purpose AI models with systemic risks?", "answer": "Providers must perform model evaluation using standardized protocols, including adversarial testing, to identify and mitigate systemic risks as outlined in Article 55(a).", "context": "(b)draw up, keep up-to-date and make available information and documentation to providers of AI systems who intend to\nintegrate the general-purpose AI model into their AI systems. Without prejudice to the need to observe and protect\nintellectual property rights and confidential business information or trade secrets in accordance with Union and\nnational law, the information and documentation shall:\n\n(i)enable providers of AI systems to have a good understanding of the capabilities and limitations of the\ngeneral-purpose AI model and to comply with their obligations pursuant to this Regulation; and\n\n(ii)contain, at a minimum, the elements set out in Annex XII;\n\n(c)put in place a policy to comply with Union law on copyright and related rights, and in particular to identify and comply\nwith, including through state-of-the-art technologies, a reservation of rights expressed pursuant to Article 4(3) of\nDirective (EU) 2019/790;\n\n(d)draw up and make publicly available a sufficiently detailed summary about the content used for training of the\ngeneral-purpose AI model, according to a template provided by the AI Office.\n\n# EN OJ L, 12.7.2024\n\n84/144 ELI: [http://data.europa.eu/eli/reg/2024/1689...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0036", "generated_at": "2025-05-14T17:43:23.288889", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "cc3c3c07-5866-454f-a75e-b07331916695", "question": "What is the deadline for Member States to establish AI regulatory sandboxes at national level?", "answer": "Member States shall ensure that their competent authorities establish at least one AI regulatory sandbox at national level, which shall be operational by 2 August 2026.", "context": "7. The AI Office may invite all providers of general-purpose AI models to adhere to the codes of practice. For providers\nof general-purpose AI models not presenting systemic risks this adherence may be limited to the obligations provided for in\nArticle 53, unless they declare explicitly their interest to join the full code.\n8. The AI Office shall, as appropriate, also encourage and facilitate the review and adaptation of the codes of practice, in\nparticular in light of emerging standards. The AI Office shall assist in the assessment of available standards.\n9. Codes of practice shall be ready at the latest by 2 May 2025. The AI Office shall take the necessary steps, including\ninviting providers pursuant to paragraph 7.\n\nIf, by 2 August 2025, a code of practice cannot be finalised, or if the AI Office deems it is not adequate following its\nassessment under paragraph 6 of this Article, the Commission may provide, by means of implementing acts, common rules\nfor the implementation of the obligations provided for in Articles 53 and 55, including the issues set out in paragraph 2 of\nthis Article. Those implementing acts shall be adopted in accordance with the examination procedure referre...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0037", "generated_at": "2025-05-14T17:49:55.589185", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "237197c1-e658-47e0-b782-c56615120e64", "question": "What is the obligation for Member States regarding AI regulatory sandboxes?", "answer": "Member States shall ensure that their competent authorities establish at least one AI regulatory sandbox at national level, which shall be operational by 2 August 2026. That sandbox may also be established jointly with the competent authorities of other Member States.", "context": "7. The AI Office may invite all providers of general-purpose AI models to adhere to the codes of practice. For providers\nof general-purpose AI models not presenting systemic risks this adherence may be limited to the obligations provided for in\nArticle 53, unless they declare explicitly their interest to join the full code.\n8. The AI Office shall, as appropriate, also encourage and facilitate the review and adaptation of the codes of practice, in\nparticular in light of emerging standards. The AI Office shall assist in the assessment of available standards.\n9. Codes of practice shall be ready at the latest by 2 May 2025. The AI Office shall take the necessary steps, including\ninviting providers pursuant to paragraph 7.\n\nIf, by 2 August 2025, a code of practice cannot be finalised, or if the AI Office deems it is not adequate following its\nassessment under paragraph 6 of this Article, the Commission may provide, by means of implementing acts, common rules\nfor the implementation of the obligations provided for in Articles 53 and 55, including the issues set out in paragraph 2 of\nthis Article. Those implementing acts shall be adopted in accordance with the examination procedure referre...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0037", "generated_at": "2025-05-14T17:49:55.591135", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "3314eab0-6491-4edb-8d30-f5fa54805508", "question": "What role does the Commission play in the establishment of AI regulatory sandboxes?", "answer": "The Commission may provide technical support, advice and tools for the establishment and operation of AI regulatory sandboxes.", "context": "7. The AI Office may invite all providers of general-purpose AI models to adhere to the codes of practice. For providers\nof general-purpose AI models not presenting systemic risks this adherence may be limited to the obligations provided for in\nArticle 53, unless they declare explicitly their interest to join the full code.\n8. The AI Office shall, as appropriate, also encourage and facilitate the review and adaptation of the codes of practice, in\nparticular in light of emerging standards. The AI Office shall assist in the assessment of available standards.\n9. Codes of practice shall be ready at the latest by 2 May 2025. The AI Office shall take the necessary steps, including\ninviting providers pursuant to paragraph 7.\n\nIf, by 2 August 2025, a code of practice cannot be finalised, or if the AI Office deems it is not adequate following its\nassessment under paragraph 6 of this Article, the Commission may provide, by means of implementing acts, common rules\nfor the implementation of the obligations provided for in Articles 53 and 55, including the issues set out in paragraph 2 of\nthis Article. Those implementing acts shall be adopted in accordance with the examination procedure referre...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0037", "generated_at": "2025-05-14T17:49:55.592894", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "812d292c-214e-4711-b38b-8aa6c04181ca", "question": "By what date must codes of practice for general-purpose AI models be ready?", "answer": "Codes of practice shall be ready at the latest by 2 May 2025.", "context": "7. The AI Office may invite all providers of general-purpose AI models to adhere to the codes of practice. For providers\nof general-purpose AI models not presenting systemic risks this adherence may be limited to the obligations provided for in\nArticle 53, unless they declare explicitly their interest to join the full code.\n8. The AI Office shall, as appropriate, also encourage and facilitate the review and adaptation of the codes of practice, in\nparticular in light of emerging standards. The AI Office shall assist in the assessment of available standards.\n9. Codes of practice shall be ready at the latest by 2 May 2025. The AI Office shall take the necessary steps, including\ninviting providers pursuant to paragraph 7.\n\nIf, by 2 August 2025, a code of practice cannot be finalised, or if the AI Office deems it is not adequate following its\nassessment under paragraph 6 of this Article, the Commission may provide, by means of implementing acts, common rules\nfor the implementation of the obligations provided for in Articles 53 and 55, including the issues set out in paragraph 2 of\nthis Article. Those implementing acts shall be adopted in accordance with the examination procedure referre...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0037", "generated_at": "2025-05-14T17:49:55.594574", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "30810ee6-fc03-42f8-b8d6-36a675bbad26", "question": "What happens if the code of practice cannot be finalised by 2 August 2025?", "answer": "The Commission may provide, by means of implementing acts, common rules for the implementation of the obligations provided for in Articles 53 and 55, including the issues set out in paragraph 2 of this Article.", "context": "7. The AI Office may invite all providers of general-purpose AI models to adhere to the codes of practice. For providers\nof general-purpose AI models not presenting systemic risks this adherence may be limited to the obligations provided for in\nArticle 53, unless they declare explicitly their interest to join the full code.\n8. The AI Office shall, as appropriate, also encourage and facilitate the review and adaptation of the codes of practice, in\nparticular in light of emerging standards. The AI Office shall assist in the assessment of available standards.\n9. Codes of practice shall be ready at the latest by 2 May 2025. The AI Office shall take the necessary steps, including\ninviting providers pursuant to paragraph 7.\n\nIf, by 2 August 2025, a code of practice cannot be finalised, or if the AI Office deems it is not adequate following its\nassessment under paragraph 6 of this Article, the Commission may provide, by means of implementing acts, common rules\nfor the implementation of the obligations provided for in Articles 53 and 55, including the issues set out in paragraph 2 of\nthis Article. Those implementing acts shall be adopted in accordance with the examination procedure referre...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0037", "generated_at": "2025-05-14T17:49:55.596247", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "1625b52e-a94a-4d19-9814-fffea9a3e888", "question": "What are the requirements for access to AI regulatory sandboxes for SMEs and start-ups?", "answer": "Access to the AI regulatory sandboxes is free of charge for SMEs, including start-ups, without prejudice to exceptional costs that national competent authorities may recover in a fair and proportionate manner.", "context": "(b)that AI regulatory sandboxes allow broad and equal access and keep up with demand for participation; providers and\nprospective providers may also submit applications in partnerships with deployers and other relevant third parties;\n\n(c) that the detailed arrangements for, and conditions concerning AI regulatory sandboxes support, to the best extent\npossible, flexibility for national competent authorities to establish and operate their AI regulatory sandboxes;\n\n(d)that access to the AI regulatory sandboxes is free of charge for SMEs, including start-ups, without prejudice to\nexceptional costs that national competent authorities may recover in a fair and proportionate manner;\n\n(e) that they facilitate providers and prospective providers, by means of the learning outcomes of the AI regulatory\nsandboxes, in complying with conformity assessment obligations under this Regulation and the voluntary application\nof the codes of conduct referred to in Article 95;\n\n(f) that AI regulatory sandboxes facilitate the involvement of other relevant actors within the AI ecosystem, such as notified\nbodies and standardisation organisations, SMEs, including start-ups, enterprises, innovators, testing a...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0038", "generated_at": "2025-05-14T17:46:06.665576", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "c533419d-675b-4faf-bd4c-efbec74206f3", "question": "How do AI regulatory sandboxes support national competent authorities?", "answer": "The detailed arrangements for, and conditions concerning AI regulatory sandboxes support, to the best extent possible, flexibility for national competent authorities to establish and operate their AI regulatory sandboxes.", "context": "(b)that AI regulatory sandboxes allow broad and equal access and keep up with demand for participation; providers and\nprospective providers may also submit applications in partnerships with deployers and other relevant third parties;\n\n(c) that the detailed arrangements for, and conditions concerning AI regulatory sandboxes support, to the best extent\npossible, flexibility for national competent authorities to establish and operate their AI regulatory sandboxes;\n\n(d)that access to the AI regulatory sandboxes is free of charge for SMEs, including start-ups, without prejudice to\nexceptional costs that national competent authorities may recover in a fair and proportionate manner;\n\n(e) that they facilitate providers and prospective providers, by means of the learning outcomes of the AI regulatory\nsandboxes, in complying with conformity assessment obligations under this Regulation and the voluntary application\nof the codes of conduct referred to in Article 95;\n\n(f) that AI regulatory sandboxes facilitate the involvement of other relevant actors within the AI ecosystem, such as notified\nbodies and standardisation organisations, SMEs, including start-ups, enterprises, innovators, testing a...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0038", "generated_at": "2025-05-14T17:46:06.668168", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "b14488e5-008f-458d-8621-c07883d569c3", "question": "What is the purpose of AI regulatory sandboxes in terms of compliance with regulations?", "answer": "AI regulatory sandboxes facilitate providers and prospective providers, by means of the learning outcomes of the AI regulatory sandboxes, in complying with conformity assessment obligations under this Regulation and the voluntary application of the codes of conduct referred to in Article 95.", "context": "(b)that AI regulatory sandboxes allow broad and equal access and keep up with demand for participation; providers and\nprospective providers may also submit applications in partnerships with deployers and other relevant third parties;\n\n(c) that the detailed arrangements for, and conditions concerning AI regulatory sandboxes support, to the best extent\npossible, flexibility for national competent authorities to establish and operate their AI regulatory sandboxes;\n\n(d)that access to the AI regulatory sandboxes is free of charge for SMEs, including start-ups, without prejudice to\nexceptional costs that national competent authorities may recover in a fair and proportionate manner;\n\n(e) that they facilitate providers and prospective providers, by means of the learning outcomes of the AI regulatory\nsandboxes, in complying with conformity assessment obligations under this Regulation and the voluntary application\nof the codes of conduct referred to in Article 95;\n\n(f) that AI regulatory sandboxes facilitate the involvement of other relevant actors within the AI ecosystem, such as notified\nbodies and standardisation organisations, SMEs, including start-ups, enterprises, innovators, testing a...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0038", "generated_at": "2025-05-14T17:46:06.670134", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "6bc382a1-a659-4f16-acef-b38ab3825f24", "question": "Which actors are involved in AI regulatory sandboxes?", "answer": "AI regulatory sandboxes facilitate the involvement of other relevant actors within the AI ecosystem, such as notified bodies and standardisation organisations, SMEs, including start-ups, enterprises, innovators, testing and experimentation facilities, research and experimentation labs and European Digital Innovation Hubs, centres of excellence, individual researchers.", "context": "(b)that AI regulatory sandboxes allow broad and equal access and keep up with demand for participation; providers and\nprospective providers may also submit applications in partnerships with deployers and other relevant third parties;\n\n(c) that the detailed arrangements for, and conditions concerning AI regulatory sandboxes support, to the best extent\npossible, flexibility for national competent authorities to establish and operate their AI regulatory sandboxes;\n\n(d)that access to the AI regulatory sandboxes is free of charge for SMEs, including start-ups, without prejudice to\nexceptional costs that national competent authorities may recover in a fair and proportionate manner;\n\n(e) that they facilitate providers and prospective providers, by means of the learning outcomes of the AI regulatory\nsandboxes, in complying with conformity assessment obligations under this Regulation and the voluntary application\nof the codes of conduct referred to in Article 95;\n\n(f) that AI regulatory sandboxes facilitate the involvement of other relevant actors within the AI ecosystem, such as notified\nbodies and standardisation organisations, SMEs, including start-ups, enterprises, innovators, testing a...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0038", "generated_at": "2025-05-14T17:46:06.671919", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "a857b053-017e-4e08-a24e-eea8f5c1d4f4", "question": "What are the administrative requirements for participating in AI regulatory sandboxes?", "answer": "Procedures, processes and administrative requirements for application, selection, participation and exiting the AI regulatory sandbox are simple, easily intelligible, and clearly communicated in order to facilitate the participation of SMEs, including start-ups, with limited legal and administrative capacities and are streamlined across the Union, in order to avoid fragmentation.", "context": "(b)that AI regulatory sandboxes allow broad and equal access and keep up with demand for participation; providers and\nprospective providers may also submit applications in partnerships with deployers and other relevant third parties;\n\n(c) that the detailed arrangements for, and conditions concerning AI regulatory sandboxes support, to the best extent\npossible, flexibility for national competent authorities to establish and operate their AI regulatory sandboxes;\n\n(d)that access to the AI regulatory sandboxes is free of charge for SMEs, including start-ups, without prejudice to\nexceptional costs that national competent authorities may recover in a fair and proportionate manner;\n\n(e) that they facilitate providers and prospective providers, by means of the learning outcomes of the AI regulatory\nsandboxes, in complying with conformity assessment obligations under this Regulation and the voluntary application\nof the codes of conduct referred to in Article 95;\n\n(f) that AI regulatory sandboxes facilitate the involvement of other relevant actors within the AI ecosystem, such as notified\nbodies and standardisation organisations, SMEs, including start-ups, enterprises, innovators, testing a...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0038", "generated_at": "2025-05-14T17:46:06.673597", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "15f68e54-b2d2-4c18-b07f-d905c59804c3", "question": "What is the maximum duration for testing in real world conditions under this regulation?", "answer": "The testing in real world conditions does not last longer than six months, which may be extended for an additional period of six months, subject to prior notification by the provider or prospective provider to the market surveillance authority.", "context": "(f) the testing in real world conditions does not last longer than necessary to achieve its objectives and in any case not\nlonger than six months, which may be extended for an additional period of six months, subject to prior notification by\nthe provider or prospective provider to the market surveillance authority, accompanied by an explanation of the need\nfor such an extension;\n\n(g) the subjects of the testing in real world conditions who are persons belonging to vulnerable groups due to their age or\ndisability, are appropriately protected;\n\n(h)where a provider or prospective provider organises the testing in real world conditions in cooperation with one or more\ndeployers or prospective deployers, the latter have been informed of all aspects of the testing that are relevant to their\ndecision to participate, and given the relevant instructions for use of the AI system referred to in Article 13; the\nprovider or prospective provider and the deployer or prospective deployer shall conclude an agreement specifying their\nroles and responsibilities with a view to ensuring compliance with the provisions for testing in real world conditions\nunder this Regulation and under other applicable U...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0039", "generated_at": "2025-05-14T17:48:15.530286", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "8668973e-921c-4cd1-b1dc-54d0db02ecf4", "question": "How are vulnerable groups protected during real world testing?", "answer": "The subjects of the testing in real world conditions who are persons belonging to vulnerable groups due to their age or disability are appropriately protected.", "context": "(f) the testing in real world conditions does not last longer than necessary to achieve its objectives and in any case not\nlonger than six months, which may be extended for an additional period of six months, subject to prior notification by\nthe provider or prospective provider to the market surveillance authority, accompanied by an explanation of the need\nfor such an extension;\n\n(g) the subjects of the testing in real world conditions who are persons belonging to vulnerable groups due to their age or\ndisability, are appropriately protected;\n\n(h)where a provider or prospective provider organises the testing in real world conditions in cooperation with one or more\ndeployers or prospective deployers, the latter have been informed of all aspects of the testing that are relevant to their\ndecision to participate, and given the relevant instructions for use of the AI system referred to in Article 13; the\nprovider or prospective provider and the deployer or prospective deployer shall conclude an agreement specifying their\nroles and responsibilities with a view to ensuring compliance with the provisions for testing in real world conditions\nunder this Regulation and under other applicable U...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0039", "generated_at": "2025-05-14T17:48:15.532832", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "839c38f3-8ae5-4f8c-8259-7274b5be56ea", "question": "What obligations apply to providers and deployers collaborating on real world testing?", "answer": "The provider or prospective provider and the deployer or prospective deployer shall conclude an agreement specifying their roles and responsibilities to ensure compliance with testing requirements under this Regulation and other applicable law.", "context": "(f) the testing in real world conditions does not last longer than necessary to achieve its objectives and in any case not\nlonger than six months, which may be extended for an additional period of six months, subject to prior notification by\nthe provider or prospective provider to the market surveillance authority, accompanied by an explanation of the need\nfor such an extension;\n\n(g) the subjects of the testing in real world conditions who are persons belonging to vulnerable groups due to their age or\ndisability, are appropriately protected;\n\n(h)where a provider or prospective provider organises the testing in real world conditions in cooperation with one or more\ndeployers or prospective deployers, the latter have been informed of all aspects of the testing that are relevant to their\ndecision to participate, and given the relevant instructions for use of the AI system referred to in Article 13; the\nprovider or prospective provider and the deployer or prospective deployer shall conclude an agreement specifying their\nroles and responsibilities with a view to ensuring compliance with the provisions for testing in real world conditions\nunder this Regulation and under other applicable U...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0039", "generated_at": "2025-05-14T17:48:15.534509", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "a57c72ad-725c-4032-b5bc-be2470768e4f", "question": "What are the data protection requirements for real world testing subjects?", "answer": "The subjects of the testing in real world conditions have given informed consent in accordance with Article 61, or in the case of law enforcement, the testing itself and its outcomes shall not have any negative effect on the subjects, and their personal data shall be deleted after the test is performed.", "context": "(f) the testing in real world conditions does not last longer than necessary to achieve its objectives and in any case not\nlonger than six months, which may be extended for an additional period of six months, subject to prior notification by\nthe provider or prospective provider to the market surveillance authority, accompanied by an explanation of the need\nfor such an extension;\n\n(g) the subjects of the testing in real world conditions who are persons belonging to vulnerable groups due to their age or\ndisability, are appropriately protected;\n\n(h)where a provider or prospective provider organises the testing in real world conditions in cooperation with one or more\ndeployers or prospective deployers, the latter have been informed of all aspects of the testing that are relevant to their\ndecision to participate, and given the relevant instructions for use of the AI system referred to in Article 13; the\nprovider or prospective provider and the deployer or prospective deployer shall conclude an agreement specifying their\nroles and responsibilities with a view to ensuring compliance with the provisions for testing in real world conditions\nunder this Regulation and under other applicable U...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0039", "generated_at": "2025-05-14T17:48:15.536335", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "1c1a1750-2ef7-46d1-adb0-e1f17a056328", "question": "Who is responsible for overseeing real world testing activities?", "answer": "The testing in real world conditions is effectively overseen by the provider or prospective provider, as well as by deployers or prospective deployers through persons who are suitably qualified in the relevant field and have the necessary capacity, training and authority to perform their tasks.", "context": "(f) the testing in real world conditions does not last longer than necessary to achieve its objectives and in any case not\nlonger than six months, which may be extended for an additional period of six months, subject to prior notification by\nthe provider or prospective provider to the market surveillance authority, accompanied by an explanation of the need\nfor such an extension;\n\n(g) the subjects of the testing in real world conditions who are persons belonging to vulnerable groups due to their age or\ndisability, are appropriately protected;\n\n(h)where a provider or prospective provider organises the testing in real world conditions in cooperation with one or more\ndeployers or prospective deployers, the latter have been informed of all aspects of the testing that are relevant to their\ndecision to participate, and given the relevant instructions for use of the AI system referred to in Article 13; the\nprovider or prospective provider and the deployer or prospective deployer shall conclude an agreement specifying their\nroles and responsibilities with a view to ensuring compliance with the provisions for testing in real world conditions\nunder this Regulation and under other applicable U...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0039", "generated_at": "2025-05-14T17:48:15.538042", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "6435b83c-8d69-4c41-816d-1a0aba94fe29", "question": "What are the tasks of the Board under Article 66?", "answer": "The Board shall advise and assist the Commission and the Member States to facilitate the consistent and effective application of the Regulation. Tasks include coordination among national authorities, sharing expertise, providing advice on enforcement, harmonizing administrative practices, issuing recommendations, supporting AI literacy, developing common criteria, cooperating with other institutions, contributing to cooperation with third countries, assisting in expertise development, supporting AI regulatory sandboxes, developing guidance documents, advising on international matters, providing opinions on qualified alerts, and receiving Member State opinions on monitoring AI systems.", "context": "7. The Board shall be organised and operated so as to safeguard the objectivity and impartiality of its activities.\n8. The Board shall be chaired by one of the representatives of the Member States. The AI Office shall provide the\nsecretariat for the Board, convene the meetings upon request of the Chair, and prepare the agenda in accordance with the\ntasks of the Board pursuant to this Regulation and its rules of procedure.\n\nArticle 66\n\nTasks of the Board\n\nThe Board shall advise and assist the Commission and the Member States in order to facilitate the consistent and effective\napplication of this Regulation. To that end, the Board may in particular:\n\n(a) contribute to the coordination among national competent authorities responsible for the application of this Regulation\nand, in cooperation with and subject to the agreement of the market surveillance authorities concerned, support joint\nactivities of market surveillance authorities referred to in Article 74(11);\n\n(b) collect and share technical and regulatory expertise and best practices among Member States;\n\n(c) provide advice on the implementation of this Regulation, in particular as regards the enforcement of rules on\ngeneral-purp...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0040", "generated_at": "2025-05-14T17:48:15.688530", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "a0cdec38-a605-4465-b687-72707fca069e", "question": "Who chairs the Board and what is the AI Office's role?", "answer": "The Board is chaired by a representative of a Member State. The AI Office provides the secretariat, convenes meetings upon the Chair's request, and prepares the agenda in accordance with the Board's tasks and rules of procedure.", "context": "7. The Board shall be organised and operated so as to safeguard the objectivity and impartiality of its activities.\n8. The Board shall be chaired by one of the representatives of the Member States. The AI Office shall provide the\nsecretariat for the Board, convene the meetings upon request of the Chair, and prepare the agenda in accordance with the\ntasks of the Board pursuant to this Regulation and its rules of procedure.\n\nArticle 66\n\nTasks of the Board\n\nThe Board shall advise and assist the Commission and the Member States in order to facilitate the consistent and effective\napplication of this Regulation. To that end, the Board may in particular:\n\n(a) contribute to the coordination among national competent authorities responsible for the application of this Regulation\nand, in cooperation with and subject to the agreement of the market surveillance authorities concerned, support joint\nactivities of market surveillance authorities referred to in Article 74(11);\n\n(b) collect and share technical and regulatory expertise and best practices among Member States;\n\n(c) provide advice on the implementation of this Regulation, in particular as regards the enforcement of rules on\ngeneral-purp...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0040", "generated_at": "2025-05-14T17:48:15.690682", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "236b918d-bdd7-427d-8725-9b2a8b3e8575", "question": "What are the criteria for membership in the advisory forum under Article 67?", "answer": "The advisory forum's membership must represent a balanced selection of stakeholders including industry, start-ups, SMEs, civil society, and academia. It must balance commercial and non-commercial interests, with particular attention to SMEs within commercial interests.", "context": "7. The Board shall be organised and operated so as to safeguard the objectivity and impartiality of its activities.\n8. The Board shall be chaired by one of the representatives of the Member States. The AI Office shall provide the\nsecretariat for the Board, convene the meetings upon request of the Chair, and prepare the agenda in accordance with the\ntasks of the Board pursuant to this Regulation and its rules of procedure.\n\nArticle 66\n\nTasks of the Board\n\nThe Board shall advise and assist the Commission and the Member States in order to facilitate the consistent and effective\napplication of this Regulation. To that end, the Board may in particular:\n\n(a) contribute to the coordination among national competent authorities responsible for the application of this Regulation\nand, in cooperation with and subject to the agreement of the market surveillance authorities concerned, support joint\nactivities of market surveillance authorities referred to in Article 74(11);\n\n(b) collect and share technical and regulatory expertise and best practices among Member States;\n\n(c) provide advice on the implementation of this Regulation, in particular as regards the enforcement of rules on\ngeneral-purp...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0040", "generated_at": "2025-05-14T17:48:15.692689", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "f6249537-3849-439d-a9bd-c8fd759327f7", "question": "What conditions must experts on the scientific panel meet under Article 68?", "answer": "Experts must have up-to-date scientific or technical expertise in AI, be independent from AI system providers, and demonstrate the ability to perform tasks diligently, accurately, and objectively. The Commission ensures fair gender and geographical representation.", "context": "7. The Board shall be organised and operated so as to safeguard the objectivity and impartiality of its activities.\n8. The Board shall be chaired by one of the representatives of the Member States. The AI Office shall provide the\nsecretariat for the Board, convene the meetings upon request of the Chair, and prepare the agenda in accordance with the\ntasks of the Board pursuant to this Regulation and its rules of procedure.\n\nArticle 66\n\nTasks of the Board\n\nThe Board shall advise and assist the Commission and the Member States in order to facilitate the consistent and effective\napplication of this Regulation. To that end, the Board may in particular:\n\n(a) contribute to the coordination among national competent authorities responsible for the application of this Regulation\nand, in cooperation with and subject to the agreement of the market surveillance authorities concerned, support joint\nactivities of market surveillance authorities referred to in Article 74(11);\n\n(b) collect and share technical and regulatory expertise and best practices among Member States;\n\n(c) provide advice on the implementation of this Regulation, in particular as regards the enforcement of rules on\ngeneral-purp...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0040", "generated_at": "2025-05-14T17:48:15.694781", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "c85bb480-acf2-4c86-b594-0cd21358c098", "question": "How do Member States access the scientific panel's experts under Article 69?", "answer": "Member States may call upon scientific panel experts to support their enforcement activities. The Commission facilitates timely access and ensures efficient organization of support activities alongside Union AI testing support.", "context": "7. The Board shall be organised and operated so as to safeguard the objectivity and impartiality of its activities.\n8. The Board shall be chaired by one of the representatives of the Member States. The AI Office shall provide the\nsecretariat for the Board, convene the meetings upon request of the Chair, and prepare the agenda in accordance with the\ntasks of the Board pursuant to this Regulation and its rules of procedure.\n\nArticle 66\n\nTasks of the Board\n\nThe Board shall advise and assist the Commission and the Member States in order to facilitate the consistent and effective\napplication of this Regulation. To that end, the Board may in particular:\n\n(a) contribute to the coordination among national competent authorities responsible for the application of this Regulation\nand, in cooperation with and subject to the agreement of the market surveillance authorities concerned, support joint\nactivities of market surveillance authorities referred to in Article 74(11);\n\n(b) collect and share technical and regulatory expertise and best practices among Member States;\n\n(c) provide advice on the implementation of this Regulation, in particular as regards the enforcement of rules on\ngeneral-purp...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0040", "generated_at": "2025-05-14T17:48:15.696821", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "dc2cd83c-1c9e-4b88-a1f9-d342ef046cd1", "question": "What are the requirements for Member States regarding national competent authorities under this Regulation?", "answer": "Each Member State shall establish or designate at least one notifying authority and at least one market surveillance authority. These authorities must exercise their powers independently, impartially, and without bias to safeguard objectivity and ensure the Regulation's application.", "context": "1. Each Member State shall establish or designate as national competent authorities at least one notifying authority and\nat least one market surveillance authority for the purposes of this Regulation. Those national competent authorities shall\nexercise their powers independently, impartially and without bias so as to safeguard the objectivity of their activities and\ntasks, and to ensure the application and implementation of this Regulation. The members of those authorities shall refrain\nfrom any action incompatible with their duties. Provided that those principles are observed, such activities and tasks may be\nperformed by one or more designated authorities, in accordance with the organisational needs of the Member State.\n2. Member States shall communicate to the Commission the identity of the notifying authorities and the market\nsurveillance authorities and the tasks of those authorities, as well as any subsequent changes thereto. Member States shall\nmake publicly available information on how competent authorities and single points of contact can be contacted, through\nelectronic communication means by 2 August 2025. Member States shall designate a market surveillance authority to ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0041", "generated_at": "2025-05-14T17:50:27.773739", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "108cf3e8-702c-4531-91fe-f630b2e79c20", "question": "When must Member States communicate the identity and tasks of their notifying and market surveillance authorities?", "answer": "Member States must communicate this information to the Commission and make it publicly available through electronic means by 2 August 2025.", "context": "1. Each Member State shall establish or designate as national competent authorities at least one notifying authority and\nat least one market surveillance authority for the purposes of this Regulation. Those national competent authorities shall\nexercise their powers independently, impartially and without bias so as to safeguard the objectivity of their activities and\ntasks, and to ensure the application and implementation of this Regulation. The members of those authorities shall refrain\nfrom any action incompatible with their duties. Provided that those principles are observed, such activities and tasks may be\nperformed by one or more designated authorities, in accordance with the organisational needs of the Member State.\n2. Member States shall communicate to the Commission the identity of the notifying authorities and the market\nsurveillance authorities and the tasks of those authorities, as well as any subsequent changes thereto. Member States shall\nmake publicly available information on how competent authorities and single points of contact can be contacted, through\nelectronic communication means by 2 August 2025. Member States shall designate a market surveillance authority to ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0041", "generated_at": "2025-05-14T17:50:27.775565", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "da7c7612-0e0a-4267-bf84-c2fccb08893a", "question": "What resources must national competent authorities have to fulfill their tasks under this Regulation?", "answer": "They must have adequate technical, financial, human resources, and infrastructure. Personnel must have expertise in AI technologies, data protection, cybersecurity, fundamental rights, and legal requirements.", "context": "1. Each Member State shall establish or designate as national competent authorities at least one notifying authority and\nat least one market surveillance authority for the purposes of this Regulation. Those national competent authorities shall\nexercise their powers independently, impartially and without bias so as to safeguard the objectivity of their activities and\ntasks, and to ensure the application and implementation of this Regulation. The members of those authorities shall refrain\nfrom any action incompatible with their duties. Provided that those principles are observed, such activities and tasks may be\nperformed by one or more designated authorities, in accordance with the organisational needs of the Member State.\n2. Member States shall communicate to the Commission the identity of the notifying authorities and the market\nsurveillance authorities and the tasks of those authorities, as well as any subsequent changes thereto. Member States shall\nmake publicly available information on how competent authorities and single points of contact can be contacted, through\nelectronic communication means by 2 August 2025. Member States shall designate a market surveillance authority to ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0041", "generated_at": "2025-05-14T17:50:27.777209", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "9a976144-0374-4e15-9486-231c17252c78", "question": "What cybersecurity measures must national competent authorities take?", "answer": "They must take appropriate measures to ensure an adequate level of cybersecurity.", "context": "1. Each Member State shall establish or designate as national competent authorities at least one notifying authority and\nat least one market surveillance authority for the purposes of this Regulation. Those national competent authorities shall\nexercise their powers independently, impartially and without bias so as to safeguard the objectivity of their activities and\ntasks, and to ensure the application and implementation of this Regulation. The members of those authorities shall refrain\nfrom any action incompatible with their duties. Provided that those principles are observed, such activities and tasks may be\nperformed by one or more designated authorities, in accordance with the organisational needs of the Member State.\n2. Member States shall communicate to the Commission the identity of the notifying authorities and the market\nsurveillance authorities and the tasks of those authorities, as well as any subsequent changes thereto. Member States shall\nmake publicly available information on how competent authorities and single points of contact can be contacted, through\nelectronic communication means by 2 August 2025. Member States shall designate a market surveillance authority to ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0041", "generated_at": "2025-05-14T17:50:27.778845", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "3e78969a-4306-4599-aac5-165551fe5be8", "question": "What confidentiality obligations apply to national competent authorities when performing their tasks?", "answer": "They must act in accordance with the confidentiality obligations set out in Article 78.", "context": "1. Each Member State shall establish or designate as national competent authorities at least one notifying authority and\nat least one market surveillance authority for the purposes of this Regulation. Those national competent authorities shall\nexercise their powers independently, impartially and without bias so as to safeguard the objectivity of their activities and\ntasks, and to ensure the application and implementation of this Regulation. The members of those authorities shall refrain\nfrom any action incompatible with their duties. Provided that those principles are observed, such activities and tasks may be\nperformed by one or more designated authorities, in accordance with the organisational needs of the Member State.\n2. Member States shall communicate to the Commission the identity of the notifying authorities and the market\nsurveillance authorities and the tasks of those authorities, as well as any subsequent changes thereto. Member States shall\nmake publicly available information on how competent authorities and single points of contact can be contacted, through\nelectronic communication means by 2 August 2025. Member States shall designate a market surveillance authority to ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0041", "generated_at": "2025-05-14T17:50:27.780479", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "ec883bbc-63d7-4067-ac6e-5acc383663f0", "question": "Which high-risk AI systems are subject to notification limits under Article 9?", "answer": "High-risk AI systems referred to in Annex III that are placed on the market or put into service by providers subject to Union legislative instruments with equivalent reporting obligations.", "context": "7. Upon receiving a notification related to a serious incident referred to in Article 3, point (49)(c), the relevant market\nsurveillance authority shall inform the national public authorities or bodies referred to in Article 77(1). The Commission\nshall develop dedicated guidance to facilitate compliance with the obligations set out in paragraph 1 of this Article. That\nguidance shall be issued by 2 August 2025, and shall be assessed regularly.\n8. The market surveillance authority shall take appropriate measures, as provided for in Article 19 of Regulation (EU)\n2019/1020, within seven days from the date it received the notification referred to in paragraph 1 of this Article, and shall\nfollow the notification procedures as provided in that Regulation.\n9. For high-risk AI systems referred to in Annex III that are placed on the market or put into service by providers that are\nsubject to Union legislative instruments laying down reporting obligations equivalent to those set out in this Regulation, the\nnotification of serious incidents shall be limited to those referred to in Article 3, point (49)(c).\n10. For high-risk AI systems which are safety components of devices, or are themselves d...", "source": "EU AI Act", "category": "Compliance", "metadata": {"chunk_id": "chunk_0042", "generated_at": "2025-05-14T17:51:43.161967", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "b2293a8a-5525-4ebb-ba73-5cb0229a4894", "question": "For high-risk AI systems covered by Regulations (EU) 2017/745 and (EU) 2017/746, where should serious incident notifications be made?", "answer": "To the national competent authority chosen by the Member States where the incident occurred.", "context": "7. Upon receiving a notification related to a serious incident referred to in Article 3, point (49)(c), the relevant market\nsurveillance authority shall inform the national public authorities or bodies referred to in Article 77(1). The Commission\nshall develop dedicated guidance to facilitate compliance with the obligations set out in paragraph 1 of this Article. That\nguidance shall be issued by 2 August 2025, and shall be assessed regularly.\n8. The market surveillance authority shall take appropriate measures, as provided for in Article 19 of Regulation (EU)\n2019/1020, within seven days from the date it received the notification referred to in paragraph 1 of this Article, and shall\nfollow the notification procedures as provided in that Regulation.\n9. For high-risk AI systems referred to in Annex III that are placed on the market or put into service by providers that are\nsubject to Union legislative instruments laying down reporting obligations equivalent to those set out in this Regulation, the\nnotification of serious incidents shall be limited to those referred to in Article 3, point (49)(c).\n10. For high-risk AI systems which are safety components of devices, or are themselves d...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0042", "generated_at": "2025-05-14T17:51:43.163792", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "84a72169-ae70-423d-9f83-6cc77b09ad54", "question": "What is the role of the AI Office in monitoring compliance with obligations under this Regulation?", "answer": "The AI Office shall have powers to monitor and supervise compliance of AI systems based on general-purpose AI models, with all powers of a market surveillance authority.", "context": "7. Upon receiving a notification related to a serious incident referred to in Article 3, point (49)(c), the relevant market\nsurveillance authority shall inform the national public authorities or bodies referred to in Article 77(1). The Commission\nshall develop dedicated guidance to facilitate compliance with the obligations set out in paragraph 1 of this Article. That\nguidance shall be issued by 2 August 2025, and shall be assessed regularly.\n8. The market surveillance authority shall take appropriate measures, as provided for in Article 19 of Regulation (EU)\n2019/1020, within seven days from the date it received the notification referred to in paragraph 1 of this Article, and shall\nfollow the notification procedures as provided in that Regulation.\n9. For high-risk AI systems referred to in Annex III that are placed on the market or put into service by providers that are\nsubject to Union legislative instruments laying down reporting obligations equivalent to those set out in this Regulation, the\nnotification of serious incidents shall be limited to those referred to in Article 3, point (49)(c).\n10. For high-risk AI systems which are safety components of devices, or are themselves d...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0042", "generated_at": "2025-05-14T17:51:43.165459", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "362b4b24-1668-4122-9707-71581ffaa490", "question": "What powers do national authorities have regarding high-risk AI systems under Article 77?", "answer": "National public authorities or bodies supervising fundamental rights must request and access documentation under this Regulation to fulfill their mandates, with the market surveillance authority informed of such requests.", "context": "2. Where testing in real world conditions is conducted for AI systems that are supervised within an AI regulatory\nsandbox under Article 58, the market surveillance authorities shall verify the compliance with Article 60 as part of their\nsupervisory role for the AI regulatory sandbox. Those authorities may, as appropriate, allow the testing in real world\nconditions to be conducted by the provider or prospective provider, in derogation from the conditions set out in Article\n60(4), points (f) and (g).\n3. Where a market surveillance authority has been informed by the prospective provider, the provider or any third party\nof a serious incident or has other grounds for considering that the conditions set out in Articles 60 and 61 are not met, it\nmay take either of the following decisions on its territory, as appropriate:\n\n(a)to suspend or terminate the testing in real world conditions;\n\n(b)to require the provider or prospective provider and the deployer or prospective deployer to modify any aspect of the\ntesting in real world conditions.\n\n4. Where a market surveillance authority has taken a decision referred to in paragraph 3 of this Article, or has issued an\nobjection within the meaning ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0043", "generated_at": "2025-05-14T17:52:10.464848", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "b9cc4ad0-80d4-4d98-92b3-54d786c8414c", "question": "What confidentiality obligations apply to information obtained under this Regulation?", "answer": "Confidentiality must protect intellectual property, trade secrets, national security, and other sensitive data, with strict data handling and deletion requirements as per Article 78.", "context": "2. Where testing in real world conditions is conducted for AI systems that are supervised within an AI regulatory\nsandbox under Article 58, the market surveillance authorities shall verify the compliance with Article 60 as part of their\nsupervisory role for the AI regulatory sandbox. Those authorities may, as appropriate, allow the testing in real world\nconditions to be conducted by the provider or prospective provider, in derogation from the conditions set out in Article\n60(4), points (f) and (g).\n3. Where a market surveillance authority has been informed by the prospective provider, the provider or any third party\nof a serious incident or has other grounds for considering that the conditions set out in Articles 60 and 61 are not met, it\nmay take either of the following decisions on its territory, as appropriate:\n\n(a)to suspend or terminate the testing in real world conditions;\n\n(b)to require the provider or prospective provider and the deployer or prospective deployer to modify any aspect of the\ntesting in real world conditions.\n\n4. Where a market surveillance authority has taken a decision referred to in paragraph 3 of this Article, or has issued an\nobjection within the meaning ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0043", "generated_at": "2025-05-14T17:52:10.466849", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "abbe7930-9f70-4abc-873f-7a6fda3355da", "question": "What procedure applies to AI systems presenting a risk under Article 79?", "answer": "Market surveillance authorities must evaluate AI systems for compliance, require corrective actions, and notify the Commission and other Member States if non-compliance affects multiple territories.", "context": "2. Where testing in real world conditions is conducted for AI systems that are supervised within an AI regulatory\nsandbox under Article 58, the market surveillance authorities shall verify the compliance with Article 60 as part of their\nsupervisory role for the AI regulatory sandbox. Those authorities may, as appropriate, allow the testing in real world\nconditions to be conducted by the provider or prospective provider, in derogation from the conditions set out in Article\n60(4), points (f) and (g).\n3. Where a market surveillance authority has been informed by the prospective provider, the provider or any third party\nof a serious incident or has other grounds for considering that the conditions set out in Articles 60 and 61 are not met, it\nmay take either of the following decisions on its territory, as appropriate:\n\n(a)to suspend or terminate the testing in real world conditions;\n\n(b)to require the provider or prospective provider and the deployer or prospective deployer to modify any aspect of the\ntesting in real world conditions.\n\n4. Where a market surveillance authority has taken a decision referred to in paragraph 3 of this Article, or has issued an\nobjection within the meaning ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0043", "generated_at": "2025-05-14T17:52:10.468528", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "3d47573a-2663-4096-8b77-f114e22a85b0", "question": "Under what circumstances can market surveillance authorities suspend real-world testing of AI systems?", "answer": "If a serious incident occurs or conditions in Articles 60/61 are not met, authorities may suspend or terminate testing, requiring modifications to the testing process.", "context": "2. Where testing in real world conditions is conducted for AI systems that are supervised within an AI regulatory\nsandbox under Article 58, the market surveillance authorities shall verify the compliance with Article 60 as part of their\nsupervisory role for the AI regulatory sandbox. Those authorities may, as appropriate, allow the testing in real world\nconditions to be conducted by the provider or prospective provider, in derogation from the conditions set out in Article\n60(4), points (f) and (g).\n3. Where a market surveillance authority has been informed by the prospective provider, the provider or any third party\nof a serious incident or has other grounds for considering that the conditions set out in Articles 60 and 61 are not met, it\nmay take either of the following decisions on its territory, as appropriate:\n\n(a)to suspend or terminate the testing in real world conditions;\n\n(b)to require the provider or prospective provider and the deployer or prospective deployer to modify any aspect of the\ntesting in real world conditions.\n\n4. Where a market surveillance authority has taken a decision referred to in paragraph 3 of this Article, or has issued an\nobjection within the meaning ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0043", "generated_at": "2025-05-14T17:52:10.470176", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "9d0246e2-3b3b-446a-8011-834d48f76438", "question": "By what date must Member States identify public authorities for fundamental rights protection?", "answer": "By 2 November 2024, each Member State must identify and publicly list relevant authorities, notifying the Commission and other Member States.", "context": "2. Where testing in real world conditions is conducted for AI systems that are supervised within an AI regulatory\nsandbox under Article 58, the market surveillance authorities shall verify the compliance with Article 60 as part of their\nsupervisory role for the AI regulatory sandbox. Those authorities may, as appropriate, allow the testing in real world\nconditions to be conducted by the provider or prospective provider, in derogation from the conditions set out in Article\n60(4), points (f) and (g).\n3. Where a market surveillance authority has been informed by the prospective provider, the provider or any third party\nof a serious incident or has other grounds for considering that the conditions set out in Articles 60 and 61 are not met, it\nmay take either of the following decisions on its territory, as appropriate:\n\n(a)to suspend or terminate the testing in real world conditions;\n\n(b)to require the provider or prospective provider and the deployer or prospective deployer to modify any aspect of the\ntesting in real world conditions.\n\n4. Where a market surveillance authority has taken a decision referred to in paragraph 3 of this Article, or has issued an\nobjection within the meaning ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0043", "generated_at": "2025-05-14T17:52:10.471824", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "3681197f-8723-4817-bfb7-70893c4ca5d3", "question": "What procedure applies when a market surveillance authority determines a non-high-risk AI system is actually high-risk?", "answer": "The market surveillance authority shall carry out an evaluation of the AI system concerned in respect of its classification as a high-risk AI system based on the conditions set out in Article 6(3) and the Commission guidelines.", "context": "7. The market surveillance authorities other than the market surveillance authority of the Member State initiating the\nprocedure shall, without undue delay, inform the Commission and the other Member States of any measures adopted and of\nany additional information at their disposal relating to the non-compliance of the AI system concerned, and, in the event of\ndisagreement with the notified national measure, of their objections.\n8. Where, within three months of receipt of the notification referred to in paragraph 5 of this Article, no objection has\nbeen raised by either a market surveillance authority of a Member State or by the Commission in respect of a provisional\nmeasure taken by a market surveillance authority of another Member State, that measure shall be deemed justified. This\nshall be without prejudice to the procedural rights of the concerned operator in accordance with Article 18 of Regulation\n(EU) 2019/1020. The three-month period referred to in this paragraph shall be reduced to 30 days in the event of\nnon-compliance with the prohibition of the AI practices referred to in Article 5 of this Regulation.\n9. The market surveillance authorities shall ensure that appropriate ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0044", "generated_at": "2025-05-14T17:54:02.552420", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "dd180c07-ce0d-439d-8c6c-cda49c93673c", "question": "How long does the Commission have to evaluate a national measure under Article 81?", "answer": "The Commission shall, within six months, or within 60 days in the case of non-compliance with the prohibition of the AI practices referred to in Article 5, starting from the notification referred to in Article 79(5), decide whether the national measure is justified.", "context": "7. The market surveillance authorities other than the market surveillance authority of the Member State initiating the\nprocedure shall, without undue delay, inform the Commission and the other Member States of any measures adopted and of\nany additional information at their disposal relating to the non-compliance of the AI system concerned, and, in the event of\ndisagreement with the notified national measure, of their objections.\n8. Where, within three months of receipt of the notification referred to in paragraph 5 of this Article, no objection has\nbeen raised by either a market surveillance authority of a Member State or by the Commission in respect of a provisional\nmeasure taken by a market surveillance authority of another Member State, that measure shall be deemed justified. This\nshall be without prejudice to the procedural rights of the concerned operator in accordance with Article 18 of Regulation\n(EU) 2019/1020. The three-month period referred to in this paragraph shall be reduced to 30 days in the event of\nnon-compliance with the prohibition of the AI practices referred to in Article 5 of this Regulation.\n9. The market surveillance authorities shall ensure that appropriate ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0044", "generated_at": "2025-05-14T17:54:02.554304", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "3f759c6e-7af1-44b9-8e42-1dea818e293b", "question": "What actions must a provider take if their AI system is found non-compliant under Article 80?", "answer": "The provider shall ensure that all necessary action is taken to bring the AI system into compliance with the requirements and obligations laid down in this Regulation, and shall be subject to fines in accordance with Article 99 if non-compliance persists.", "context": "7. The market surveillance authorities other than the market surveillance authority of the Member State initiating the\nprocedure shall, without undue delay, inform the Commission and the other Member States of any measures adopted and of\nany additional information at their disposal relating to the non-compliance of the AI system concerned, and, in the event of\ndisagreement with the notified national measure, of their objections.\n8. Where, within three months of receipt of the notification referred to in paragraph 5 of this Article, no objection has\nbeen raised by either a market surveillance authority of a Member State or by the Commission in respect of a provisional\nmeasure taken by a market surveillance authority of another Member State, that measure shall be deemed justified. This\nshall be without prejudice to the procedural rights of the concerned operator in accordance with Article 18 of Regulation\n(EU) 2019/1020. The three-month period referred to in this paragraph shall be reduced to 30 days in the event of\nnon-compliance with the prohibition of the AI practices referred to in Article 5 of this Regulation.\n9. The market surveillance authorities shall ensure that appropriate ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0044", "generated_at": "2025-05-14T17:54:02.556134", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "0e6931d8-e7fd-46c8-8e79-ef444bd530a3", "question": "What are examples of formal non-compliance issues under Article 83?", "answer": "Examples include the CE marking being affixed in violation of Article 48, not being affixed, incorrect EU declaration of conformity, or lack of registration in the EU database referred to in Article 71.", "context": "7. The market surveillance authorities other than the market surveillance authority of the Member State initiating the\nprocedure shall, without undue delay, inform the Commission and the other Member States of any measures adopted and of\nany additional information at their disposal relating to the non-compliance of the AI system concerned, and, in the event of\ndisagreement with the notified national measure, of their objections.\n8. Where, within three months of receipt of the notification referred to in paragraph 5 of this Article, no objection has\nbeen raised by either a market surveillance authority of a Member State or by the Commission in respect of a provisional\nmeasure taken by a market surveillance authority of another Member State, that measure shall be deemed justified. This\nshall be without prejudice to the procedural rights of the concerned operator in accordance with Article 18 of Regulation\n(EU) 2019/1020. The three-month period referred to in this paragraph shall be reduced to 30 days in the event of\nnon-compliance with the prohibition of the AI practices referred to in Article 5 of this Regulation.\n9. The market surveillance authorities shall ensure that appropriate ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0044", "generated_at": "2025-05-14T17:54:02.557977", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "b062fdfc-866d-402b-b710-feb5c37d2ee9", "question": "What right does an affected person have regarding decisions made by a high-risk AI system?", "answer": "Any affected person subject to a decision based on a high-risk AI system has the right to obtain clear and meaningful explanations of the role of the AI system in the decision-making procedure and the main elements of the decision taken.", "context": "7. The market surveillance authorities other than the market surveillance authority of the Member State initiating the\nprocedure shall, without undue delay, inform the Commission and the other Member States of any measures adopted and of\nany additional information at their disposal relating to the non-compliance of the AI system concerned, and, in the event of\ndisagreement with the notified national measure, of their objections.\n8. Where, within three months of receipt of the notification referred to in paragraph 5 of this Article, no objection has\nbeen raised by either a market surveillance authority of a Member State or by the Commission in respect of a provisional\nmeasure taken by a market surveillance authority of another Member State, that measure shall be deemed justified. This\nshall be without prejudice to the procedural rights of the concerned operator in accordance with Article 18 of Regulation\n(EU) 2019/1020. The three-month period referred to in this paragraph shall be reduced to 30 days in the event of\nnon-compliance with the prohibition of the AI practices referred to in Article 5 of this Regulation.\n9. The market surveillance authorities shall ensure that appropriate ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0044", "generated_at": "2025-05-14T17:54:02.559721", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "b5455981-4fc3-4f50-b0c6-462b39069c39", "question": "What powers does the AI Office have regarding monitoring compliance with the Regulation?", "answer": "The AI Office may take necessary actions to monitor the effective implementation and compliance with this Regulation by providers of general-purpose AI models, including their adherence to approved codes of practice (Article 89(1)).", "context": "1. The Commission shall have exclusive powers to supervise and enforce Chapter V, taking into account the procedural\nguarantees under Article 94. The Commission shall entrust the implementation of these tasks to the AI Office, without\nprejudice to the powers of organisation of the Commission and the division of competences between Member States and\nthe Union based on the Treaties.\n2. Without prejudice to Article 75(3), market surveillance authorities may request the Commission to exercise the\npowers laid down in this Section, where that is necessary and proportionate to assist with the fulfilment of their tasks under\nthis Regulation.\n\n# EN OJ L, 12.7.2024\n\n110/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\nArticle 89\n\nMonitoring actions\n\n1. For the purpose of carrying out the tasks assigned to it under this Section, the AI Office may take the necessary\nactions to monitor the effective implementation and compliance with this Regulation by providers of general-purpose AI\nmodels, including their adherence to approved codes of practice.\n2. Downstream providers shall have the right to lodge a complaint alleging an infringement of this ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0045", "generated_at": "2025-05-14T17:53:35.375811", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "24c845d8-5704-4067-8cff-5a30818a64a1", "question": "Under what conditions can the scientific panel issue a qualified alert?", "answer": "The scientific panel may issue a qualified alert if it suspects a general-purpose AI model poses concrete identifiable risk at Union level or meets the conditions in Article 51 (Article 90(1)).", "context": "1. The Commission shall have exclusive powers to supervise and enforce Chapter V, taking into account the procedural\nguarantees under Article 94. The Commission shall entrust the implementation of these tasks to the AI Office, without\nprejudice to the powers of organisation of the Commission and the division of competences between Member States and\nthe Union based on the Treaties.\n2. Without prejudice to Article 75(3), market surveillance authorities may request the Commission to exercise the\npowers laid down in this Section, where that is necessary and proportionate to assist with the fulfilment of their tasks under\nthis Regulation.\n\n# EN OJ L, 12.7.2024\n\n110/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\nArticle 89\n\nMonitoring actions\n\n1. For the purpose of carrying out the tasks assigned to it under this Section, the AI Office may take the necessary\nactions to monitor the effective implementation and compliance with this Regulation by providers of general-purpose AI\nmodels, including their adherence to approved codes of practice.\n2. Downstream providers shall have the right to lodge a complaint alleging an infringement of this ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0045", "generated_at": "2025-05-14T17:53:35.377652", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "3b264a89-f2f0-4b05-9a1c-19450c562076", "question": "What information must a downstream provider include in a complaint alleging an infringement?", "answer": "A complaint must include the provider's point of contact, a description of relevant facts and applicable Regulation provisions, and any other relevant information, including self-initiated findings (Article 89(2)).", "context": "1. The Commission shall have exclusive powers to supervise and enforce Chapter V, taking into account the procedural\nguarantees under Article 94. The Commission shall entrust the implementation of these tasks to the AI Office, without\nprejudice to the powers of organisation of the Commission and the division of competences between Member States and\nthe Union based on the Treaties.\n2. Without prejudice to Article 75(3), market surveillance authorities may request the Commission to exercise the\npowers laid down in this Section, where that is necessary and proportionate to assist with the fulfilment of their tasks under\nthis Regulation.\n\n# EN OJ L, 12.7.2024\n\n110/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\nArticle 89\n\nMonitoring actions\n\n1. For the purpose of carrying out the tasks assigned to it under this Section, the AI Office may take the necessary\nactions to monitor the effective implementation and compliance with this Regulation by providers of general-purpose AI\nmodels, including their adherence to approved codes of practice.\n2. Downstream providers shall have the right to lodge a complaint alleging an infringement of this ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0045", "generated_at": "2025-05-14T17:53:35.379318", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "f1a7df4a-f765-48dc-9e08-bccc89a0730c", "question": "What details must a request for information from the Commission contain?", "answer": "The request must state the legal basis, purpose, required information, deadline, and mention Article 101 fines for incorrect/incomplete information (Article 91(4)).", "context": "1. The Commission shall have exclusive powers to supervise and enforce Chapter V, taking into account the procedural\nguarantees under Article 94. The Commission shall entrust the implementation of these tasks to the AI Office, without\nprejudice to the powers of organisation of the Commission and the division of competences between Member States and\nthe Union based on the Treaties.\n2. Without prejudice to Article 75(3), market surveillance authorities may request the Commission to exercise the\npowers laid down in this Section, where that is necessary and proportionate to assist with the fulfilment of their tasks under\nthis Regulation.\n\n# EN OJ L, 12.7.2024\n\n110/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\nArticle 89\n\nMonitoring actions\n\n1. For the purpose of carrying out the tasks assigned to it under this Section, the AI Office may take the necessary\nactions to monitor the effective implementation and compliance with this Regulation by providers of general-purpose AI\nmodels, including their adherence to approved codes of practice.\n2. Downstream providers shall have the right to lodge a complaint alleging an infringement of this ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0045", "generated_at": "2025-05-14T17:53:35.380976", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "e298a2fd-4c44-4876-a85a-429270a80961", "question": "What procedural rights apply to providers of general-purpose AI models?", "answer": "Providers are subject to Article 18 of Regulation (EU) 2019/1020 mutatis mutandis, without prejudice to more specific rights in this Regulation (Article 94).", "context": "1. The Commission shall have exclusive powers to supervise and enforce Chapter V, taking into account the procedural\nguarantees under Article 94. The Commission shall entrust the implementation of these tasks to the AI Office, without\nprejudice to the powers of organisation of the Commission and the division of competences between Member States and\nthe Union based on the Treaties.\n2. Without prejudice to Article 75(3), market surveillance authorities may request the Commission to exercise the\npowers laid down in this Section, where that is necessary and proportionate to assist with the fulfilment of their tasks under\nthis Regulation.\n\n# EN OJ L, 12.7.2024\n\n110/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\nArticle 89\n\nMonitoring actions\n\n1. For the purpose of carrying out the tasks assigned to it under this Section, the AI Office may take the necessary\nactions to monitor the effective implementation and compliance with this Regulation by providers of general-purpose AI\nmodels, including their adherence to approved codes of practice.\n2. Downstream providers shall have the right to lodge a complaint alleging an infringement of this ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0045", "generated_at": "2025-05-14T17:53:35.382642", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "a711b2ea-549d-4ca8-b022-925343358541", "question": "What factors should the guidelines take into account?", "answer": "The guidelines shall take due account of the generally acknowledged state of the art on AI, as well as relevant harmonised standards and common specifications referred to in Articles 40 and 41, or those set out pursuant to Union harmonisation law.", "context": "The guidelines referred to in the first subparagraph of this paragraph shall take due account of the generally acknowledged\nstate of the art on AI, as well as of relevant harmonised standards and common specifications that are referred to in\nArticles 40 and 41, or of those harmonised standards or technical specifications that are set out pursuant to Union\nharmonisation law.\n\n2. At the request of the Member States or the AI Office, or on its own initiative, the Commission shall update guidelines\npreviously adopted when deemed necessary.\n\nCHAPTER XI\nDELEGATION OF POWER AND COMMITTEE PROCEDURE\n\nArticle 97\n\nExercise of the delegation\n\n1. The power to adopt delegated acts is conferred on the Commission subject to the conditions laid down in this Article.\n2. The power to adopt delegated acts referred to in Article 6(6) and (7), Article 7(1) and (3), Article 11(3), Article 43(5)\nand (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6) shall be conferred on the Commission for\na period of five years from 1 August 2024. The Commission shall draw up a report in respect of the delegation of power\nnot later than nine months before the end of the five-year period. The deleg...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0046", "generated_at": "2025-05-14T17:55:50.866016", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "fe0699d8-4907-4442-aad1-6bb0e9828abf", "question": "Under what circumstances may the Commission update previously adopted guidelines?", "answer": "The Commission shall update guidelines previously adopted when requested by Member States or the AI Office, or on its own initiative when deemed necessary.", "context": "The guidelines referred to in the first subparagraph of this paragraph shall take due account of the generally acknowledged\nstate of the art on AI, as well as of relevant harmonised standards and common specifications that are referred to in\nArticles 40 and 41, or of those harmonised standards or technical specifications that are set out pursuant to Union\nharmonisation law.\n\n2. At the request of the Member States or the AI Office, or on its own initiative, the Commission shall update guidelines\npreviously adopted when deemed necessary.\n\nCHAPTER XI\nDELEGATION OF POWER AND COMMITTEE PROCEDURE\n\nArticle 97\n\nExercise of the delegation\n\n1. The power to adopt delegated acts is conferred on the Commission subject to the conditions laid down in this Article.\n2. The power to adopt delegated acts referred to in Article 6(6) and (7), Article 7(1) and (3), Article 11(3), Article 43(5)\nand (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6) shall be conferred on the Commission for\na period of five years from 1 August 2024. The Commission shall draw up a report in respect of the delegation of power\nnot later than nine months before the end of the five-year period. The deleg...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0046", "generated_at": "2025-05-14T17:55:50.867836", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "670ac3c7-3f70-43e9-928c-23d44671bf71", "question": "For how long is the delegation of power to the Commission valid?", "answer": "The delegation of power is valid for a period of five years from 1 August 2024, with tacit extension unless opposed by the European Parliament or Council.", "context": "The guidelines referred to in the first subparagraph of this paragraph shall take due account of the generally acknowledged\nstate of the art on AI, as well as of relevant harmonised standards and common specifications that are referred to in\nArticles 40 and 41, or of those harmonised standards or technical specifications that are set out pursuant to Union\nharmonisation law.\n\n2. At the request of the Member States or the AI Office, or on its own initiative, the Commission shall update guidelines\npreviously adopted when deemed necessary.\n\nCHAPTER XI\nDELEGATION OF POWER AND COMMITTEE PROCEDURE\n\nArticle 97\n\nExercise of the delegation\n\n1. The power to adopt delegated acts is conferred on the Commission subject to the conditions laid down in this Article.\n2. The power to adopt delegated acts referred to in Article 6(6) and (7), Article 7(1) and (3), Article 11(3), Article 43(5)\nand (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6) shall be conferred on the Commission for\na period of five years from 1 August 2024. The Commission shall draw up a report in respect of the delegation of power\nnot later than nine months before the end of the five-year period. The deleg...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0046", "generated_at": "2025-05-14T17:55:50.869497", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "327fe1a2-1cef-431e-92a4-ee47b3e99a50", "question": "Which bodies have the authority to revoke the delegation of power?", "answer": "The European Parliament or the Council may revoke the delegation of power at any time.", "context": "The guidelines referred to in the first subparagraph of this paragraph shall take due account of the generally acknowledged\nstate of the art on AI, as well as of relevant harmonised standards and common specifications that are referred to in\nArticles 40 and 41, or of those harmonised standards or technical specifications that are set out pursuant to Union\nharmonisation law.\n\n2. At the request of the Member States or the AI Office, or on its own initiative, the Commission shall update guidelines\npreviously adopted when deemed necessary.\n\nCHAPTER XI\nDELEGATION OF POWER AND COMMITTEE PROCEDURE\n\nArticle 97\n\nExercise of the delegation\n\n1. The power to adopt delegated acts is conferred on the Commission subject to the conditions laid down in this Article.\n2. The power to adopt delegated acts referred to in Article 6(6) and (7), Article 7(1) and (3), Article 11(3), Article 43(5)\nand (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6) shall be conferred on the Commission for\na period of five years from 1 August 2024. The Commission shall draw up a report in respect of the delegation of power\nnot later than nine months before the end of the five-year period. The deleg...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0046", "generated_at": "2025-05-14T17:55:50.871137", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "e748a9fb-5d9f-4dbc-928b-61acb11a038a", "question": "What penalties apply to SMEs for non-compliance with AI regulations?", "answer": "For SMEs, including start-ups, administrative fines shall be up to the percentages or amount referred to in paragraphs 3, 4 and 5, whichever is lower.", "context": "The guidelines referred to in the first subparagraph of this paragraph shall take due account of the generally acknowledged\nstate of the art on AI, as well as of relevant harmonised standards and common specifications that are referred to in\nArticles 40 and 41, or of those harmonised standards or technical specifications that are set out pursuant to Union\nharmonisation law.\n\n2. At the request of the Member States or the AI Office, or on its own initiative, the Commission shall update guidelines\npreviously adopted when deemed necessary.\n\nCHAPTER XI\nDELEGATION OF POWER AND COMMITTEE PROCEDURE\n\nArticle 97\n\nExercise of the delegation\n\n1. The power to adopt delegated acts is conferred on the Commission subject to the conditions laid down in this Article.\n2. The power to adopt delegated acts referred to in Article 6(6) and (7), Article 7(1) and (3), Article 11(3), Article 43(5)\nand (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6) shall be conferred on the Commission for\na period of five years from 1 August 2024. The Commission shall draw up a report in respect of the delegation of power\nnot later than nine months before the end of the five-year period. The deleg...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0046", "generated_at": "2025-05-14T17:55:50.872767", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "a50eefe8-695a-442b-a4b0-b90286ee017c", "question": "What is the maximum fine for providers of general-purpose AI models under Article 101?", "answer": "Fines not exceeding 3 % of their annual total worldwide turnover in the preceding financial year or EUR 15 000 000, whichever is higher.", "context": "2. Non-compliance with the prohibition of the AI practices referred to in Article 5 shall be subject to administrative\nfines of up to EUR 1 500 000.\n3. The non-compliance of the AI system with any requirements or obligations under this Regulation, other than those\nlaid down in Article 5, shall be subject to administrative fines of up to EUR 750 000.\n4. Before taking decisions pursuant to this Article, the European Data Protection Supervisor shall give the Union\ninstitution, body, office or agency which is the subject of the proceedings conducted by the European Data Protection\nSupervisor the opportunity of being heard on the matter regarding the possible infringement. The European Data\nProtection Supervisor shall base his or her decisions only on elements and circumstances on which the parties concerned\nhave been able to comment. Complainants, if any, shall be associated closely with the proceedings.\n5. The rights of defence of the parties concerned shall be fully respected in the proceedings. They shall be entitled to\nhave access to the European Data Protection Supervisor s file, subject to the legitimate interest of individuals or\nundertakings in the protection of their personal ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0047", "generated_at": "2025-05-14T17:55:26.460650", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "82bcb25f-6306-4b20-8c16-995f91241001", "question": "What rights does the provider of a general-purpose AI model have before a fine decision?", "answer": "The provider must be given an opportunity to be heard and receive preliminary findings from the Commission.", "context": "2. Non-compliance with the prohibition of the AI practices referred to in Article 5 shall be subject to administrative\nfines of up to EUR 1 500 000.\n3. The non-compliance of the AI system with any requirements or obligations under this Regulation, other than those\nlaid down in Article 5, shall be subject to administrative fines of up to EUR 750 000.\n4. Before taking decisions pursuant to this Article, the European Data Protection Supervisor shall give the Union\ninstitution, body, office or agency which is the subject of the proceedings conducted by the European Data Protection\nSupervisor the opportunity of being heard on the matter regarding the possible infringement. The European Data\nProtection Supervisor shall base his or her decisions only on elements and circumstances on which the parties concerned\nhave been able to comment. Complainants, if any, shall be associated closely with the proceedings.\n5. The rights of defence of the parties concerned shall be fully respected in the proceedings. They shall be entitled to\nhave access to the European Data Protection Supervisor s file, subject to the legitimate interest of individuals or\nundertakings in the protection of their personal ...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0047", "generated_at": "2025-05-14T17:55:26.462513", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "f8193298-02ba-4d18-8548-5ea881c2c9d2", "question": "What requirements must be considered when adopting delegated acts concerning AI safety components under Regulation (EU) 2024/1689?", "answer": "The requirements set out in Chapter III, Section 2, of Regulation (EU) 2024/1689 shall be taken into account.", "context": "When adopting those implementing acts concerning Artificial Intelligence systems which are safety components within\nthe meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III, Section 2, of that Regulation shall\nbe taken into account. ;\n\n# EN OJ L, 12.7.2024\n\n120/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\n(6)in Article 58, the following paragraph is added:\n\n 3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which\nare safety components within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III,\nSection 2, of that Regulation shall be taken into account. .\n\nArticle 109\n\nAmendment to Regulation (EU) 2019/2144\n\nIn Article 11 of Regulation (EU) 2019/2144, the following paragraph is added:\n\n 3. When adopting the implementing acts pursuant to paragraph 2, concerning artificial intelligence systems which are\nsafety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*),\nthe requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.\n\n(*) Re...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0048", "generated_at": "2025-05-14T17:57:57.917609", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "5ea047f3-8e93-481c-a00f-aaee172fe7b5", "question": "When must AI systems already on the market comply with Regulation (EU) 2024/1689?", "answer": "AI systems which are components of large-scale IT systems established by legal acts listed in Annex X, placed on the market before 2 August 2027, must comply by 31 December 2030.", "context": "When adopting those implementing acts concerning Artificial Intelligence systems which are safety components within\nthe meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III, Section 2, of that Regulation shall\nbe taken into account. ;\n\n# EN OJ L, 12.7.2024\n\n120/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\n(6)in Article 58, the following paragraph is added:\n\n 3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which\nare safety components within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III,\nSection 2, of that Regulation shall be taken into account. .\n\nArticle 109\n\nAmendment to Regulation (EU) 2019/2144\n\nIn Article 11 of Regulation (EU) 2019/2144, the following paragraph is added:\n\n 3. When adopting the implementing acts pursuant to paragraph 2, concerning artificial intelligence systems which are\nsafety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*),\nthe requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.\n\n(*) Re...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0048", "generated_at": "2025-05-14T17:57:57.919414", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "81e28c81-f471-4943-9b6f-c815b4bbae0f", "question": "Which directive was amended to include a reference to Regulation (EU) 2024/1689?", "answer": "Directive (EU) 2020/1828 was amended to include a reference to Regulation (EU) 2024/1689 in Annex I.", "context": "When adopting those implementing acts concerning Artificial Intelligence systems which are safety components within\nthe meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III, Section 2, of that Regulation shall\nbe taken into account. ;\n\n# EN OJ L, 12.7.2024\n\n120/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\n(6)in Article 58, the following paragraph is added:\n\n 3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which\nare safety components within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III,\nSection 2, of that Regulation shall be taken into account. .\n\nArticle 109\n\nAmendment to Regulation (EU) 2019/2144\n\nIn Article 11 of Regulation (EU) 2019/2144, the following paragraph is added:\n\n 3. When adopting the implementing acts pursuant to paragraph 2, concerning artificial intelligence systems which are\nsafety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*),\nthe requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.\n\n(*) Re...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0048", "generated_at": "2025-05-14T17:57:57.921070", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "ee169899-0f63-4d9e-a202-5b271ee5e8fe", "question": "What is the purpose of the evaluation and review process outlined in Article 112?", "answer": "The Commission must assess the need for amendments to Annex III, prohibited AI practices, and evaluate the effectiveness of supervision and governance systems.", "context": "When adopting those implementing acts concerning Artificial Intelligence systems which are safety components within\nthe meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III, Section 2, of that Regulation shall\nbe taken into account. ;\n\n# EN OJ L, 12.7.2024\n\n120/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\n(6)in Article 58, the following paragraph is added:\n\n 3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which\nare safety components within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III,\nSection 2, of that Regulation shall be taken into account. .\n\nArticle 109\n\nAmendment to Regulation (EU) 2019/2144\n\nIn Article 11 of Regulation (EU) 2019/2144, the following paragraph is added:\n\n 3. When adopting the implementing acts pursuant to paragraph 2, concerning artificial intelligence systems which are\nsafety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*),\nthe requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.\n\n(*) Re...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0048", "generated_at": "2025-05-14T17:57:57.922853", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "d53448f9-701a-42db-be5b-f3de65c55ce5", "question": "When does Regulation (EU) 2024/1689 enter into force and apply?", "answer": "This Regulation shall enter into force on the twentieth day following its publication and apply from 2 August 2026, with specific provisions applying from 2 February 2025 and 2 August 2025.", "context": "When adopting those implementing acts concerning Artificial Intelligence systems which are safety components within\nthe meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III, Section 2, of that Regulation shall\nbe taken into account. ;\n\n# EN OJ L, 12.7.2024\n\n120/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\n(6)in Article 58, the following paragraph is added:\n\n 3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which\nare safety components within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III,\nSection 2, of that Regulation shall be taken into account. .\n\nArticle 109\n\nAmendment to Regulation (EU) 2019/2144\n\nIn Article 11 of Regulation (EU) 2019/2144, the following paragraph is added:\n\n 3. When adopting the implementing acts pursuant to paragraph 2, concerning artificial intelligence systems which are\nsafety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*),\nthe requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.\n\n(*) Re...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0048", "generated_at": "2025-05-14T17:57:57.924544", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "b1c0ac94-0859-45d0-831d-448d7018d401", "question": "Which EU directive addresses the safety of toys?", "answer": "Directive 2009/48/EC of the European Parliament and of the Council of 18 June 2009 on the safety of toys (OJ L 170, 30.6.2009, p. 1).", "context": "1. Directive 2006/42/EC of the European Parliament and of the Council of 17 May 2006 on machinery, and amending\n    Directive 95/16/EC (OJ L 157, 9.6.2006, p. 24);\n2. Directive 2009/48/EC of the European Parliament and of the Council of 18 June 2009 on the safety of toys (OJ\n    L 170, 30.6.2009, p. 1);\n3. Directive 2013/53/EU of the European Parliament and of the Council of 20 November 2013 on recreational craft\n    and personal watercraft and repealing Directive 94/25/EC (OJ L 354, 28.12.2013, p. 90);\n4. Directive 2014/33/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of\n    the laws of the Member States relating to lifts and safety components for lifts (OJ L 96, 29.3.2014, p. 251);\n5. Directive 2014/34/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of\n    the laws of the Member States relating to equipment and protective systems intended for use in potentially explosive\n    atmospheres (OJ L 96, 29.3.2014, p. 309);\n6. Directive 2014/53/EU of the European Parliament and of the Council of 16 April 2014 on the harmonisation of the\n    laws of the Member States relating to the making available on t...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0049", "generated_at": "2025-05-14T17:57:36.046654", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "fb152eb2-194a-4108-a150-bff314e94c13", "question": "What is listed in Annex II of the document?", "answer": "Criminal offences referred to in Article 5(1), first subparagraph, point (h)(iii), including terrorism, trafficking in human beings, sexual exploitation of children, and child pornography, among others.", "context": "1. Directive 2006/42/EC of the European Parliament and of the Council of 17 May 2006 on machinery, and amending\n    Directive 95/16/EC (OJ L 157, 9.6.2006, p. 24);\n2. Directive 2009/48/EC of the European Parliament and of the Council of 18 June 2009 on the safety of toys (OJ\n    L 170, 30.6.2009, p. 1);\n3. Directive 2013/53/EU of the European Parliament and of the Council of 20 November 2013 on recreational craft\n    and personal watercraft and repealing Directive 94/25/EC (OJ L 354, 28.12.2013, p. 90);\n4. Directive 2014/33/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of\n    the laws of the Member States relating to lifts and safety components for lifts (OJ L 96, 29.3.2014, p. 251);\n5. Directive 2014/34/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of\n    the laws of the Member States relating to equipment and protective systems intended for use in potentially explosive\n    atmospheres (OJ L 96, 29.3.2014, p. 309);\n6. Directive 2014/53/EU of the European Parliament and of the Council of 16 April 2014 on the harmonisation of the\n    laws of the Member States relating to the making available on t...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0049", "generated_at": "2025-05-14T17:57:36.048500", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "45306996-b81e-4057-97f3-7b5d1be574d6", "question": "Which AI systems are classified as high-risk under Article 6(2)?", "answer": "High-risk AI systems include those in biometrics (e.g., remote biometric identification), critical infrastructure, education, employment, access to services, law enforcement, and other specified areas.", "context": "1. Directive 2006/42/EC of the European Parliament and of the Council of 17 May 2006 on machinery, and amending\n    Directive 95/16/EC (OJ L 157, 9.6.2006, p. 24);\n2. Directive 2009/48/EC of the European Parliament and of the Council of 18 June 2009 on the safety of toys (OJ\n    L 170, 30.6.2009, p. 1);\n3. Directive 2013/53/EU of the European Parliament and of the Council of 20 November 2013 on recreational craft\n    and personal watercraft and repealing Directive 94/25/EC (OJ L 354, 28.12.2013, p. 90);\n4. Directive 2014/33/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of\n    the laws of the Member States relating to lifts and safety components for lifts (OJ L 96, 29.3.2014, p. 251);\n5. Directive 2014/34/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of\n    the laws of the Member States relating to equipment and protective systems intended for use in potentially explosive\n    atmospheres (OJ L 96, 29.3.2014, p. 309);\n6. Directive 2014/53/EU of the European Parliament and of the Council of 16 April 2014 on the harmonisation of the\n    laws of the Member States relating to the making available on t...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0049", "generated_at": "2025-05-14T17:57:36.050166", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "a915e6f4-d97a-4c4c-b8df-2b6dde994752", "question": "Which regulation governs the approval and market surveillance of motor vehicles?", "answer": "Regulation (EU) 2018/858 of the European Parliament and of the Council of 30 May 2018 on the approval and market surveillance of motor vehicles and their trailers.", "context": "1. Directive 2006/42/EC of the European Parliament and of the Council of 17 May 2006 on machinery, and amending\n    Directive 95/16/EC (OJ L 157, 9.6.2006, p. 24);\n2. Directive 2009/48/EC of the European Parliament and of the Council of 18 June 2009 on the safety of toys (OJ\n    L 170, 30.6.2009, p. 1);\n3. Directive 2013/53/EU of the European Parliament and of the Council of 20 November 2013 on recreational craft\n    and personal watercraft and repealing Directive 94/25/EC (OJ L 354, 28.12.2013, p. 90);\n4. Directive 2014/33/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of\n    the laws of the Member States relating to lifts and safety components for lifts (OJ L 96, 29.3.2014, p. 251);\n5. Directive 2014/34/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of\n    the laws of the Member States relating to equipment and protective systems intended for use in potentially explosive\n    atmospheres (OJ L 96, 29.3.2014, p. 309);\n6. Directive 2014/53/EU of the European Parliament and of the Council of 16 April 2014 on the harmonisation of the\n    laws of the Member States relating to the making available on t...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0049", "generated_at": "2025-05-14T17:57:36.051801", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "b4dc2c5c-86bb-48e6-8f04-2d07f77899c3", "question": "What is the purpose of Regulation (EU) 2019/2144?", "answer": "To establish type-approval requirements for motor vehicles and their trailers, focusing on general safety and protection of vehicle occupants and vulnerable road users.", "context": "1. Directive 2006/42/EC of the European Parliament and of the Council of 17 May 2006 on machinery, and amending\n    Directive 95/16/EC (OJ L 157, 9.6.2006, p. 24);\n2. Directive 2009/48/EC of the European Parliament and of the Council of 18 June 2009 on the safety of toys (OJ\n    L 170, 30.6.2009, p. 1);\n3. Directive 2013/53/EU of the European Parliament and of the Council of 20 November 2013 on recreational craft\n    and personal watercraft and repealing Directive 94/25/EC (OJ L 354, 28.12.2013, p. 90);\n4. Directive 2014/33/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of\n    the laws of the Member States relating to lifts and safety components for lifts (OJ L 96, 29.3.2014, p. 251);\n5. Directive 2014/34/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of\n    the laws of the Member States relating to equipment and protective systems intended for use in potentially explosive\n    atmospheres (OJ L 96, 29.3.2014, p. 309);\n6. Directive 2014/53/EU of the European Parliament and of the Council of 16 April 2014 on the harmonisation of the\n    laws of the Member States relating to the making available on t...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0049", "generated_at": "2025-05-14T17:57:36.053447", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "1604292d-4008-450e-8dde-efbe66c038cd", "question": "Which AI systems are permitted for use in migration, asylum, and border control management?", "answer": "AI systems intended to be used by competent public authorities or Union institutions for polygraphs, risk assessment, asylum application examination, and detecting/recognizing natural persons (excluding travel document verification).", "context": "(d)AI systems intended to be used by law enforcement authorities or on their behalf or by Union institutions,\nbodies, offices or agencies in support of law enforcement authorities for assessing the risk of a natural person\noffending or re-offending not solely on the basis of the profiling of natural persons as referred to in Article 3(4)\nof Directive (EU) 2016/680, or to assess personality traits and characteristics or past criminal behaviour of\nnatural persons or groups;\n\n(e)AI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies,\noffices or agencies in support of law enforcement authorities for the profiling of natural persons as referred to in\nArticle 3(4) of Directive (EU) 2016/680 in the course of the detection, investigation or prosecution of criminal\noffences.\n\n7. Migration, asylum and border control management, in so far as their use is permitted under relevant Union or\n    national law:\n\n(a)AI systems intended to be used by or on behalf of competent public authorities or by Union institutions, bodies,\noffices or agencies as polygraphs or similar tools;\n\n(b)AI systems intended to be used by or on behalf of competent pub...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0050", "generated_at": "2025-05-14T17:59:24.479048", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "66c4f087-86d4-419c-863b-bd1464ffa2ac", "question": "What must the technical documentation of an AI system include?", "answer": "A general description of the AI system, detailed development process, monitoring and control information, performance metrics, risk management system, lifecycle changes, harmonized standards, and post-market evaluation plans.", "context": "(d)AI systems intended to be used by law enforcement authorities or on their behalf or by Union institutions,\nbodies, offices or agencies in support of law enforcement authorities for assessing the risk of a natural person\noffending or re-offending not solely on the basis of the profiling of natural persons as referred to in Article 3(4)\nof Directive (EU) 2016/680, or to assess personality traits and characteristics or past criminal behaviour of\nnatural persons or groups;\n\n(e)AI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies,\noffices or agencies in support of law enforcement authorities for the profiling of natural persons as referred to in\nArticle 3(4) of Directive (EU) 2016/680 in the course of the detection, investigation or prosecution of criminal\noffences.\n\n7. Migration, asylum and border control management, in so far as their use is permitted under relevant Union or\n    national law:\n\n(a)AI systems intended to be used by or on behalf of competent public authorities or by Union institutions, bodies,\noffices or agencies as polygraphs or similar tools;\n\n(b)AI systems intended to be used by or on behalf of competent pub...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0050", "generated_at": "2025-05-14T17:59:24.481600", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "01e1ae82-d800-4f8f-aa9a-85ddacc41d62", "question": "What is required for the EU declaration of conformity?", "answer": "The declaration must include the AI system's name, provider details, a statement of conformity, data protection compliance, references to harmonized standards, notified body information, and the signing details.", "context": "(d)AI systems intended to be used by law enforcement authorities or on their behalf or by Union institutions,\nbodies, offices or agencies in support of law enforcement authorities for assessing the risk of a natural person\noffending or re-offending not solely on the basis of the profiling of natural persons as referred to in Article 3(4)\nof Directive (EU) 2016/680, or to assess personality traits and characteristics or past criminal behaviour of\nnatural persons or groups;\n\n(e)AI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies,\noffices or agencies in support of law enforcement authorities for the profiling of natural persons as referred to in\nArticle 3(4) of Directive (EU) 2016/680 in the course of the detection, investigation or prosecution of criminal\noffences.\n\n7. Migration, asylum and border control management, in so far as their use is permitted under relevant Union or\n    national law:\n\n(a)AI systems intended to be used by or on behalf of competent public authorities or by Union institutions, bodies,\noffices or agencies as polygraphs or similar tools;\n\n(b)AI systems intended to be used by or on behalf of competent pub...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0050", "generated_at": "2025-05-14T17:59:24.483269", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "a9d7f497-ed6a-4e6a-afd4-32db51969599", "question": "What is the conformity assessment procedure based on internal control?", "answer": "The provider verifies compliance with Article 17 requirements, assesses technical documentation against essential requirements, and ensures alignment with post-market monitoring processes.", "context": "(d)AI systems intended to be used by law enforcement authorities or on their behalf or by Union institutions,\nbodies, offices or agencies in support of law enforcement authorities for assessing the risk of a natural person\noffending or re-offending not solely on the basis of the profiling of natural persons as referred to in Article 3(4)\nof Directive (EU) 2016/680, or to assess personality traits and characteristics or past criminal behaviour of\nnatural persons or groups;\n\n(e)AI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies,\noffices or agencies in support of law enforcement authorities for the profiling of natural persons as referred to in\nArticle 3(4) of Directive (EU) 2016/680 in the course of the detection, investigation or prosecution of criminal\noffences.\n\n7. Migration, asylum and border control management, in so far as their use is permitted under relevant Union or\n    national law:\n\n(a)AI systems intended to be used by or on behalf of competent public authorities or by Union institutions, bodies,\noffices or agencies as polygraphs or similar tools;\n\n(b)AI systems intended to be used by or on behalf of competent pub...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0050", "generated_at": "2025-05-14T17:59:24.484922", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "78c652db-3cde-4af9-9c23-c6ccb28ab666", "question": "What are the key elements of the technical documentation for AI systems?", "answer": "It must include system description, development process, architecture, data requirements, human oversight measures, validation procedures, cybersecurity measures, and risk management details.", "context": "(d)AI systems intended to be used by law enforcement authorities or on their behalf or by Union institutions,\nbodies, offices or agencies in support of law enforcement authorities for assessing the risk of a natural person\noffending or re-offending not solely on the basis of the profiling of natural persons as referred to in Article 3(4)\nof Directive (EU) 2016/680, or to assess personality traits and characteristics or past criminal behaviour of\nnatural persons or groups;\n\n(e)AI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies,\noffices or agencies in support of law enforcement authorities for the profiling of natural persons as referred to in\nArticle 3(4) of Directive (EU) 2016/680 in the course of the detection, investigation or prosecution of criminal\noffences.\n\n7. Migration, asylum and border control management, in so far as their use is permitted under relevant Union or\n    national law:\n\n(a)AI systems intended to be used by or on behalf of competent public authorities or by Union institutions, bodies,\noffices or agencies as polygraphs or similar tools;\n\n(b)AI systems intended to be used by or on behalf of competent pub...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0050", "generated_at": "2025-05-14T17:59:24.486757", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "768e91f5-2f8e-4f94-8d83-688cdc5aa12e", "question": "What information must be included in the application for the quality management system under Article 17?", "answer": "The application must include the provider's name and address, list of AI systems covered, technical documentation for each system, documentation of the quality management system covering Article 17 aspects, procedures to maintain system adequacy, and a declaration that the application hasn't been submitted to another notified body.", "context": "2. Overview\n    The approved quality management system for the design, development and testing of AI systems pursuant to\n    Article 17 shall be examined in accordance with point 3 and shall be subject to surveillance as specified in point 5.\n    The technical documentation of the AI system shall be examined in accordance with point 4.\n3. Quality management system\n\n3.1. The application of the provider shall include:\n\n(a)the name and address of the provider and, if the application is lodged by an authorised representative, also their\nname and address;\n\n(b)the list of AI systems covered under the same quality management system;\n\n(c)the technical documentation for each AI system covered under the same quality management system;\n\n(d)the documentation concerning the quality management system which shall cover all the aspects listed under\nArticle 17;\n\n(e)a description of the procedures in place to ensure that the quality management system remains adequate and\neffective;\n\n(f) a written declaration that the same application has not been lodged with any other notified body.\n\n3.2. The quality management system shall be assessed by the notified body, which shall determine whether it satisfies...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0051", "generated_at": "2025-05-14T18:00:03.666132", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "b26232ca-b2ff-475e-86ab-3e77c262b930", "question": "How is the quality management system assessed by the notified body?", "answer": "The notified body assesses whether the system meets Article 17 requirements. The decision is notified to the provider, including assessment conclusions and reasoned decisions.", "context": "2. Overview\n    The approved quality management system for the design, development and testing of AI systems pursuant to\n    Article 17 shall be examined in accordance with point 3 and shall be subject to surveillance as specified in point 5.\n    The technical documentation of the AI system shall be examined in accordance with point 4.\n3. Quality management system\n\n3.1. The application of the provider shall include:\n\n(a)the name and address of the provider and, if the application is lodged by an authorised representative, also their\nname and address;\n\n(b)the list of AI systems covered under the same quality management system;\n\n(c)the technical documentation for each AI system covered under the same quality management system;\n\n(d)the documentation concerning the quality management system which shall cover all the aspects listed under\nArticle 17;\n\n(e)a description of the procedures in place to ensure that the quality management system remains adequate and\neffective;\n\n(f) a written declaration that the same application has not been lodged with any other notified body.\n\n3.2. The quality management system shall be assessed by the notified body, which shall determine whether it satisfies...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0051", "generated_at": "2025-05-14T18:00:03.667954", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "bd125930-19a7-42bc-a463-3b9e7acc77d6", "question": "What steps must a provider take if they intend to change the approved quality management system?", "answer": "The provider must inform the notified body of proposed changes. The notified body examines the changes and decides if reassessment is needed, notifying the provider of its decision.", "context": "2. Overview\n    The approved quality management system for the design, development and testing of AI systems pursuant to\n    Article 17 shall be examined in accordance with point 3 and shall be subject to surveillance as specified in point 5.\n    The technical documentation of the AI system shall be examined in accordance with point 4.\n3. Quality management system\n\n3.1. The application of the provider shall include:\n\n(a)the name and address of the provider and, if the application is lodged by an authorised representative, also their\nname and address;\n\n(b)the list of AI systems covered under the same quality management system;\n\n(c)the technical documentation for each AI system covered under the same quality management system;\n\n(d)the documentation concerning the quality management system which shall cover all the aspects listed under\nArticle 17;\n\n(e)a description of the procedures in place to ensure that the quality management system remains adequate and\neffective;\n\n(f) a written declaration that the same application has not been lodged with any other notified body.\n\n3.2. The quality management system shall be assessed by the notified body, which shall determine whether it satisfies...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0051", "generated_at": "2025-05-14T18:00:03.669625", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "d1b95eaf-6fcb-459c-9d42-f4c3378634dd", "question": "What is required for the technical documentation assessment of an AI system?", "answer": "The provider must submit an application to the notified body, including the provider's details, a declaration of no prior submission, and technical documentation as per Annex IV. The notified body examines the documentation, may request additional evidence, and issues a conformity certificate if applicable.", "context": "2. Overview\n    The approved quality management system for the design, development and testing of AI systems pursuant to\n    Article 17 shall be examined in accordance with point 3 and shall be subject to surveillance as specified in point 5.\n    The technical documentation of the AI system shall be examined in accordance with point 4.\n3. Quality management system\n\n3.1. The application of the provider shall include:\n\n(a)the name and address of the provider and, if the application is lodged by an authorised representative, also their\nname and address;\n\n(b)the list of AI systems covered under the same quality management system;\n\n(c)the technical documentation for each AI system covered under the same quality management system;\n\n(d)the documentation concerning the quality management system which shall cover all the aspects listed under\nArticle 17;\n\n(e)a description of the procedures in place to ensure that the quality management system remains adequate and\neffective;\n\n(f) a written declaration that the same application has not been lodged with any other notified body.\n\n3.2. The quality management system shall be assessed by the notified body, which shall determine whether it satisfies...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0051", "generated_at": "2025-05-14T18:00:03.671273", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "265d7ce6-d1d7-4ba8-b368-05f5a4947a3b", "question": "What information must be submitted for the registration of high-risk AI systems under Article 49?", "answer": "Providers must submit details like provider contact information, AI system description, intended purpose, data usage, system status, certificates, EU conformity declaration, and electronic instructions for use (excluding specific law enforcement areas). Deployers must also provide summaries of fundamental rights and data protection impact assessments.", "context": "2. Overview\n    The approved quality management system for the design, development and testing of AI systems pursuant to\n    Article 17 shall be examined in accordance with point 3 and shall be subject to surveillance as specified in point 5.\n    The technical documentation of the AI system shall be examined in accordance with point 4.\n3. Quality management system\n\n3.1. The application of the provider shall include:\n\n(a)the name and address of the provider and, if the application is lodged by an authorised representative, also their\nname and address;\n\n(b)the list of AI systems covered under the same quality management system;\n\n(c)the technical documentation for each AI system covered under the same quality management system;\n\n(d)the documentation concerning the quality management system which shall cover all the aspects listed under\nArticle 17;\n\n(e)a description of the procedures in place to ensure that the quality management system remains adequate and\neffective;\n\n(f) a written declaration that the same application has not been lodged with any other notified body.\n\n3.2. The quality management system shall be assessed by the notified body, which shall determine whether it satisfies...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0051", "generated_at": "2025-05-14T18:00:03.672918", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "23d5ece4-6e7f-4929-a16d-a44d315c8351", "question": "What information must be provided when registering high-risk AI systems for real-world testing under Article 60?", "answer": "A Union-wide unique ID, provider and deployer details, system description, testing plan summary, and information on suspension or termination of testing.", "context": "# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 137/144\n\nANNEX IX\n\nInformation to be submitted upon the registration of high-risk AI systems listed in Annex III in\nrelation to testing in real world conditions in accordance with Article 60\n\nThe following information shall be provided and thereafter kept up to date with regard to testing in real world conditions to\nbe registered in accordance with Article 60:\n\n1. A Union-wide unique single identification number of the testing in real world conditions;\n2. The name and contact details of the provider or prospective provider and of the deployers involved in the testing in\n    real world conditions;\n3. A brief description of the AI system, its intended purpose, and other information necessary for the identification of\n    the system;\n4. A summary of the main characteristics of the plan for testing in real world conditions;\n5. Information on the suspension or termination of the testing in real world conditions.\n\n# EN OJ L, 12.7.2024\n\n138/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\nANNEX X\n\nUnion legislative acts on la...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0052", "generated_at": "2025-05-14T18:01:16.783444", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "494c92e4-566a-4f70-897b-8e593d77198e", "question": "Which EU information systems are listed in Annex X under the Freedom, Security and Justice area?", "answer": "Schengen Information System, Visa Information System, Eurodac, Entry/Exit System, European Travel Information and Authorisation System (ETIAS), and European Criminal Records Information System (ECRIS-TCN).", "context": "# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 137/144\n\nANNEX IX\n\nInformation to be submitted upon the registration of high-risk AI systems listed in Annex III in\nrelation to testing in real world conditions in accordance with Article 60\n\nThe following information shall be provided and thereafter kept up to date with regard to testing in real world conditions to\nbe registered in accordance with Article 60:\n\n1. A Union-wide unique single identification number of the testing in real world conditions;\n2. The name and contact details of the provider or prospective provider and of the deployers involved in the testing in\n    real world conditions;\n3. A brief description of the AI system, its intended purpose, and other information necessary for the identification of\n    the system;\n4. A summary of the main characteristics of the plan for testing in real world conditions;\n5. Information on the suspension or termination of the testing in real world conditions.\n\n# EN OJ L, 12.7.2024\n\n138/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\nANNEX X\n\nUnion legislative acts on la...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0052", "generated_at": "2025-05-14T18:01:16.785254", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "06a62456-1540-4b5e-b9b1-cc38579d5a92", "question": "What details must providers of general-purpose AI models include in their technical documentation under Article 53(1)(a)?", "answer": "Model description, training data details, computational resources, energy consumption, architecture, parameters, modality, and acceptable use policies.", "context": "# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 137/144\n\nANNEX IX\n\nInformation to be submitted upon the registration of high-risk AI systems listed in Annex III in\nrelation to testing in real world conditions in accordance with Article 60\n\nThe following information shall be provided and thereafter kept up to date with regard to testing in real world conditions to\nbe registered in accordance with Article 60:\n\n1. A Union-wide unique single identification number of the testing in real world conditions;\n2. The name and contact details of the provider or prospective provider and of the deployers involved in the testing in\n    real world conditions;\n3. A brief description of the AI system, its intended purpose, and other information necessary for the identification of\n    the system;\n4. A summary of the main characteristics of the plan for testing in real world conditions;\n5. Information on the suspension or termination of the testing in real world conditions.\n\n# EN OJ L, 12.7.2024\n\n138/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\nANNEX X\n\nUnion legislative acts on la...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0052", "generated_at": "2025-05-14T18:01:16.786921", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "5aa28ade-e477-427a-a913-e35ae99d74ae", "question": "What transparency information must be provided to downstream providers integrating a general-purpose AI model?", "answer": "Model tasks, acceptable use policies, interaction with external systems, software versions, architecture, parameters, modality, and licence details.", "context": "# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 137/144\n\nANNEX IX\n\nInformation to be submitted upon the registration of high-risk AI systems listed in Annex III in\nrelation to testing in real world conditions in accordance with Article 60\n\nThe following information shall be provided and thereafter kept up to date with regard to testing in real world conditions to\nbe registered in accordance with Article 60:\n\n1. A Union-wide unique single identification number of the testing in real world conditions;\n2. The name and contact details of the provider or prospective provider and of the deployers involved in the testing in\n    real world conditions;\n3. A brief description of the AI system, its intended purpose, and other information necessary for the identification of\n    the system;\n4. A summary of the main characteristics of the plan for testing in real world conditions;\n5. Information on the suspension or termination of the testing in real world conditions.\n\n# EN OJ L, 12.7.2024\n\n138/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\nANNEX X\n\nUnion legislative acts on la...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0052", "generated_at": "2025-05-14T18:01:16.788564", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "7b17dda3-d4de-4010-9015-21b566fafaf0", "question": "What criteria does the Commission use to designate general-purpose AI models with systemic risk?", "answer": "Capabilities or impact equivalent to those in Article 51(1)(a), including potential for widespread disruption, data processing scale, and societal impact.", "context": "# OJ L, 12.7.2024 EN\n\nELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj) 137/144\n\nANNEX IX\n\nInformation to be submitted upon the registration of high-risk AI systems listed in Annex III in\nrelation to testing in real world conditions in accordance with Article 60\n\nThe following information shall be provided and thereafter kept up to date with regard to testing in real world conditions to\nbe registered in accordance with Article 60:\n\n1. A Union-wide unique single identification number of the testing in real world conditions;\n2. The name and contact details of the provider or prospective provider and of the deployers involved in the testing in\n    real world conditions;\n3. A brief description of the AI system, its intended purpose, and other information necessary for the identification of\n    the system;\n4. A summary of the main characteristics of the plan for testing in real world conditions;\n5. Information on the suspension or termination of the testing in real world conditions.\n\n# EN OJ L, 12.7.2024\n\n138/144 ELI: [http://data.europa.eu/eli/reg/2024/1689/oj](http://data.europa.eu/eli/reg/2024/1689/oj)\n\nANNEX X\n\nUnion legislative acts on la...", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0052", "generated_at": "2025-05-14T18:01:16.790225", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "1bdfb495-5e36-47cc-9be3-51a39f802b24", "question": "What aspects are evaluated regarding the data set used for training?", "answer": "(b) the quality or size of the data set, for example measured through tokens;", "context": "(a) the number of parameters of the model;\n(b) the quality or size of the data set, for example measured through tokens;\n\n(c) the amount of computation used for training the model, measured in floating point operations or indicated by\na combination of other variables such as estimated cost of training, estimated time required for the training, or\nestimated energy consumption for the training;\n\n(d) the input and output modalities of the model, such as text to text (large language models), text to image,\nmulti-modality, and the state of the art thresholds for determining high-impact capabilities for each modality, and\nthe specific type of inputs and outputs (e.g. biological sequences);\n\n(e) the benchmarks and evaluations of capabilities of the model, including considering the number of tasks without\nadditional training, adaptability to learn new, distinct tasks, its level of autonomy and scalability, the tools it has\naccess to;\n\n(f) whether it has a high impact on the internal market due to its reach, which shall be presumed when it has been\nmade available to at least 10 000 registered business users established in the Union;\n\n(g) the number of registered end-users.\n\n# EN OJ L, 12.7....", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0053", "generated_at": "2025-05-14T18:02:04.332128", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "c5c9d153-b02f-4d0e-b493-0da867d29962", "question": "What metrics are used to evaluate the computational requirements for training an AI model?", "answer": "(c) the amount of computation used for training the model, measured in floating point operations or indicated by a combination of other variables such as estimated cost of training, estimated time required for the training, or estimated energy consumption for the training;", "context": "(a) the number of parameters of the model;\n(b) the quality or size of the data set, for example measured through tokens;\n\n(c) the amount of computation used for training the model, measured in floating point operations or indicated by\na combination of other variables such as estimated cost of training, estimated time required for the training, or\nestimated energy consumption for the training;\n\n(d) the input and output modalities of the model, such as text to text (large language models), text to image,\nmulti-modality, and the state of the art thresholds for determining high-impact capabilities for each modality, and\nthe specific type of inputs and outputs (e.g. biological sequences);\n\n(e) the benchmarks and evaluations of capabilities of the model, including considering the number of tasks without\nadditional training, adaptability to learn new, distinct tasks, its level of autonomy and scalability, the tools it has\naccess to;\n\n(f) whether it has a high impact on the internal market due to its reach, which shall be presumed when it has been\nmade available to at least 10 000 registered business users established in the Union;\n\n(g) the number of registered end-users.\n\n# EN OJ L, 12.7....", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0053", "generated_at": "2025-05-14T18:02:04.334035", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "b9dee1b2-be2a-47f0-bd0e-f6e181ad4d27", "question": "What factors are considered for determining the input and output modalities of an AI model?", "answer": "(d) the input and output modalities of the model, such as text to text (large language models), text to image, multi-modality, and the state of the art thresholds for determining high-impact capabilities for each modality, and the specific type of inputs and outputs (e.g. biological sequences);", "context": "(a) the number of parameters of the model;\n(b) the quality or size of the data set, for example measured through tokens;\n\n(c) the amount of computation used for training the model, measured in floating point operations or indicated by\na combination of other variables such as estimated cost of training, estimated time required for the training, or\nestimated energy consumption for the training;\n\n(d) the input and output modalities of the model, such as text to text (large language models), text to image,\nmulti-modality, and the state of the art thresholds for determining high-impact capabilities for each modality, and\nthe specific type of inputs and outputs (e.g. biological sequences);\n\n(e) the benchmarks and evaluations of capabilities of the model, including considering the number of tasks without\nadditional training, adaptability to learn new, distinct tasks, its level of autonomy and scalability, the tools it has\naccess to;\n\n(f) whether it has a high impact on the internal market due to its reach, which shall be presumed when it has been\nmade available to at least 10 000 registered business users established in the Union;\n\n(g) the number of registered end-users.\n\n# EN OJ L, 12.7....", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0053", "generated_at": "2025-05-14T18:02:04.335708", "model": "qwen3:8b", "confidence": 1.0}}
{"id": "3f91c7a7-19d8-4c07-9067-d186df7b8fe7", "question": "What criteria are used to evaluate the capabilities of an AI model?", "answer": "(e) the benchmarks and evaluations of capabilities of the model, including considering the number of tasks without additional training, adaptability to learn new, distinct tasks, its level of autonomy and scalability, the tools it has access to;", "context": "(a) the number of parameters of the model;\n(b) the quality or size of the data set, for example measured through tokens;\n\n(c) the amount of computation used for training the model, measured in floating point operations or indicated by\na combination of other variables such as estimated cost of training, estimated time required for the training, or\nestimated energy consumption for the training;\n\n(d) the input and output modalities of the model, such as text to text (large language models), text to image,\nmulti-modality, and the state of the art thresholds for determining high-impact capabilities for each modality, and\nthe specific type of inputs and outputs (e.g. biological sequences);\n\n(e) the benchmarks and evaluations of capabilities of the model, including considering the number of tasks without\nadditional training, adaptability to learn new, distinct tasks, its level of autonomy and scalability, the tools it has\naccess to;\n\n(f) whether it has a high impact on the internal market due to its reach, which shall be presumed when it has been\nmade available to at least 10 000 registered business users established in the Union;\n\n(g) the number of registered end-users.\n\n# EN OJ L, 12.7....", "source": "EU AI Act", "category": "Other", "metadata": {"chunk_id": "chunk_0053", "generated_at": "2025-05-14T18:02:04.337378", "model": "qwen3:8b", "confidence": 1.0}}
